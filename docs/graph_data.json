{
  "nodes": [
    {
      "id": "2601.04107v1",
      "name": "From Abstract Threats to Institutional Realities: A Comparative Semantic Network Analysis of AI Securitisation in the US, EU, and China",
      "group": "paper",
      "val": 30,
      "abstract": "Artificial intelligence governance exhibits a striking paradox: while major jurisdictions converge rhetorically around concepts such as safety, risk, and accountability, their regulatory frameworks remain fundamentally divergent and mutually unintelligible. This paper argues that this fragmentation cannot be explained solely by geopolitical rivalry, institutional complexity, or instrument selection. Instead, it stems from how AI is constituted as an object of governance through distinct institutional logics. Integrating securitisation theory with the concept of the dispositif, we demonstrate that jurisdictions govern ontologically different objects under the same vocabulary. Using semantic network analysis of official policy texts from the European Union, the United States, and China (2023-2025), we trace how concepts like safety are embedded within divergent semantic architectures. Our findings reveal that the EU juridifies AI as a certifiable product through legal-bureaucratic logic; the US operationalises AI as an optimisable system through market-liberal logic; and China governs AI as socio-technical infrastructure through holistic state logic. We introduce the concept of structural incommensurability to describe this condition of ontological divergence masked by terminological convergence. This reframing challenges ethics-by-principles approaches to global AI governance, suggesting that coordination failures arise not from disagreement over values but from the absence of a shared reference object.",
      "url": "https://arxiv.org/pdf/2601.04107v1",
      "date": "2026-01-07T17:12:03+00:00"
    },
    {
      "id": "auth_ruiyi_guo",
      "name": "Ruiyi Guo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_bodong_zhang",
      "name": "Bodong Zhang",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_institutional_logics",
      "name": "institutional logics",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_governance",
      "name": "ai governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_governs_ai",
      "name": "governs ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intelligence_governance",
      "name": "intelligence governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_govern_ontologically",
      "name": "govern ontologically",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04094v1",
      "name": "The Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning",
      "group": "paper",
      "val": 30,
      "abstract": "The EU AI Act adopts a horizontal and adaptive approach to govern AI technologies characterised by rapid development and unpredictable emerging capabilities. To maintain relevance, the Act embeds provisions for regulatory learning. However, these provisions operate within a complex network of actors and mechanisms that lack a clearly defined technical basis for scalable information flow. This paper addresses this gap by establishing a theoretical model of regulatory learning space defined by the AI Act, decomposed into micro, meso, and macro levels. Drawing from this functional perspective of this model, we situate the diverse stakeholders - ranging from the EU Commission at the macro level to AI developers at the micro level - within the transitions of enforcement (macro-micro) and evidence aggregation (micro-macro). We identify AI Technical Sandboxes as the essential engine for evidence generation at the micro level, providing the necessary data to drive scalable learning across all levels of the model. By providing an extensive discussion of the requirements and challenges for AITSes to serve as this micro-level evidence generator, we aim to bridge the gap between legislative commands and technical operationalisation, thereby enabling a structured discourse between technical and legal experts.",
      "url": "https://arxiv.org/pdf/2601.04094v1",
      "date": "2026-01-07T17:01:06+00:00"
    },
    {
      "id": "auth_tom_deckenbrunnen",
      "name": "Tom Deckenbrunnen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_alessio_buscemi",
      "name": "Alessio Buscemi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marco_almada",
      "name": "Marco Almada",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_alfredo_capozucca",
      "name": "Alfredo Capozucca",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_german_castignani",
      "name": "German Castignani",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_eu_ai",
      "name": "eu ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_act",
      "name": "ai act",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_enforcement_macro",
      "name": "enforcement macro",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_govern_ai",
      "name": "govern ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_technical",
      "name": "ai technical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03788v1",
      "name": "Criminal Liability of Generative Artificial Intelligence Providers for User-Generated Child Sexual Abuse Material",
      "group": "paper",
      "val": 30,
      "abstract": "The development of more powerful Generative Artificial Intelligence (GenAI) has expanded its capabilities and the variety of outputs. This has introduced significant legal challenges, including gray areas in various legal systems, such as the assessment of criminal liability for those responsible for these models. Therefore, we conducted a multidisciplinary study utilizing the statutory interpretation of relevant German laws, which, in conjunction with scenarios, provides a perspective on the different properties of GenAI in the context of Child Sexual Abuse Material (CSAM) generation. We found that generating CSAM with GenAI may have criminal and legal consequences not only for the user committing the primary offense but also for individuals responsible for the models, such as independent software developers, researchers, and company representatives. Additionally, the assessment of criminal liability may be affected by contextual and technical factors, including the type of generated image, content moderation policies, and the model's intended purpose. Based on our findings, we discussed the implications for different roles, as well as the requirements when developing such systems.",
      "url": "https://arxiv.org/pdf/2601.03788v1",
      "date": "2026-01-07T10:38:35+00:00"
    },
    {
      "id": "auth_anamaria_mojica_hanke",
      "name": "Anamaria Mojica-Hanke",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_thomas_goger",
      "name": "Thomas Goger",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_svenja_wölfel",
      "name": "Svenja Wölfel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_brian_valerius",
      "name": "Brian Valerius",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_steffen_herbold",
      "name": "Steffen Herbold",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_offense_individuals",
      "name": "offense individuals",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_criminal_liability",
      "name": "criminal liability",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_consequences",
      "name": "legal consequences",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_criminal_legal",
      "name": "criminal legal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_systems",
      "name": "legal systems",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03733v1",
      "name": "RadDiff: Describing Differences in Radiology Image Sets with Natural Language",
      "group": "paper",
      "val": 30,
      "abstract": "Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.",
      "url": "https://arxiv.org/pdf/2601.03733v1",
      "date": "2026-01-07T09:25:04+00:00"
    },
    {
      "id": "auth_xiaoxian_shen",
      "name": "Xiaoxian Shen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yuhui_zhang",
      "name": "Yuhui Zhang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sahithi_ankireddy",
      "name": "Sahithi Ankireddy",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_xiaohan_wang",
      "name": "Xiaohan Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_maya_varma",
      "name": "Maya Varma",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_henry_guo",
      "name": "Henry Guo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_curtis_langlotz",
      "name": "Curtis Langlotz",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_serena_yeung_levy",
      "name": "Serena Yeung-Levy",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_multimodal_reasoning",
      "name": "multimodal reasoning",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_medical_ai",
      "name": "medical ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_radiology_image",
      "name": "radiology image",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_understanding_radiology",
      "name": "understanding radiology",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_raddiff_multimodal",
      "name": "raddiff multimodal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03709v1",
      "name": "The Power of 10: New Rules for the Digital World",
      "group": "paper",
      "val": 30,
      "abstract": "As artificial intelligence rapidly advances, society is increasingly captivated by promises of superhuman machines and seamless digital futures. Yet these visions often obscure mounting social, ethical, and psychological concerns tied to pervasive digital technologies - from surveillance to mental health crises. This article argues that a guiding ethos is urgently needed to navigate these transformations. Inspired by the lasting influence of the biblical Ten Commandments, a European interdisciplinary group has proposed \"Ten Rules for the Digital World\" - a novel ethical framework to help individuals and societies make prudent, human-centered decisions in the age of \"supercharged\" technology.",
      "url": "https://arxiv.org/pdf/2601.03709v1",
      "date": "2026-01-07T08:49:02+00:00"
    },
    {
      "id": "auth_sarah_spiekermann_hoff",
      "name": "Sarah Spiekermann-Hoff",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marc_langheinrich",
      "name": "Marc Langheinrich",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_johannes_hoff",
      "name": "Johannes Hoff",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_christiane_wendehorst",
      "name": "Christiane Wendehorst",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jürgen_pfeffer",
      "name": "Jürgen Pfeffer",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_thomas_fuchs",
      "name": "Thomas Fuchs",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_armin_grunwald",
      "name": "Armin Grunwald",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_digital_world",
      "name": "digital world",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital_technologies",
      "name": "digital technologies",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ethical_psychological",
      "name": "ethical psychological",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_rules_digital",
      "name": "rules digital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital",
      "name": "digital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03693v1",
      "name": "Can AI Chatbots Provide Coaching in Engineering? Beyond Information Processing Toward Mastery",
      "group": "paper",
      "val": 30,
      "abstract": "Engineering education faces a double disruption: traditional apprenticeship models that cultivated judgment and tacit skill are eroding, just as generative AI emerges as an informal coaching partner. This convergence rekindles long-standing questions in the philosophy of AI and cognition about the limits of computation, the nature of embodied rationality, and the distinction between information processing and wisdom. Building on this rich intellectual tradition, this paper examines whether AI chatbots can provide coaching that fosters mastery rather than merely delivering information. We synthesize critical perspectives from decades of scholarship on expertise, tacit knowledge, and human-machine interaction, situating them within the context of contemporary AI-driven education. Empirically, we report findings from a mixed-methods study (N = 75 students, N = 7 faculty) exploring the use of a coaching chatbot in engineering education. Results reveal a consistent boundary: participants accept AI for technical problem solving (convergent tasks; M = 3.84 on a 1-5 Likert scale) but remain skeptical of its capacity for moral, emotional, and contextual judgment (divergent tasks). Faculty express stronger concerns over risk (M = 4.71 vs. M = 4.14, p = 0.003), and privacy emerges as a key requirement, with 64-71 percent of participants demanding strict confidentiality. Our findings suggest that while generative AI can democratize access to cognitive and procedural support, it cannot replicate the embodied, value-laden dimensions of human mentorship. We propose a multiplex coaching framework that integrates human wisdom within expert-in-the-loop models, preserving the depth of apprenticeship while leveraging AI scalability to enrich the next generation of engineering education.",
      "url": "https://arxiv.org/pdf/2601.03693v1",
      "date": "2026-01-07T08:28:47+00:00"
    },
    {
      "id": "auth_junaid_qadir",
      "name": "Junaid Qadir",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_muhammad_adil_attique",
      "name": "Muhammad Adil Attique",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_saleha_shoaib",
      "name": "Saleha Shoaib",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_syed_ibrahim_ghaznavi",
      "name": "Syed Ibrahim Ghaznavi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_examines_ai",
      "name": "examines ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_coaching_chatbot",
      "name": "coaching chatbot",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intellectual",
      "name": "intellectual",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_chatbots",
      "name": "ai chatbots",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03547v1",
      "name": "Governance of Technological Transition: A Predator-Prey Analysis of AI Capital in China's Economy and Its Policy Implications",
      "group": "paper",
      "val": 30,
      "abstract": "The rapid integration of Artificial Intelligence (AI) into China's economy presents a classic governance challenge: how to harness its growth potential while managing its disruptive effects on traditional capital and labor markets. This study addresses this policy dilemma by modeling the dynamic interactions between AI capital, physical capital, and labor within a Lotka-Volterra predator-prey framework. Using annual Chinese data (2016-2023), we quantify the interaction strengths, identify stable equilibria, and perform a global sensitivity analysis. Our results reveal a consistent pattern where AI capital acts as the 'prey', stimulating both physical capital accumulation and labor compensation (wage bill), while facing only weak constraining feedback. The equilibrium points are stable nodes, indicating a policy-mediated convergence path rather than volatile cycles. Critically, the sensitivity analysis shows that the labor market equilibrium is overwhelmingly driven by AI-related parameters, whereas the physical capital equilibrium is also influenced by its own saturation dynamics. These findings provide a systemic, quantitative basis for policymakers: (1) to calibrate AI promotion policies by recognizing the asymmetric leverage points in capital vs. labor markets; (2) to anticipate and mitigate structural rigidities that may arise from current regulatory settings; and (3) to prioritize interventions that foster complementary growth between AI and traditional economic structures while ensuring broad-base distribution of technological gains.",
      "url": "https://arxiv.org/pdf/2601.03547v1",
      "date": "2026-01-07T03:30:46+00:00"
    },
    {
      "id": "auth_kunpeng_wang",
      "name": "Kunpeng Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jiahui_hu",
      "name": "Jiahui Hu",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_china",
      "name": "ai china",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_capital",
      "name": "ai capital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_china_economy",
      "name": "china economy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_growth_ai",
      "name": "growth ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_capital_equilibrium",
      "name": "capital equilibrium",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03469v1",
      "name": "Content vs. Form: What Drives the Writing Score Gap Across Socioeconomic Backgrounds? A Generated Panel Approach",
      "group": "paper",
      "val": 30,
      "abstract": "Students from different socioeconomic backgrounds exhibit persistent gaps in test scores, gaps that can translate into unequal educational and labor-market outcomes later in life. In many assessments, performance reflects not only what students know, but also how effectively they can communicate that knowledge. This distinction is especially salient in writing assessments, where scores jointly reward the substance of students' ideas and the way those ideas are expressed. As a result, observed score gaps may conflate differences in underlying content with differences in expressive skill. A central question, therefore, is how much of the socioeconomic-status (SES) gap in scores is driven by differences in what students say versus how they say it. We study this question using a large corpus of persuasive essays written by U.S. middle- and high-school students. We introduce a new measurement strategy that separates content from style by leveraging large language models to generate multiple stylistic variants of each essay. These rewrites preserve the underlying arguments while systematically altering surface expression, creating a \"generated panel\" that introduces controlled within-essay variation in style. This approach allows us to decompose SES gaps in writing scores into contributions from content and style. We find an SES gap of 0.67 points on a 1-6 scale. Approximately 69% of the gap is attributable to differences in essay content quality, Style differences account for 26% of the gap, and differences in evaluation standards across SES groups account for the remaining 5%. These patterns seems stable across demographic subgroups and writing tasks. More broadly, our approach shows how large language models can be used to generate controlled variation in observational data, enabling researchers to isolate and quantify the contributions of otherwise entangled factors.",
      "url": "https://arxiv.org/pdf/2601.03469v1",
      "date": "2026-01-06T23:45:18+00:00"
    },
    {
      "id": "auth_nadav_kunievsky",
      "name": "Nadav Kunievsky",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_pedro_pertusi",
      "name": "Pedro Pertusi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_essay_variation",
      "name": "essay variation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_essay_rewrites",
      "name": "essay rewrites",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_persuasive_essays",
      "name": "persuasive essays",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_writing_scores",
      "name": "writing scores",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_essay_content",
      "name": "essay content",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03458v1",
      "name": "Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant",
      "group": "paper",
      "val": 30,
      "abstract": "Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging.   We present a system that processes free-form natural language input, handles a wide range of edge cases, and comments competently not only on the technical correctness of submitted proofs, but also on style and presentation issues. We discuss the advantages and disadvantages of various approaches to the evaluation of such a system, and show that by the metrics we evaluate, the quality of the feedback generated is comparable to that produced by human experts when assessing early undergraduate homework. We stress-test our system with a small set of more advanced and unusual questions, and report both significant gaps and encouraging successes in that more challenging setting.   Our system uses large language models in a modular workflow. The workflow configuration is human-readable and editable without programming knowledge, and allows some intermediate steps to be precomputed or injected by the instructor.   A version of our tool is deployed on the Imperial mathematics homework platform Lambdafeedback. We report also on the integration of our tool into this platform.",
      "url": "https://arxiv.org/pdf/2601.03458v1",
      "date": "2026-01-06T23:02:22+00:00"
    },
    {
      "id": "auth_aron_gohr",
      "name": "Aron Gohr",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marie_amelie_lawn",
      "name": "Marie-Amelie Lawn",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kevin_gao",
      "name": "Kevin Gao",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_inigo_serjeant",
      "name": "Inigo Serjeant",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_stephen_heslip",
      "name": "Stephen Heslip",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_tutoring_systems",
      "name": "tutoring systems",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intelligent_tutoring",
      "name": "intelligent tutoring",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_tutoring",
      "name": "tutoring",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_homework_platform",
      "name": "homework platform",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_editable_programming",
      "name": "editable programming",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03430v1",
      "name": "An Empirical Analysis of Community and Coding Patterns in OSS4SG vs. Conventional OSS",
      "group": "paper",
      "val": 30,
      "abstract": "Open Source Software for Social Good (OSS4SG) projects aim to address critical societal challenges, such as healthcare access and community safety. Understanding the community dynamics and contributor patterns in these projects is essential for ensuring their sustainability and long-term impact. However, while extensive research has focused on conventional Open Source Software (OSS), little is known about how the mission-driven nature of OSS4SG influences its development practices. To address this gap, we conduct a large-scale empirical study of 1,039 GitHub repositories, comprising 422 OSS4SG and 617 conventional OSS projects, to compare community structure, contributor engagement, and coding practices. Our findings reveal that OSS4SG projects foster significantly more stable and \"sticky\" (63.4%) communities, whereas conventional OSS projects are more \"magnetic\" (75.4%), attracting a high turnover of contributors. OSS4SG projects also demonstrate consistent engagement throughout the year, while conventional OSS communities exhibit seasonal fluctuations. Additionally, OSS4SG projects rely heavily on core contributors for both code quality and issue resolution, while conventional OSS projects leverage casual contributors for issue resolution, with core contributors focusing primarily on code quality.",
      "url": "https://arxiv.org/pdf/2601.03430v1",
      "date": "2026-01-06T21:37:12+00:00"
    },
    {
      "id": "auth_mohamed_ouf",
      "name": "Mohamed Ouf",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_shayan_noei",
      "name": "Shayan Noei",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_zeph_van_iterson",
      "name": "Zeph Van Iterson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariam_guizani",
      "name": "Mariam Guizani",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ying_zou",
      "name": "Ying Zou",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_contributors_oss4sg",
      "name": "contributors oss4sg",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_contributors_code",
      "name": "contributors code",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_github",
      "name": "github",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_oss_communities",
      "name": "oss communities",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_oss_projects",
      "name": "oss projects",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03222v1",
      "name": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI",
      "group": "paper",
      "val": 30,
      "abstract": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness.",
      "url": "https://arxiv.org/pdf/2601.03222v1",
      "date": "2026-01-06T18:07:52+00:00"
    },
    {
      "id": "auth_jacob_erickson",
      "name": "Jacob Erickson",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_trust_ai",
      "name": "trust ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_autonomy_trust",
      "name": "autonomy trust",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_conversational_ai",
      "name": "conversational ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_agents",
      "name": "ai agents",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_focusing_trust",
      "name": "focusing trust",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04175v1",
      "name": "Legal Alignment for Safe and Ethical AI",
      "group": "paper",
      "val": 30,
      "abstract": "Alignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law. In this paper, we aim to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging field -- legal alignment -- focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.",
      "url": "https://arxiv.org/pdf/2601.04175v1",
      "date": "2026-01-07T18:42:04+00:00"
    },
    {
      "id": "auth_noam_kolt",
      "name": "Noam Kolt",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_nicholas_caputo",
      "name": "Nicholas Caputo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jack_boeglin",
      "name": "Jack Boeglin",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_cullen_o'keefe",
      "name": "Cullen O'Keefe",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_rishi_bommasani",
      "name": "Rishi Bommasani",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_stephen_casper",
      "name": "Stephen Casper",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariano_florentino_cuéllar",
      "name": "Mariano-Florentino Cuéllar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_noah_feldman",
      "name": "Noah Feldman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_iason_gabriel",
      "name": "Iason Gabriel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_gillian_k_hadfield",
      "name": "Gillian K. Hadfield",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_lewis_hammond",
      "name": "Lewis Hammond",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_peter_henderson",
      "name": "Peter Henderson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_atoosa_kasirzadeh",
      "name": "Atoosa Kasirzadeh",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_seth_lazar",
      "name": "Seth Lazar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_anka_reuel",
      "name": "Anka Reuel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kevin_l_wei",
      "name": "Kevin L. Wei",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jonathan_zittrain",
      "name": "Jonathan Zittrain",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_legal_alignment",
      "name": "legal alignment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_compliance",
      "name": "legal compliance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_exploring_legal",
      "name": "exploring legal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_specifying_ai",
      "name": "specifying ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ensuring_ai",
      "name": "ensuring ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05232v1",
      "name": "Measuring and Fostering Peace through Machine Learning and Artificial Intelligence",
      "group": "paper",
      "val": 30,
      "abstract": "We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.",
      "url": "https://arxiv.org/pdf/2601.05232v1",
      "date": "2026-01-08T18:57:01+00:00"
    },
    {
      "id": "auth_p_gilda",
      "name": "P. Gilda",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_p_dungarwal",
      "name": "P. Dungarwal",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_a_thongkham",
      "name": "A. Thongkham",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_e_t_ajayi",
      "name": "E. T. Ajayi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_s_choudhary",
      "name": "S. Choudhary",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_t_m_terol",
      "name": "T. M. Terol",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_c_lam",
      "name": "C. Lam",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_j_p_araujo",
      "name": "J. P. Araujo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_m_mcfadyen_mungalln",
      "name": "M. McFadyen-Mungalln",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_l_s_liebovitch",
      "name": "L. S. Liebovitch",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_p_t_coleman",
      "name": "P. T. Coleman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_h_west",
      "name": "H. West",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_k_sieck",
      "name": "K. Sieck",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_s_carter",
      "name": "S. Carter",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_news_social",
      "name": "news social",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_social_media",
      "name": "social media",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_news_dataset",
      "name": "news dataset",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_videos_social",
      "name": "videos social",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_media_youtube",
      "name": "media youtube",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04958v1",
      "name": "The unsuitability of existing regulations to reach sustainable AI",
      "group": "paper",
      "val": 30,
      "abstract": "This paper examines the European Union's emerging regulatory landscape - focusing on the AI Act, corporate sustainability reporting and due diligence regimes (CSRD and CSDDD), and data center regulation - to assess whether it can effectively govern AI's environmental footprint. We argue that, despite incremental progress, current approaches remain ill-suited to correcting the market failures underpinning AI-related energy use, water consumption, and material demand. Key shortcomings include narrow disclosure requirements, excessive reliance on voluntary standards, weak enforcement mechanisms, and a structural disconnect between AI-specific impacts and broader sustainability laws. The analysis situates these regulatory gaps within a wider ecosystem of academic research, civil society advocacy, standard-setting, and industry initiatives, highlighting risks of regulatory capture and greenwashing. Building on this diagnosis, the paper advances strategic recommendations for the COP30 Action Agenda, calling for binding transparency obligations, harmonized international standards for lifecycle assessment, stricter governance of data center expansion, and meaningful public participation in AI infrastructure decisions.",
      "url": "https://arxiv.org/pdf/2601.04958v1",
      "date": "2026-01-08T14:02:51+00:00"
    },
    {
      "id": "auth_thomas_le_goff",
      "name": "Thomas Le Goff",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_sustainability_laws",
      "name": "sustainability laws",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regulation_assess",
      "name": "regulation assess",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regulatory_landscape",
      "name": "regulatory landscape",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sustainability_reporting",
      "name": "sustainability reporting",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_environmental",
      "name": "ai environmental",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04403v1",
      "name": "Balancing Usability and Compliance in AI Smart Devices: A Privacy-by-Design Audit of Google Home, Alexa, and Siri",
      "group": "paper",
      "val": 30,
      "abstract": "This paper investigates the privacy and usability of AI-enabled smart devices commonly used by youth, focusing on Google Home Mini, Amazon Alexa, and Apple Siri. While these devices provide convenience and efficiency, they also raise privacy and transparency concerns due to their always-listening design and complex data management processes. The study proposes and applies a combined framework of Heuristic Evaluation, Personal Information Protection and Electronic Documents Act (PIPEDA) Compliance Assessment, and Youth-Centered Usability Testing to assess whether these devices align with Privacy-by-Design principles and support meaningful user control. Results show that Google Home achieved the highest usability score, while Siri scored highest in regulatory compliance, indicating a trade-off between user convenience and privacy protection. Alexa demonstrated clearer task navigation but weaker transparency in data retention. Findings suggest that although youth may feel capable of managing their data, their privacy self-efficacy remains limited by technical design, complex settings, and unclear data policies. The paper concludes that enhancing transparency, embedding privacy guidance during onboarding, and improving policy alignment are critical steps toward ensuring that smart devices are both usable and compliant with privacy standards that protect young users.",
      "url": "https://arxiv.org/pdf/2601.04403v1",
      "date": "2026-01-07T21:20:58+00:00"
    },
    {
      "id": "auth_trevor_de_clark",
      "name": "Trevor De Clark",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yulia_bobkova",
      "name": "Yulia Bobkova",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ajay_kumar_shrestha",
      "name": "Ajay Kumar Shrestha",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_privacy_usability",
      "name": "privacy usability",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_self",
      "name": "privacy self",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_guidance",
      "name": "privacy guidance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_compliant_privacy",
      "name": "compliant privacy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_standards",
      "name": "privacy standards",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05961v1",
      "name": "Navigating the Sociotechnical Imaginaries of Brazilian Tech Workers",
      "group": "paper",
      "val": 30,
      "abstract": "This chapter examines the sociotechnical imaginaries of Brazilian tech workers, a group often overlooked in digital labor research despite their role in designing the digital systems that shape everyday life. Grounded in the idea of sociotechnical imaginaries as collectively constructed visions that guide technology development and governance, the chapter argues that looking from the Global South helps challenge data universalism and foregrounds locally situated values, constraints, and futures. Drawing on semi-structured interviews with 26 Brazilian professionals conducted between July and December 2023, it maps how workers make sense of responsibility, bias, and power in AI and platform development. The findings highlight recurring tensions between academic and industry discourse on algorithmic bias, the limits of corporate accountability regarding user harm and surveillance, and the contested meanings of digital sovereignty, including grassroots initiatives that seek alternative technological futures aligned with marginalized communities needs.",
      "url": "https://arxiv.org/pdf/2601.05961v1",
      "date": "2026-01-09T17:30:04+00:00"
    },
    {
      "id": "auth_kenzo_soares_seto",
      "name": "Kenzo Soares Seto",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_examines_sociotechnical",
      "name": "examines sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital_sovereignty",
      "name": "digital sovereignty",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sociotechnical_imaginaries",
      "name": "sociotechnical imaginaries",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sociotechnical",
      "name": "sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_idea_sociotechnical",
      "name": "idea sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05918v1",
      "name": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
      "group": "paper",
      "val": 30,
      "abstract": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
      "url": "https://arxiv.org/pdf/2601.05918v1",
      "date": "2026-01-09T16:32:33+00:00"
    },
    {
      "id": "auth_tianshi_li",
      "name": "Tianshi Li",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_interviewer_ai",
      "name": "interviewer ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_identifying_interviewees",
      "name": "identifying interviewees",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_interviewees",
      "name": "interviewees",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_anthropic_interviewer",
      "name": "anthropic interviewer",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_search_agentic",
      "name": "search agentic",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05904v1",
      "name": "Can AI mediation improve democratic deliberation?",
      "group": "paper",
      "val": 30,
      "abstract": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.",
      "url": "https://arxiv.org/pdf/2601.05904v1",
      "date": "2026-01-09T16:22:26+00:00"
    },
    {
      "id": "auth_michael_henry_tessler",
      "name": "Michael Henry Tessler",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_georgina_evans",
      "name": "Georgina Evans",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_michiel_a_bakker",
      "name": "Michiel A. Bakker",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sophie_bridgers",
      "name": "Sophie Bridgers",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_rishub_jain",
      "name": "Rishub Jain",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_raphael_koster",
      "name": "Raphael Koster",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_verena_rieser",
      "name": "Verena Rieser",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_anca_dragan",
      "name": "Anca Dragan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matthew_botvinick",
      "name": "Matthew Botvinick",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_christopher_summerfield",
      "name": "Christopher Summerfield",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_mediated_deliberation",
      "name": "mediated deliberation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_enhancing",
      "name": "deliberation enhancing",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_political",
      "name": "deliberation political",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_meaningful_deliberation",
      "name": "meaningful deliberation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_augmentation",
      "name": "deliberation augmentation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05879v1",
      "name": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law",
      "group": "paper",
      "val": 30,
      "abstract": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.",
      "url": "https://arxiv.org/pdf/2601.05879v1",
      "date": "2026-01-09T15:55:03+00:00"
    },
    {
      "id": "auth_jakub_harasta",
      "name": "Jakub Harasta",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matej_vasina",
      "name": "Matej Vasina",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_martin_kornel",
      "name": "Martin Kornel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_tomas_foltynek",
      "name": "Tomas Foltynek",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_legal_contexts",
      "name": "legal contexts",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_family_law",
      "name": "family law",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_self",
      "name": "legal self",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scenario_gendered",
      "name": "scenario gendered",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_guidance",
      "name": "legal guidance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05826v1",
      "name": "Cross-National Evidence of Disproportionate Media Visibility for the Radical Right in the 2024 European Elections",
      "group": "paper",
      "val": 30,
      "abstract": "This study provides a systematic comparative analysis of media visibility of different political families during the 2024 European Parliament elections. We analyzed close to 21,500 unique news from leading national outlets in Austria, Germany, Ireland, Poland, and Portugal - countries with diverse political contexts and levels of media trust. Combining computational and human classification, we identified parties, political leaders, and groups from the article's URLs and titles, and clustered them according to European Parliament political families and broad political leanings. Cross-country comparison shows that the Mainstream and the Radical Right were mentioned more often than the other political groups. Moreover, the Radical Right received disproportionate attention relative to electoral results (from 2019 or 2024) and electoral projections, particularly in Austria, Germany, and Ireland. This imbalance increased in the final weeks of the campaign, when media influence on undecided voters is greatest. Outlet-level analysis shows that coverage of right-leaning entities dominated across news sources, especially those generating the highest traffic, suggesting a structural rather than outlet-specific pattern. Media visibility is a central resource, and this systematic mapping of online coverage highlights how traditional media can contribute to structural asymmetries in democratic competition.",
      "url": "https://arxiv.org/pdf/2601.05826v1",
      "date": "2026-01-09T15:00:59+00:00"
    },
    {
      "id": "auth_íris_damião",
      "name": "Íris Damião",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_joão_franco",
      "name": "João Franco",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariana_silva",
      "name": "Mariana Silva",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_paulo_almeida",
      "name": "Paulo Almeida",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_pedro_c_magalhães",
      "name": "Pedro C. Magalhães",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_joana_gonçalves_sá",
      "name": "Joana Gonçalves-Sá",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_campaign_media",
      "name": "campaign media",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_elections_analyzed",
      "name": "elections analyzed",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_electoral_projections",
      "name": "electoral projections",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_different_political",
      "name": "different political",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_media_influence",
      "name": "media influence",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05666v1",
      "name": "Advancing credit mobility through stakeholder-informed AI design and adoption",
      "group": "paper",
      "val": 30,
      "abstract": "Transferring from a 2-year to a 4-year college is crucial for socioeconomic mobility, yet students often face challenges ensuring their credits are fully recognized, leading to delays in their academic progress and unexpected costs. Determining whether courses at different institutions are equivalent (i.e., articulation) is essential for successful credit transfer, as it minimizes unused credits and increases the likelihood of bachelor's degree completion. However, establishing articulation agreements remains time- and resource-intensive, as all candidate articulations are reviewed manually. Although recent efforts have explored the use of artificial intelligence to support this work, its use in articulation practice remains limited. Given these challenges and the need for scalable support, this study applies artificial intelligence to suggest articulations between institutions in collaboration with the State University of New York system, one of the largest systems of higher education in the US. To develop our methodology, we first surveyed articulation staff and faculty to assess adoption rates of baseline algorithmic recommendations and gather feedback on perceptions and concerns about these recommendations. Building on these insights, we developed a supervised alignment method that addresses superficial matching and institutional biases in catalog descriptions, achieving a 5.5-fold improvement in accuracy over previous methods. Based on articulation predictions of this method and a 61% average surveyed adoption rate among faculty and staff, these findings project a 12-fold increase in valid credit mobility opportunities that would otherwise remain unrealized. This study suggests that stakeholder-informed design of AI in higher education administration can expand student credit mobility and help reshape current institutional decision-making in course articulation.",
      "url": "https://arxiv.org/pdf/2601.05666v1",
      "date": "2026-01-09T09:39:12+00:00"
    },
    {
      "id": "auth_yerin_kwak",
      "name": "Yerin Kwak",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_siddharth_adelkar",
      "name": "Siddharth Adelkar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_zachary_a_pardos",
      "name": "Zachary A. Pardos",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_student_credit",
      "name": "student credit",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_course_articulation",
      "name": "course articulation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_articulations_institutions",
      "name": "articulations institutions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ensuring_credits",
      "name": "ensuring credits",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_candidate_articulations",
      "name": "candidate articulations",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05574v1",
      "name": "Research Integrity and Academic Authority in the Age of Artificial Intelligence: From Discovery to Curation?",
      "group": "paper",
      "val": 30,
      "abstract": "Artificial intelligence is reshaping the organization and practice of research in ways that extend far beyond gains in productivity. AI systems now accelerate discovery, reorganize scholarly labour, and mediate access to expanding scientific literatures. At the same time, generative models capable of producing text, images, and data at scale introduce new epistemic and institutional vulnerabilities. They exacerbate challenges of reproducibility, blur lines of authorship and accountability, and place unprecedented pressure on peer review and editorial systems. These risks coincide with a deeper political-economic shift: the centre of gravity in AI research has moved decisively from universities to private laboratories with privileged access to data, compute, and engineering talent. As frontier models become increasingly proprietary and opaque, universities face growing difficulty interrogating, reproducing, or contesting the systems on which scientific inquiry increasingly depends.   This article argues that these developments challenge research integrity and erode traditional bases of academic authority, understood as the institutional capacity to render knowledge credible, contestable, and independent of concentrated power. Rather than competing with corporate laboratories at the technological frontier, universities can sustain their legitimacy by strengthening roles that cannot be readily automated or commercialized: exercising judgement over research quality in an environment saturated with synthetic outputs; curating the provenance, transparency, and reproducibility of knowledge; and acting as ethical and epistemic counterweights to private interests. In an era of informational abundance, the future authority of universities lies less in maximizing discovery alone than in sustaining the institutional conditions under which knowledge can be trusted and publicly valued.",
      "url": "https://arxiv.org/pdf/2601.05574v1",
      "date": "2026-01-09T06:47:01+00:00"
    },
    {
      "id": "auth_simon_chesterman",
      "name": "Simon Chesterman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_loy_hui_chieh",
      "name": "Loy Hui Chieh",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_academic_authority",
      "name": "academic authority",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_research_integrity",
      "name": "research integrity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scholarly_labour",
      "name": "scholarly labour",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scholarly",
      "name": "scholarly",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_institutional_capacity",
      "name": "institutional capacity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05384v1",
      "name": "Conformity and Social Impact on AI Agents",
      "group": "paper",
      "val": 30,
      "abstract": "As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.",
      "url": "https://arxiv.org/pdf/2601.05384v1",
      "date": "2026-01-08T21:16:28+00:00"
    },
    {
      "id": "auth_alessandro_bellina",
      "name": "Alessandro Bellina",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_giordano_de_marzo",
      "name": "Giordano De Marzo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_david_garcia",
      "name": "David Garcia",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_collective_ai",
      "name": "collective ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_agent",
      "name": "ai agent",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_investigate_ai",
      "name": "investigate ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_reveal_ai",
      "name": "reveal ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05358v1",
      "name": "The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques",
      "group": "paper",
      "val": 30,
      "abstract": "Public debates about \"left-\" or \"right-wing\" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a \"table of media-bias elements\". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.",
      "url": "https://arxiv.org/pdf/2601.05358v1",
      "date": "2026-01-08T20:18:55+00:00"
    },
    {
      "id": "auth_tim_menzner",
      "name": "Tim Menzner",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jochen_l_leidner",
      "name": "Jochen L. Leidner",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_media_bias",
      "name": "media bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bias_propaganda",
      "name": "bias propaganda",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bias_types",
      "name": "bias types",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_fact_bias",
      "name": "fact bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_political_spectrum",
      "name": "political spectrum",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05307v1",
      "name": "The LLM Mirage: Economic Interests and the Subversion of Weaponization Controls",
      "group": "paper",
      "val": 30,
      "abstract": "U.S. AI security policy is increasingly shaped by an $\\textit{LLM Mirage}$, the belief that national security risks scale in proportion to the compute used to train frontier language models. That premise fails in two ways. It miscalibrates strategy because adversaries can obtain weaponizable capabilities with task-specific systems that use specialized data, algorithmic efficiency, and widely available hardware, while compute controls harden only a high-end perimeter. It also destabilizes regulation because, absent a settled definition of \"AI weaponization,\" compute thresholds are easily renegotiated as domestic priorities shift, turning security policy into a proxy contest over industrial competitiveness. We analyze how the LLM Mirage took hold, propose an intent-and-capability definition of AI weaponization grounded in effects and international humanitarian law, and outline measurement infrastructure based on live benchmarks across the full AI Triad (data, algorithms, compute) for weaponization-relevant capabilities.",
      "url": "https://arxiv.org/pdf/2601.05307v1",
      "date": "2026-01-08T18:59:47+00:00"
    },
    {
      "id": "auth_ritwik_gupta",
      "name": "Ritwik Gupta",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_andrew_w_reddie",
      "name": "Andrew W. Reddie",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_weaponization",
      "name": "ai weaponization",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_security",
      "name": "ai security",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_national_security",
      "name": "national security",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_weaponization_compute",
      "name": "weaponization compute",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_weaponizable_capabilities",
      "name": "weaponizable capabilities",
      "group": "topic",
      "val": 10
    }
  ],
  "links": [
    {
      "source": "2601.04175v1",
      "target": "auth_noam_kolt",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_nicholas_caputo",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jack_boeglin",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_cullen_o'keefe",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_rishi_bommasani",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_stephen_casper",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_mariano_florentino_cuéllar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noah_feldman",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_gillian_k_hadfield",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_lewis_hammond",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_peter_henderson",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_atoosa_kasirzadeh",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_seth_lazar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_anka_reuel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_kevin_l_wei",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jonathan_zittrain",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_alignment",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_compliance",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_exploring_legal",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_specifying_ai",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_ensuring_ai",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "auth_ruiyi_guo",
      "value": 5
    },
    {
      "source": "2601.04107v1",
      "target": "auth_bodong_zhang",
      "value": 5
    },
    {
      "source": "2601.04107v1",
      "target": "topic_institutional_logics",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_ai_governance",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_governs_ai",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_intelligence_governance",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_govern_ontologically",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "auth_tom_deckenbrunnen",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_alessio_buscemi",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_marco_almada",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_alfredo_capozucca",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_german_castignani",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "topic_eu_ai",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_ai_act",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_enforcement_macro",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_govern_ai",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_ai_technical",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "auth_anamaria_mojica_hanke",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_thomas_goger",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_svenja_wölfel",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_brian_valerius",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_steffen_herbold",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "topic_offense_individuals",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_criminal_liability",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_legal_consequences",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_criminal_legal",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_legal_systems",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "auth_xiaoxian_shen",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_yuhui_zhang",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_sahithi_ankireddy",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_xiaohan_wang",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_maya_varma",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_henry_guo",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_curtis_langlotz",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_serena_yeung_levy",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "topic_multimodal_reasoning",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_medical_ai",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_radiology_image",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_understanding_radiology",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_raddiff_multimodal",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "auth_sarah_spiekermann_hoff",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_marc_langheinrich",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_johannes_hoff",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_christiane_wendehorst",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_jürgen_pfeffer",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_thomas_fuchs",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_armin_grunwald",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital_world",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital_technologies",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_ethical_psychological",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_rules_digital",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "auth_junaid_qadir",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_muhammad_adil_attique",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_saleha_shoaib",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_syed_ibrahim_ghaznavi",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "topic_examines_ai",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_coaching_chatbot",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_ai_technical",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_intellectual",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_ai_chatbots",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "auth_kunpeng_wang",
      "value": 5
    },
    {
      "source": "2601.03547v1",
      "target": "auth_jiahui_hu",
      "value": 5
    },
    {
      "source": "2601.03547v1",
      "target": "topic_ai_china",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_ai_capital",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_china_economy",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_growth_ai",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_capital_equilibrium",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "auth_nadav_kunievsky",
      "value": 5
    },
    {
      "source": "2601.03469v1",
      "target": "auth_pedro_pertusi",
      "value": 5
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_variation",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_rewrites",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_persuasive_essays",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_writing_scores",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_content",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "auth_aron_gohr",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_marie_amelie_lawn",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_kevin_gao",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_inigo_serjeant",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_stephen_heslip",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "topic_tutoring_systems",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_intelligent_tutoring",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_tutoring",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_homework_platform",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_editable_programming",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "auth_mohamed_ouf",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_shayan_noei",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_zeph_van_iterson",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_mariam_guizani",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_ying_zou",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "topic_contributors_oss4sg",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_contributors_code",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_github",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_oss_communities",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_oss_projects",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "auth_jacob_erickson",
      "value": 5
    },
    {
      "source": "2601.03222v1",
      "target": "topic_trust_ai",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_autonomy_trust",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_conversational_ai",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_ai_agents",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_focusing_trust",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noam_kolt",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_nicholas_caputo",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jack_boeglin",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_cullen_o'keefe",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_rishi_bommasani",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_stephen_casper",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_mariano_florentino_cuéllar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noah_feldman",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_gillian_k_hadfield",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_lewis_hammond",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_peter_henderson",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_atoosa_kasirzadeh",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_seth_lazar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_anka_reuel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_kevin_l_wei",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jonathan_zittrain",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_alignment",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_compliance",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_exploring_legal",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_specifying_ai",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_ensuring_ai",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_gilda",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_dungarwal",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_a_thongkham",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_e_t_ajayi",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_s_choudhary",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_t_m_terol",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_c_lam",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_j_p_araujo",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_m_mcfadyen_mungalln",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_l_s_liebovitch",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_t_coleman",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_h_west",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_k_sieck",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_s_carter",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "topic_news_social",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_social_media",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_news_dataset",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_videos_social",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_media_youtube",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "auth_thomas_le_goff",
      "value": 5
    },
    {
      "source": "2601.04958v1",
      "target": "topic_sustainability_laws",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_regulation_assess",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_regulatory_landscape",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_sustainability_reporting",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_ai_environmental",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "auth_trevor_de_clark",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "auth_yulia_bobkova",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "auth_ajay_kumar_shrestha",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_usability",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_self",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_guidance",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_compliant_privacy",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_standards",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "auth_kenzo_soares_seto",
      "value": 5
    },
    {
      "source": "2601.05961v1",
      "target": "topic_examines_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_digital_sovereignty",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_sociotechnical_imaginaries",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_idea_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "auth_tianshi_li",
      "value": 5
    },
    {
      "source": "2601.05918v1",
      "target": "topic_interviewer_ai",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_identifying_interviewees",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_interviewees",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_anthropic_interviewer",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_search_agentic",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "auth_michael_henry_tessler",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_georgina_evans",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_michiel_a_bakker",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_sophie_bridgers",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_rishub_jain",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_raphael_koster",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_verena_rieser",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_anca_dragan",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_matthew_botvinick",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_christopher_summerfield",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "topic_mediated_deliberation",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_enhancing",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_political",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_meaningful_deliberation",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_augmentation",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "auth_jakub_harasta",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_matej_vasina",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_martin_kornel",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_tomas_foltynek",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_contexts",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_family_law",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_self",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_scenario_gendered",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_guidance",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "auth_íris_damião",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_joão_franco",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_mariana_silva",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_paulo_almeida",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_pedro_c_magalhães",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_joana_gonçalves_sá",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "topic_campaign_media",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_elections_analyzed",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_electoral_projections",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_different_political",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_media_influence",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "auth_yerin_kwak",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "auth_siddharth_adelkar",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "auth_zachary_a_pardos",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "topic_student_credit",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_course_articulation",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_articulations_institutions",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_ensuring_credits",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_candidate_articulations",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "auth_simon_chesterman",
      "value": 5
    },
    {
      "source": "2601.05574v1",
      "target": "auth_loy_hui_chieh",
      "value": 5
    },
    {
      "source": "2601.05574v1",
      "target": "topic_academic_authority",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_research_integrity",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_scholarly_labour",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_scholarly",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_institutional_capacity",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "auth_alessandro_bellina",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "auth_giordano_de_marzo",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "auth_david_garcia",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "topic_collective_ai",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_ai_agents",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_ai_agent",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_investigate_ai",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_reveal_ai",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "auth_tim_menzner",
      "value": 5
    },
    {
      "source": "2601.05358v1",
      "target": "auth_jochen_l_leidner",
      "value": 5
    },
    {
      "source": "2601.05358v1",
      "target": "topic_media_bias",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_bias_propaganda",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_bias_types",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_fact_bias",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_political_spectrum",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "auth_ritwik_gupta",
      "value": 5
    },
    {
      "source": "2601.05307v1",
      "target": "auth_andrew_w_reddie",
      "value": 5
    },
    {
      "source": "2601.05307v1",
      "target": "topic_ai_weaponization",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_ai_security",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_national_security",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_weaponization_compute",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_weaponizable_capabilities",
      "value": 2
    }
  ]
}