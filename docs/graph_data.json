{
  "nodes": [
    {
      "id": "2601.04107v1",
      "name": "From Abstract Threats to Institutional Realities: A Comparative Semantic Network Analysis of AI Securitisation in the US, EU, and China",
      "group": "paper",
      "val": 30,
      "abstract": "Artificial intelligence governance exhibits a striking paradox: while major jurisdictions converge rhetorically around concepts such as safety, risk, and accountability, their regulatory frameworks remain fundamentally divergent and mutually unintelligible. This paper argues that this fragmentation cannot be explained solely by geopolitical rivalry, institutional complexity, or instrument selection. Instead, it stems from how AI is constituted as an object of governance through distinct institutional logics. Integrating securitisation theory with the concept of the dispositif, we demonstrate that jurisdictions govern ontologically different objects under the same vocabulary. Using semantic network analysis of official policy texts from the European Union, the United States, and China (2023-2025), we trace how concepts like safety are embedded within divergent semantic architectures. Our findings reveal that the EU juridifies AI as a certifiable product through legal-bureaucratic logic; the US operationalises AI as an optimisable system through market-liberal logic; and China governs AI as socio-technical infrastructure through holistic state logic. We introduce the concept of structural incommensurability to describe this condition of ontological divergence masked by terminological convergence. This reframing challenges ethics-by-principles approaches to global AI governance, suggesting that coordination failures arise not from disagreement over values but from the absence of a shared reference object.",
      "url": "https://arxiv.org/pdf/2601.04107v1",
      "date": "2026-01-07T17:12:03+00:00"
    },
    {
      "id": "auth_ruiyi_guo",
      "name": "Ruiyi Guo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_bodong_zhang",
      "name": "Bodong Zhang",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_institutional_logics",
      "name": "institutional logics",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_governance",
      "name": "ai governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_governs_ai",
      "name": "governs ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intelligence_governance",
      "name": "intelligence governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_govern_ontologically",
      "name": "govern ontologically",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04094v1",
      "name": "The Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning",
      "group": "paper",
      "val": 30,
      "abstract": "The EU AI Act adopts a horizontal and adaptive approach to govern AI technologies characterised by rapid development and unpredictable emerging capabilities. To maintain relevance, the Act embeds provisions for regulatory learning. However, these provisions operate within a complex network of actors and mechanisms that lack a clearly defined technical basis for scalable information flow. This paper addresses this gap by establishing a theoretical model of regulatory learning space defined by the AI Act, decomposed into micro, meso, and macro levels. Drawing from this functional perspective of this model, we situate the diverse stakeholders - ranging from the EU Commission at the macro level to AI developers at the micro level - within the transitions of enforcement (macro-micro) and evidence aggregation (micro-macro). We identify AI Technical Sandboxes as the essential engine for evidence generation at the micro level, providing the necessary data to drive scalable learning across all levels of the model. By providing an extensive discussion of the requirements and challenges for AITSes to serve as this micro-level evidence generator, we aim to bridge the gap between legislative commands and technical operationalisation, thereby enabling a structured discourse between technical and legal experts.",
      "url": "https://arxiv.org/pdf/2601.04094v1",
      "date": "2026-01-07T17:01:06+00:00"
    },
    {
      "id": "auth_tom_deckenbrunnen",
      "name": "Tom Deckenbrunnen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_alessio_buscemi",
      "name": "Alessio Buscemi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marco_almada",
      "name": "Marco Almada",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_alfredo_capozucca",
      "name": "Alfredo Capozucca",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_german_castignani",
      "name": "German Castignani",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_eu_ai",
      "name": "eu ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_act",
      "name": "ai act",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_enforcement_macro",
      "name": "enforcement macro",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_govern_ai",
      "name": "govern ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_technical",
      "name": "ai technical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03788v1",
      "name": "Criminal Liability of Generative Artificial Intelligence Providers for User-Generated Child Sexual Abuse Material",
      "group": "paper",
      "val": 30,
      "abstract": "The development of more powerful Generative Artificial Intelligence (GenAI) has expanded its capabilities and the variety of outputs. This has introduced significant legal challenges, including gray areas in various legal systems, such as the assessment of criminal liability for those responsible for these models. Therefore, we conducted a multidisciplinary study utilizing the statutory interpretation of relevant German laws, which, in conjunction with scenarios, provides a perspective on the different properties of GenAI in the context of Child Sexual Abuse Material (CSAM) generation. We found that generating CSAM with GenAI may have criminal and legal consequences not only for the user committing the primary offense but also for individuals responsible for the models, such as independent software developers, researchers, and company representatives. Additionally, the assessment of criminal liability may be affected by contextual and technical factors, including the type of generated image, content moderation policies, and the model's intended purpose. Based on our findings, we discussed the implications for different roles, as well as the requirements when developing such systems.",
      "url": "https://arxiv.org/pdf/2601.03788v1",
      "date": "2026-01-07T10:38:35+00:00"
    },
    {
      "id": "auth_anamaria_mojica_hanke",
      "name": "Anamaria Mojica-Hanke",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_thomas_goger",
      "name": "Thomas Goger",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_svenja_wölfel",
      "name": "Svenja Wölfel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_brian_valerius",
      "name": "Brian Valerius",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_steffen_herbold",
      "name": "Steffen Herbold",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_offense_individuals",
      "name": "offense individuals",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_criminal_liability",
      "name": "criminal liability",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_consequences",
      "name": "legal consequences",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_criminal_legal",
      "name": "criminal legal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_systems",
      "name": "legal systems",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03733v1",
      "name": "RadDiff: Describing Differences in Radiology Image Sets with Natural Language",
      "group": "paper",
      "val": 30,
      "abstract": "Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.",
      "url": "https://arxiv.org/pdf/2601.03733v1",
      "date": "2026-01-07T09:25:04+00:00"
    },
    {
      "id": "auth_xiaoxian_shen",
      "name": "Xiaoxian Shen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yuhui_zhang",
      "name": "Yuhui Zhang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sahithi_ankireddy",
      "name": "Sahithi Ankireddy",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_xiaohan_wang",
      "name": "Xiaohan Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_maya_varma",
      "name": "Maya Varma",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_henry_guo",
      "name": "Henry Guo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_curtis_langlotz",
      "name": "Curtis Langlotz",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_serena_yeung_levy",
      "name": "Serena Yeung-Levy",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_multimodal_reasoning",
      "name": "multimodal reasoning",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_medical_ai",
      "name": "medical ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_radiology_image",
      "name": "radiology image",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_understanding_radiology",
      "name": "understanding radiology",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_raddiff_multimodal",
      "name": "raddiff multimodal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03709v1",
      "name": "The Power of 10: New Rules for the Digital World",
      "group": "paper",
      "val": 30,
      "abstract": "As artificial intelligence rapidly advances, society is increasingly captivated by promises of superhuman machines and seamless digital futures. Yet these visions often obscure mounting social, ethical, and psychological concerns tied to pervasive digital technologies - from surveillance to mental health crises. This article argues that a guiding ethos is urgently needed to navigate these transformations. Inspired by the lasting influence of the biblical Ten Commandments, a European interdisciplinary group has proposed \"Ten Rules for the Digital World\" - a novel ethical framework to help individuals and societies make prudent, human-centered decisions in the age of \"supercharged\" technology.",
      "url": "https://arxiv.org/pdf/2601.03709v1",
      "date": "2026-01-07T08:49:02+00:00"
    },
    {
      "id": "auth_sarah_spiekermann_hoff",
      "name": "Sarah Spiekermann-Hoff",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marc_langheinrich",
      "name": "Marc Langheinrich",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_johannes_hoff",
      "name": "Johannes Hoff",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_christiane_wendehorst",
      "name": "Christiane Wendehorst",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jürgen_pfeffer",
      "name": "Jürgen Pfeffer",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_thomas_fuchs",
      "name": "Thomas Fuchs",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_armin_grunwald",
      "name": "Armin Grunwald",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_digital_world",
      "name": "digital world",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital_technologies",
      "name": "digital technologies",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ethical_psychological",
      "name": "ethical psychological",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_rules_digital",
      "name": "rules digital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital",
      "name": "digital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03693v1",
      "name": "Can AI Chatbots Provide Coaching in Engineering? Beyond Information Processing Toward Mastery",
      "group": "paper",
      "val": 30,
      "abstract": "Engineering education faces a double disruption: traditional apprenticeship models that cultivated judgment and tacit skill are eroding, just as generative AI emerges as an informal coaching partner. This convergence rekindles long-standing questions in the philosophy of AI and cognition about the limits of computation, the nature of embodied rationality, and the distinction between information processing and wisdom. Building on this rich intellectual tradition, this paper examines whether AI chatbots can provide coaching that fosters mastery rather than merely delivering information. We synthesize critical perspectives from decades of scholarship on expertise, tacit knowledge, and human-machine interaction, situating them within the context of contemporary AI-driven education. Empirically, we report findings from a mixed-methods study (N = 75 students, N = 7 faculty) exploring the use of a coaching chatbot in engineering education. Results reveal a consistent boundary: participants accept AI for technical problem solving (convergent tasks; M = 3.84 on a 1-5 Likert scale) but remain skeptical of its capacity for moral, emotional, and contextual judgment (divergent tasks). Faculty express stronger concerns over risk (M = 4.71 vs. M = 4.14, p = 0.003), and privacy emerges as a key requirement, with 64-71 percent of participants demanding strict confidentiality. Our findings suggest that while generative AI can democratize access to cognitive and procedural support, it cannot replicate the embodied, value-laden dimensions of human mentorship. We propose a multiplex coaching framework that integrates human wisdom within expert-in-the-loop models, preserving the depth of apprenticeship while leveraging AI scalability to enrich the next generation of engineering education.",
      "url": "https://arxiv.org/pdf/2601.03693v1",
      "date": "2026-01-07T08:28:47+00:00"
    },
    {
      "id": "auth_junaid_qadir",
      "name": "Junaid Qadir",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_muhammad_adil_attique",
      "name": "Muhammad Adil Attique",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_saleha_shoaib",
      "name": "Saleha Shoaib",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_syed_ibrahim_ghaznavi",
      "name": "Syed Ibrahim Ghaznavi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_examines_ai",
      "name": "examines ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_coaching_chatbot",
      "name": "coaching chatbot",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intellectual",
      "name": "intellectual",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_chatbots",
      "name": "ai chatbots",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03547v1",
      "name": "Governance of Technological Transition: A Predator-Prey Analysis of AI Capital in China's Economy and Its Policy Implications",
      "group": "paper",
      "val": 30,
      "abstract": "The rapid integration of Artificial Intelligence (AI) into China's economy presents a classic governance challenge: how to harness its growth potential while managing its disruptive effects on traditional capital and labor markets. This study addresses this policy dilemma by modeling the dynamic interactions between AI capital, physical capital, and labor within a Lotka-Volterra predator-prey framework. Using annual Chinese data (2016-2023), we quantify the interaction strengths, identify stable equilibria, and perform a global sensitivity analysis. Our results reveal a consistent pattern where AI capital acts as the 'prey', stimulating both physical capital accumulation and labor compensation (wage bill), while facing only weak constraining feedback. The equilibrium points are stable nodes, indicating a policy-mediated convergence path rather than volatile cycles. Critically, the sensitivity analysis shows that the labor market equilibrium is overwhelmingly driven by AI-related parameters, whereas the physical capital equilibrium is also influenced by its own saturation dynamics. These findings provide a systemic, quantitative basis for policymakers: (1) to calibrate AI promotion policies by recognizing the asymmetric leverage points in capital vs. labor markets; (2) to anticipate and mitigate structural rigidities that may arise from current regulatory settings; and (3) to prioritize interventions that foster complementary growth between AI and traditional economic structures while ensuring broad-base distribution of technological gains.",
      "url": "https://arxiv.org/pdf/2601.03547v1",
      "date": "2026-01-07T03:30:46+00:00"
    },
    {
      "id": "auth_kunpeng_wang",
      "name": "Kunpeng Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jiahui_hu",
      "name": "Jiahui Hu",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_china",
      "name": "ai china",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_capital",
      "name": "ai capital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_china_economy",
      "name": "china economy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_growth_ai",
      "name": "growth ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_capital_equilibrium",
      "name": "capital equilibrium",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03469v1",
      "name": "Content vs. Form: What Drives the Writing Score Gap Across Socioeconomic Backgrounds? A Generated Panel Approach",
      "group": "paper",
      "val": 30,
      "abstract": "Students from different socioeconomic backgrounds exhibit persistent gaps in test scores, gaps that can translate into unequal educational and labor-market outcomes later in life. In many assessments, performance reflects not only what students know, but also how effectively they can communicate that knowledge. This distinction is especially salient in writing assessments, where scores jointly reward the substance of students' ideas and the way those ideas are expressed. As a result, observed score gaps may conflate differences in underlying content with differences in expressive skill. A central question, therefore, is how much of the socioeconomic-status (SES) gap in scores is driven by differences in what students say versus how they say it. We study this question using a large corpus of persuasive essays written by U.S. middle- and high-school students. We introduce a new measurement strategy that separates content from style by leveraging large language models to generate multiple stylistic variants of each essay. These rewrites preserve the underlying arguments while systematically altering surface expression, creating a \"generated panel\" that introduces controlled within-essay variation in style. This approach allows us to decompose SES gaps in writing scores into contributions from content and style. We find an SES gap of 0.67 points on a 1-6 scale. Approximately 69% of the gap is attributable to differences in essay content quality, Style differences account for 26% of the gap, and differences in evaluation standards across SES groups account for the remaining 5%. These patterns seems stable across demographic subgroups and writing tasks. More broadly, our approach shows how large language models can be used to generate controlled variation in observational data, enabling researchers to isolate and quantify the contributions of otherwise entangled factors.",
      "url": "https://arxiv.org/pdf/2601.03469v1",
      "date": "2026-01-06T23:45:18+00:00"
    },
    {
      "id": "auth_nadav_kunievsky",
      "name": "Nadav Kunievsky",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_pedro_pertusi",
      "name": "Pedro Pertusi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_essay_variation",
      "name": "essay variation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_essay_rewrites",
      "name": "essay rewrites",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_persuasive_essays",
      "name": "persuasive essays",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_writing_scores",
      "name": "writing scores",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_essay_content",
      "name": "essay content",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03458v1",
      "name": "Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant",
      "group": "paper",
      "val": 30,
      "abstract": "Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging.   We present a system that processes free-form natural language input, handles a wide range of edge cases, and comments competently not only on the technical correctness of submitted proofs, but also on style and presentation issues. We discuss the advantages and disadvantages of various approaches to the evaluation of such a system, and show that by the metrics we evaluate, the quality of the feedback generated is comparable to that produced by human experts when assessing early undergraduate homework. We stress-test our system with a small set of more advanced and unusual questions, and report both significant gaps and encouraging successes in that more challenging setting.   Our system uses large language models in a modular workflow. The workflow configuration is human-readable and editable without programming knowledge, and allows some intermediate steps to be precomputed or injected by the instructor.   A version of our tool is deployed on the Imperial mathematics homework platform Lambdafeedback. We report also on the integration of our tool into this platform.",
      "url": "https://arxiv.org/pdf/2601.03458v1",
      "date": "2026-01-06T23:02:22+00:00"
    },
    {
      "id": "auth_aron_gohr",
      "name": "Aron Gohr",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marie_amelie_lawn",
      "name": "Marie-Amelie Lawn",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kevin_gao",
      "name": "Kevin Gao",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_inigo_serjeant",
      "name": "Inigo Serjeant",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_stephen_heslip",
      "name": "Stephen Heslip",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_tutoring_systems",
      "name": "tutoring systems",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intelligent_tutoring",
      "name": "intelligent tutoring",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_tutoring",
      "name": "tutoring",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_homework_platform",
      "name": "homework platform",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_editable_programming",
      "name": "editable programming",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03430v1",
      "name": "An Empirical Analysis of Community and Coding Patterns in OSS4SG vs. Conventional OSS",
      "group": "paper",
      "val": 30,
      "abstract": "Open Source Software for Social Good (OSS4SG) projects aim to address critical societal challenges, such as healthcare access and community safety. Understanding the community dynamics and contributor patterns in these projects is essential for ensuring their sustainability and long-term impact. However, while extensive research has focused on conventional Open Source Software (OSS), little is known about how the mission-driven nature of OSS4SG influences its development practices. To address this gap, we conduct a large-scale empirical study of 1,039 GitHub repositories, comprising 422 OSS4SG and 617 conventional OSS projects, to compare community structure, contributor engagement, and coding practices. Our findings reveal that OSS4SG projects foster significantly more stable and \"sticky\" (63.4%) communities, whereas conventional OSS projects are more \"magnetic\" (75.4%), attracting a high turnover of contributors. OSS4SG projects also demonstrate consistent engagement throughout the year, while conventional OSS communities exhibit seasonal fluctuations. Additionally, OSS4SG projects rely heavily on core contributors for both code quality and issue resolution, while conventional OSS projects leverage casual contributors for issue resolution, with core contributors focusing primarily on code quality.",
      "url": "https://arxiv.org/pdf/2601.03430v1",
      "date": "2026-01-06T21:37:12+00:00"
    },
    {
      "id": "auth_mohamed_ouf",
      "name": "Mohamed Ouf",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_shayan_noei",
      "name": "Shayan Noei",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_zeph_van_iterson",
      "name": "Zeph Van Iterson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariam_guizani",
      "name": "Mariam Guizani",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ying_zou",
      "name": "Ying Zou",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_contributors_oss4sg",
      "name": "contributors oss4sg",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_contributors_code",
      "name": "contributors code",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_github",
      "name": "github",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_oss_communities",
      "name": "oss communities",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_oss_projects",
      "name": "oss projects",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03222v1",
      "name": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI",
      "group": "paper",
      "val": 30,
      "abstract": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness.",
      "url": "https://arxiv.org/pdf/2601.03222v1",
      "date": "2026-01-06T18:07:52+00:00"
    },
    {
      "id": "auth_jacob_erickson",
      "name": "Jacob Erickson",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_trust_ai",
      "name": "trust ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_autonomy_trust",
      "name": "autonomy trust",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_conversational_ai",
      "name": "conversational ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_agents",
      "name": "ai agents",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_focusing_trust",
      "name": "focusing trust",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04175v1",
      "name": "Legal Alignment for Safe and Ethical AI",
      "group": "paper",
      "val": 30,
      "abstract": "Alignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law. In this paper, we aim to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging field -- legal alignment -- focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.",
      "url": "https://arxiv.org/pdf/2601.04175v1",
      "date": "2026-01-07T18:42:04+00:00"
    },
    {
      "id": "auth_noam_kolt",
      "name": "Noam Kolt",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_nicholas_caputo",
      "name": "Nicholas Caputo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jack_boeglin",
      "name": "Jack Boeglin",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_cullen_o'keefe",
      "name": "Cullen O'Keefe",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_rishi_bommasani",
      "name": "Rishi Bommasani",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_stephen_casper",
      "name": "Stephen Casper",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariano_florentino_cuéllar",
      "name": "Mariano-Florentino Cuéllar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_noah_feldman",
      "name": "Noah Feldman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_iason_gabriel",
      "name": "Iason Gabriel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_gillian_k_hadfield",
      "name": "Gillian K. Hadfield",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_lewis_hammond",
      "name": "Lewis Hammond",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_peter_henderson",
      "name": "Peter Henderson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_atoosa_kasirzadeh",
      "name": "Atoosa Kasirzadeh",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_seth_lazar",
      "name": "Seth Lazar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_anka_reuel",
      "name": "Anka Reuel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kevin_l_wei",
      "name": "Kevin L. Wei",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jonathan_zittrain",
      "name": "Jonathan Zittrain",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_legal_alignment",
      "name": "legal alignment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_compliance",
      "name": "legal compliance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_exploring_legal",
      "name": "exploring legal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_specifying_ai",
      "name": "specifying ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ensuring_ai",
      "name": "ensuring ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05232v1",
      "name": "Measuring and Fostering Peace through Machine Learning and Artificial Intelligence",
      "group": "paper",
      "val": 30,
      "abstract": "We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.",
      "url": "https://arxiv.org/pdf/2601.05232v1",
      "date": "2026-01-08T18:57:01+00:00"
    },
    {
      "id": "auth_p_gilda",
      "name": "P. Gilda",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_p_dungarwal",
      "name": "P. Dungarwal",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_a_thongkham",
      "name": "A. Thongkham",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_e_t_ajayi",
      "name": "E. T. Ajayi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_s_choudhary",
      "name": "S. Choudhary",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_t_m_terol",
      "name": "T. M. Terol",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_c_lam",
      "name": "C. Lam",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_j_p_araujo",
      "name": "J. P. Araujo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_m_mcfadyen_mungalln",
      "name": "M. McFadyen-Mungalln",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_l_s_liebovitch",
      "name": "L. S. Liebovitch",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_p_t_coleman",
      "name": "P. T. Coleman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_h_west",
      "name": "H. West",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_k_sieck",
      "name": "K. Sieck",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_s_carter",
      "name": "S. Carter",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_news_social",
      "name": "news social",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_social_media",
      "name": "social media",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_news_dataset",
      "name": "news dataset",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_videos_social",
      "name": "videos social",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_media_youtube",
      "name": "media youtube",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04958v1",
      "name": "The unsuitability of existing regulations to reach sustainable AI",
      "group": "paper",
      "val": 30,
      "abstract": "This paper examines the European Union's emerging regulatory landscape - focusing on the AI Act, corporate sustainability reporting and due diligence regimes (CSRD and CSDDD), and data center regulation - to assess whether it can effectively govern AI's environmental footprint. We argue that, despite incremental progress, current approaches remain ill-suited to correcting the market failures underpinning AI-related energy use, water consumption, and material demand. Key shortcomings include narrow disclosure requirements, excessive reliance on voluntary standards, weak enforcement mechanisms, and a structural disconnect between AI-specific impacts and broader sustainability laws. The analysis situates these regulatory gaps within a wider ecosystem of academic research, civil society advocacy, standard-setting, and industry initiatives, highlighting risks of regulatory capture and greenwashing. Building on this diagnosis, the paper advances strategic recommendations for the COP30 Action Agenda, calling for binding transparency obligations, harmonized international standards for lifecycle assessment, stricter governance of data center expansion, and meaningful public participation in AI infrastructure decisions.",
      "url": "https://arxiv.org/pdf/2601.04958v1",
      "date": "2026-01-08T14:02:51+00:00"
    },
    {
      "id": "auth_thomas_le_goff",
      "name": "Thomas Le Goff",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_sustainability_laws",
      "name": "sustainability laws",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regulation_assess",
      "name": "regulation assess",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regulatory_landscape",
      "name": "regulatory landscape",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sustainability_reporting",
      "name": "sustainability reporting",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_environmental",
      "name": "ai environmental",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04403v1",
      "name": "Balancing Usability and Compliance in AI Smart Devices: A Privacy-by-Design Audit of Google Home, Alexa, and Siri",
      "group": "paper",
      "val": 30,
      "abstract": "This paper investigates the privacy and usability of AI-enabled smart devices commonly used by youth, focusing on Google Home Mini, Amazon Alexa, and Apple Siri. While these devices provide convenience and efficiency, they also raise privacy and transparency concerns due to their always-listening design and complex data management processes. The study proposes and applies a combined framework of Heuristic Evaluation, Personal Information Protection and Electronic Documents Act (PIPEDA) Compliance Assessment, and Youth-Centered Usability Testing to assess whether these devices align with Privacy-by-Design principles and support meaningful user control. Results show that Google Home achieved the highest usability score, while Siri scored highest in regulatory compliance, indicating a trade-off between user convenience and privacy protection. Alexa demonstrated clearer task navigation but weaker transparency in data retention. Findings suggest that although youth may feel capable of managing their data, their privacy self-efficacy remains limited by technical design, complex settings, and unclear data policies. The paper concludes that enhancing transparency, embedding privacy guidance during onboarding, and improving policy alignment are critical steps toward ensuring that smart devices are both usable and compliant with privacy standards that protect young users.",
      "url": "https://arxiv.org/pdf/2601.04403v1",
      "date": "2026-01-07T21:20:58+00:00"
    },
    {
      "id": "auth_trevor_de_clark",
      "name": "Trevor De Clark",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yulia_bobkova",
      "name": "Yulia Bobkova",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ajay_kumar_shrestha",
      "name": "Ajay Kumar Shrestha",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_privacy_usability",
      "name": "privacy usability",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_self",
      "name": "privacy self",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_guidance",
      "name": "privacy guidance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_compliant_privacy",
      "name": "compliant privacy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_standards",
      "name": "privacy standards",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05961v1",
      "name": "Navigating the Sociotechnical Imaginaries of Brazilian Tech Workers",
      "group": "paper",
      "val": 30,
      "abstract": "This chapter examines the sociotechnical imaginaries of Brazilian tech workers, a group often overlooked in digital labor research despite their role in designing the digital systems that shape everyday life. Grounded in the idea of sociotechnical imaginaries as collectively constructed visions that guide technology development and governance, the chapter argues that looking from the Global South helps challenge data universalism and foregrounds locally situated values, constraints, and futures. Drawing on semi-structured interviews with 26 Brazilian professionals conducted between July and December 2023, it maps how workers make sense of responsibility, bias, and power in AI and platform development. The findings highlight recurring tensions between academic and industry discourse on algorithmic bias, the limits of corporate accountability regarding user harm and surveillance, and the contested meanings of digital sovereignty, including grassroots initiatives that seek alternative technological futures aligned with marginalized communities needs.",
      "url": "https://arxiv.org/pdf/2601.05961v1",
      "date": "2026-01-09T17:30:04+00:00"
    },
    {
      "id": "auth_kenzo_soares_seto",
      "name": "Kenzo Soares Seto",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_examines_sociotechnical",
      "name": "examines sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital_sovereignty",
      "name": "digital sovereignty",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sociotechnical_imaginaries",
      "name": "sociotechnical imaginaries",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sociotechnical",
      "name": "sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_idea_sociotechnical",
      "name": "idea sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05918v1",
      "name": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
      "group": "paper",
      "val": 30,
      "abstract": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
      "url": "https://arxiv.org/pdf/2601.05918v1",
      "date": "2026-01-09T16:32:33+00:00"
    },
    {
      "id": "auth_tianshi_li",
      "name": "Tianshi Li",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_interviewer_ai",
      "name": "interviewer ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_identifying_interviewees",
      "name": "identifying interviewees",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_interviewees",
      "name": "interviewees",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_anthropic_interviewer",
      "name": "anthropic interviewer",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_search_agentic",
      "name": "search agentic",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05904v1",
      "name": "Can AI mediation improve democratic deliberation?",
      "group": "paper",
      "val": 30,
      "abstract": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.",
      "url": "https://arxiv.org/pdf/2601.05904v1",
      "date": "2026-01-09T16:22:26+00:00"
    },
    {
      "id": "auth_michael_henry_tessler",
      "name": "Michael Henry Tessler",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_georgina_evans",
      "name": "Georgina Evans",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_michiel_a_bakker",
      "name": "Michiel A. Bakker",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sophie_bridgers",
      "name": "Sophie Bridgers",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_rishub_jain",
      "name": "Rishub Jain",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_raphael_koster",
      "name": "Raphael Koster",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_verena_rieser",
      "name": "Verena Rieser",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_anca_dragan",
      "name": "Anca Dragan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matthew_botvinick",
      "name": "Matthew Botvinick",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_christopher_summerfield",
      "name": "Christopher Summerfield",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_mediated_deliberation",
      "name": "mediated deliberation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_enhancing",
      "name": "deliberation enhancing",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_political",
      "name": "deliberation political",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_meaningful_deliberation",
      "name": "meaningful deliberation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_augmentation",
      "name": "deliberation augmentation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05879v1",
      "name": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law",
      "group": "paper",
      "val": 30,
      "abstract": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.",
      "url": "https://arxiv.org/pdf/2601.05879v1",
      "date": "2026-01-09T15:55:03+00:00"
    },
    {
      "id": "auth_jakub_harasta",
      "name": "Jakub Harasta",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matej_vasina",
      "name": "Matej Vasina",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_martin_kornel",
      "name": "Martin Kornel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_tomas_foltynek",
      "name": "Tomas Foltynek",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_legal_contexts",
      "name": "legal contexts",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_family_law",
      "name": "family law",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_self",
      "name": "legal self",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scenario_gendered",
      "name": "scenario gendered",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_guidance",
      "name": "legal guidance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05826v1",
      "name": "Cross-National Evidence of Disproportionate Media Visibility for the Radical Right in the 2024 European Elections",
      "group": "paper",
      "val": 30,
      "abstract": "This study provides a systematic comparative analysis of media visibility of different political families during the 2024 European Parliament elections. We analyzed close to 21,500 unique news from leading national outlets in Austria, Germany, Ireland, Poland, and Portugal - countries with diverse political contexts and levels of media trust. Combining computational and human classification, we identified parties, political leaders, and groups from the article's URLs and titles, and clustered them according to European Parliament political families and broad political leanings. Cross-country comparison shows that the Mainstream and the Radical Right were mentioned more often than the other political groups. Moreover, the Radical Right received disproportionate attention relative to electoral results (from 2019 or 2024) and electoral projections, particularly in Austria, Germany, and Ireland. This imbalance increased in the final weeks of the campaign, when media influence on undecided voters is greatest. Outlet-level analysis shows that coverage of right-leaning entities dominated across news sources, especially those generating the highest traffic, suggesting a structural rather than outlet-specific pattern. Media visibility is a central resource, and this systematic mapping of online coverage highlights how traditional media can contribute to structural asymmetries in democratic competition.",
      "url": "https://arxiv.org/pdf/2601.05826v1",
      "date": "2026-01-09T15:00:59+00:00"
    },
    {
      "id": "auth_íris_damião",
      "name": "Íris Damião",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_joão_franco",
      "name": "João Franco",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariana_silva",
      "name": "Mariana Silva",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_paulo_almeida",
      "name": "Paulo Almeida",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_pedro_c_magalhães",
      "name": "Pedro C. Magalhães",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_joana_gonçalves_sá",
      "name": "Joana Gonçalves-Sá",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_campaign_media",
      "name": "campaign media",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_elections_analyzed",
      "name": "elections analyzed",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_electoral_projections",
      "name": "electoral projections",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_different_political",
      "name": "different political",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_media_influence",
      "name": "media influence",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05666v1",
      "name": "Advancing credit mobility through stakeholder-informed AI design and adoption",
      "group": "paper",
      "val": 30,
      "abstract": "Transferring from a 2-year to a 4-year college is crucial for socioeconomic mobility, yet students often face challenges ensuring their credits are fully recognized, leading to delays in their academic progress and unexpected costs. Determining whether courses at different institutions are equivalent (i.e., articulation) is essential for successful credit transfer, as it minimizes unused credits and increases the likelihood of bachelor's degree completion. However, establishing articulation agreements remains time- and resource-intensive, as all candidate articulations are reviewed manually. Although recent efforts have explored the use of artificial intelligence to support this work, its use in articulation practice remains limited. Given these challenges and the need for scalable support, this study applies artificial intelligence to suggest articulations between institutions in collaboration with the State University of New York system, one of the largest systems of higher education in the US. To develop our methodology, we first surveyed articulation staff and faculty to assess adoption rates of baseline algorithmic recommendations and gather feedback on perceptions and concerns about these recommendations. Building on these insights, we developed a supervised alignment method that addresses superficial matching and institutional biases in catalog descriptions, achieving a 5.5-fold improvement in accuracy over previous methods. Based on articulation predictions of this method and a 61% average surveyed adoption rate among faculty and staff, these findings project a 12-fold increase in valid credit mobility opportunities that would otherwise remain unrealized. This study suggests that stakeholder-informed design of AI in higher education administration can expand student credit mobility and help reshape current institutional decision-making in course articulation.",
      "url": "https://arxiv.org/pdf/2601.05666v1",
      "date": "2026-01-09T09:39:12+00:00"
    },
    {
      "id": "auth_yerin_kwak",
      "name": "Yerin Kwak",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_siddharth_adelkar",
      "name": "Siddharth Adelkar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_zachary_a_pardos",
      "name": "Zachary A. Pardos",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_student_credit",
      "name": "student credit",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_course_articulation",
      "name": "course articulation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_articulations_institutions",
      "name": "articulations institutions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ensuring_credits",
      "name": "ensuring credits",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_candidate_articulations",
      "name": "candidate articulations",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05574v1",
      "name": "Research Integrity and Academic Authority in the Age of Artificial Intelligence: From Discovery to Curation?",
      "group": "paper",
      "val": 30,
      "abstract": "Artificial intelligence is reshaping the organization and practice of research in ways that extend far beyond gains in productivity. AI systems now accelerate discovery, reorganize scholarly labour, and mediate access to expanding scientific literatures. At the same time, generative models capable of producing text, images, and data at scale introduce new epistemic and institutional vulnerabilities. They exacerbate challenges of reproducibility, blur lines of authorship and accountability, and place unprecedented pressure on peer review and editorial systems. These risks coincide with a deeper political-economic shift: the centre of gravity in AI research has moved decisively from universities to private laboratories with privileged access to data, compute, and engineering talent. As frontier models become increasingly proprietary and opaque, universities face growing difficulty interrogating, reproducing, or contesting the systems on which scientific inquiry increasingly depends.   This article argues that these developments challenge research integrity and erode traditional bases of academic authority, understood as the institutional capacity to render knowledge credible, contestable, and independent of concentrated power. Rather than competing with corporate laboratories at the technological frontier, universities can sustain their legitimacy by strengthening roles that cannot be readily automated or commercialized: exercising judgement over research quality in an environment saturated with synthetic outputs; curating the provenance, transparency, and reproducibility of knowledge; and acting as ethical and epistemic counterweights to private interests. In an era of informational abundance, the future authority of universities lies less in maximizing discovery alone than in sustaining the institutional conditions under which knowledge can be trusted and publicly valued.",
      "url": "https://arxiv.org/pdf/2601.05574v1",
      "date": "2026-01-09T06:47:01+00:00"
    },
    {
      "id": "auth_simon_chesterman",
      "name": "Simon Chesterman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_loy_hui_chieh",
      "name": "Loy Hui Chieh",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_academic_authority",
      "name": "academic authority",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_research_integrity",
      "name": "research integrity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scholarly_labour",
      "name": "scholarly labour",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scholarly",
      "name": "scholarly",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_institutional_capacity",
      "name": "institutional capacity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05384v1",
      "name": "Conformity and Social Impact on AI Agents",
      "group": "paper",
      "val": 30,
      "abstract": "As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.",
      "url": "https://arxiv.org/pdf/2601.05384v1",
      "date": "2026-01-08T21:16:28+00:00"
    },
    {
      "id": "auth_alessandro_bellina",
      "name": "Alessandro Bellina",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_giordano_de_marzo",
      "name": "Giordano De Marzo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_david_garcia",
      "name": "David Garcia",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_collective_ai",
      "name": "collective ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_agent",
      "name": "ai agent",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_investigate_ai",
      "name": "investigate ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_reveal_ai",
      "name": "reveal ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05358v1",
      "name": "The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques",
      "group": "paper",
      "val": 30,
      "abstract": "Public debates about \"left-\" or \"right-wing\" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a \"table of media-bias elements\". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.",
      "url": "https://arxiv.org/pdf/2601.05358v1",
      "date": "2026-01-08T20:18:55+00:00"
    },
    {
      "id": "auth_tim_menzner",
      "name": "Tim Menzner",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jochen_l_leidner",
      "name": "Jochen L. Leidner",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_media_bias",
      "name": "media bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bias_propaganda",
      "name": "bias propaganda",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bias_types",
      "name": "bias types",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_fact_bias",
      "name": "fact bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_political_spectrum",
      "name": "political spectrum",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05307v1",
      "name": "The LLM Mirage: Economic Interests and the Subversion of Weaponization Controls",
      "group": "paper",
      "val": 30,
      "abstract": "U.S. AI security policy is increasingly shaped by an $\\textit{LLM Mirage}$, the belief that national security risks scale in proportion to the compute used to train frontier language models. That premise fails in two ways. It miscalibrates strategy because adversaries can obtain weaponizable capabilities with task-specific systems that use specialized data, algorithmic efficiency, and widely available hardware, while compute controls harden only a high-end perimeter. It also destabilizes regulation because, absent a settled definition of \"AI weaponization,\" compute thresholds are easily renegotiated as domestic priorities shift, turning security policy into a proxy contest over industrial competitiveness. We analyze how the LLM Mirage took hold, propose an intent-and-capability definition of AI weaponization grounded in effects and international humanitarian law, and outline measurement infrastructure based on live benchmarks across the full AI Triad (data, algorithms, compute) for weaponization-relevant capabilities.",
      "url": "https://arxiv.org/pdf/2601.05307v1",
      "date": "2026-01-08T18:59:47+00:00"
    },
    {
      "id": "auth_ritwik_gupta",
      "name": "Ritwik Gupta",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_andrew_w_reddie",
      "name": "Andrew W. Reddie",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_weaponization",
      "name": "ai weaponization",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_security",
      "name": "ai security",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_national_security",
      "name": "national security",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_weaponization_compute",
      "name": "weaponization compute",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_weaponizable_capabilities",
      "name": "weaponizable capabilities",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07629v1",
      "name": "Fifteen Years of Learning Analytics Research: Topics, Trends, and Challenges",
      "group": "paper",
      "val": 30,
      "abstract": "The learning analytics (LA) community has recently reached two important milestones: celebrating the 15th LAK conference and updating the 2011 definition of LA to reflect the 15 years of changes in the discipline. However, despite LA's growth, little is known about how research topics, funding, and collaboration, as well as the relationships among them, have developed within the community over time. This study addressed this gap by analyzing all 936 full and short papers published at LAK over a 15-year period using unsupervised machine learning, natural language processing, and network analytics. The analysis revealed a stable core of prolific authors alongside high turnover of newcomers, systematic links between funding sources and research directions, and six enduring topical centers that remain globally shared but vary in prominence across countries. These six topical centers, which encompass LA research, are: self-regulated learning, dashboards and theory, social learning, automated feedback, multimodal analytics, and outcome prediction. Our findings highlight key challenges for the future: widening participation, reducing dependency on a narrow set of funders, and ensuring that emerging research trajectories remain responsive to educational practice and societal needs.",
      "url": "https://arxiv.org/pdf/2601.07629v1",
      "date": "2026-01-12T15:10:44+00:00"
    },
    {
      "id": "auth_valdemar_švábenský",
      "name": "Valdemar Švábenský",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_conrad_borchers",
      "name": "Conrad Borchers",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_elvin_fortuna",
      "name": "Elvin Fortuna",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_elizabeth_b_cloude",
      "name": "Elizabeth B. Cloude",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_dragan_gašević",
      "name": "Dragan Gašević",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_learning_analytics",
      "name": "learning analytics",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_analytics_la",
      "name": "analytics la",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_learning_dashboards",
      "name": "learning dashboards",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regulated_learning",
      "name": "regulated learning",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_analytics",
      "name": "analytics",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07398v1",
      "name": "On Narrative: The Rhetorical Mechanisms of Online Polarisation",
      "group": "paper",
      "val": 30,
      "abstract": "Polarisation research has demonstrated how people cluster in homogeneous groups with opposing opinions. However, this effect emerges not only through interaction between people, limiting communication between groups, but also between narratives, shaping opinions and partisan identities. Yet, how polarised groups collectively construct and negotiate opposing interpretations of reality, and whether narratives move between groups despite limited interactions, remains unexplored. To address this gap, we formalise the concept of narrative polarisation and demonstrate its measurement in 212 YouTube videos and 90,029 comments on the Israeli-Palestinian conflict. Based on structural narrative theory and implemented through a large language model, we extract the narrative roles assigned to central actors in two partisan information environments. We find that while videos produce highly polarised narratives, comments significantly reduce narrative polarisation, harmonising discourse on the surface level. However, on a deeper narrative level, recurring narrative motifs reveal additional differences between partisan groups.",
      "url": "https://arxiv.org/pdf/2601.07398v1",
      "date": "2026-01-12T10:34:57+00:00"
    },
    {
      "id": "auth_jan_elfes",
      "name": "Jan Elfes",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marco_bastos",
      "name": "Marco Bastos",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_luca_maria_aiello",
      "name": "Luca Maria Aiello",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_narrative_polarisation",
      "name": "narrative polarisation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_polarised_narratives",
      "name": "polarised narratives",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_narratives_groups",
      "name": "narratives groups",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_groups_narratives",
      "name": "groups narratives",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_palestinian_conflict",
      "name": "palestinian conflict",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07085v1",
      "name": "The AI Cognitive Trojan Horse: How Large Language Models May Bypass Human Epistemic Vigilance",
      "group": "paper",
      "val": 30,
      "abstract": "Large language model (LLM)-based conversational AI systems present a challenge to human cognition that current frameworks for understanding misinformation and persuasion do not adequately address. This paper proposes that a significant epistemic risk from conversational AI may lie not in inaccuracy or intentional deception, but in something more fundamental: these systems may be configured, through optimization processes that make them useful, to present characteristics that bypass the cognitive mechanisms humans evolved to evaluate incoming information. The Cognitive Trojan Horse hypothesis draws on Sperber and colleagues' theory of epistemic vigilance -- the parallel cognitive process monitoring communicated information for reasons to doubt -- and proposes that LLM-based systems present 'honest non-signals': genuine characteristics (fluency, helpfulness, apparent disinterest) that fail to carry the information equivalent human characteristics would carry, because in humans these are costly to produce while in LLMs they are computationally trivial. Four mechanisms of potential bypass are identified: processing fluency decoupled from understanding, trust-competence presentation without corresponding stakes, cognitive offloading that delegates evaluation itself to the AI, and optimization dynamics that systematically produce sycophancy. The framework generates testable predictions, including a counterintuitive speculation that cognitively sophisticated users may be more vulnerable to AI-mediated epistemic influence. This reframes AI safety as partly a problem of calibration -- aligning human evaluative responses with the actual epistemic status of AI-generated content -- rather than solely a problem of preventing deception.",
      "url": "https://arxiv.org/pdf/2601.07085v1",
      "date": "2026-01-11T22:28:56+00:00"
    },
    {
      "id": "auth_andrew_d_maynard",
      "name": "Andrew D. Maynard",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_risk_conversational",
      "name": "risk conversational",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_information_cognitive",
      "name": "information cognitive",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_misinformation_persuasion",
      "name": "misinformation persuasion",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_preventing_deception",
      "name": "preventing deception",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07016v1",
      "name": "Belief in False Information: A Human-Centered Security Risk in Sociotechnical Systems",
      "group": "paper",
      "val": 30,
      "abstract": "This paper provides a comprehensive literature review on the belief in false information, including misinformation, disinformation, and fake information. It addresses the increasing societal concern regarding false information, which is fueled by technological progress, especially advancements in artificial intelligence. This review systematically identifies and categorizes factors that influence the belief in false information. The review identifies 24 influence factors grouped into six main categories: demographic factors, personality traits, psychological factors, policy and values, media consumption, and preventive factors. Key findings highlight that lower education levels, high extraversion, low agreeableness, high neuroticism, and low cognitive reflection significantly increase belief in false information. The effectiveness of preventive strategies like labeling false information and promoting reflection about correctness is also discussed. This literature review conceptualizes belief in false information as a human-centered security risk in sociotechnical systems, as it can be exploited to manipulate decisions, undermine trust, and increase susceptibility to social engineering. It aims to inform preventive strategies that strengthen socio-technical security and societal resilience.",
      "url": "https://arxiv.org/pdf/2601.07016v1",
      "date": "2026-01-11T18:06:53+00:00"
    },
    {
      "id": "auth_fabian_walke",
      "name": "Fabian Walke",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_thaddäa_nürnberger",
      "name": "Thaddäa Nürnberger",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_false_information",
      "name": "false information",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_fake_information",
      "name": "fake information",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_undermine_trust",
      "name": "undermine trust",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_risk_sociotechnical",
      "name": "risk sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_misinformation_disinformation",
      "name": "misinformation disinformation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06894v1",
      "name": "How Do Ports Organise Innovation? Linking Port Governance, Ownership, and Living Labs",
      "group": "paper",
      "val": 30,
      "abstract": "Ports are pivotal to decarbonisation and resilience, yet port studies rarely examine how ownership and decision rights shape the process and outcomes of sustainability and digital pilots. Living Lab (LL) scholarship offers strong concepts, but limited sector-grounded explanation of LL-governance fit in ports. We develop and apply a governance-LL fit framework linking port governance and ownership to four LL pillars: co-creation, real-life setting, iterative learning, and institutional embedding (multi-level decision-making). We apply the framework in a comparative case study of two analytically contrasting ports, anchored in port-defined priorities: an Energy Community pilot in Aalborg and a Green Coordinator function in Trelleborg. Using an LL macro-meso-micro lens, we distinguish the stable constellation of actors and mandates (macro), the governance of specific projects (meso), and the methods used to generate and test solutions (micro). Findings show that Landlord governance offers contract- and procurement-based landing zones (concessions/leases and tender clauses) that can codify LL outputs and support scaling across tenants and infrastructures. Tool/Public Service governance embeds learning mainly through SOPs, procurement specifications, and municipal coordination, enabling internal operational gains but limiting external codification without bespoke agreements. Across both ports, key needs are clear role definition, sustained stakeholder engagement, and timely alignment with decision windows. Overall, LL effectiveness is governance-contingent, reflecting where decision rights sit and which instruments embed learning into routine practice.",
      "url": "https://arxiv.org/pdf/2601.06894v1",
      "date": "2026-01-11T12:35:17+00:00"
    },
    {
      "id": "auth_sonia_yeh",
      "name": "Sonia Yeh",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_christopher_dirzka",
      "name": "Christopher Dirzka",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_aleksandr_kondratenko",
      "name": "Aleksandr Kondratenko",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_frans_libertson",
      "name": "Frans Libertson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_benedicte_madon",
      "name": "Benedicte Madon",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_port_governance",
      "name": "port governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_landlord_governance",
      "name": "landlord governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_tenants_infrastructures",
      "name": "tenants infrastructures",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_apply_governance",
      "name": "apply governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sustained_stakeholder",
      "name": "sustained stakeholder",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06703v1",
      "name": "Mapping and Comparing Climate Equity Policy Practices Using RAG LLM-Based Semantic Analysis and Recommendation Systems",
      "group": "paper",
      "val": 30,
      "abstract": "This study investigates the use of large language models to enhance the policymaking process. We first analyze planning-related job postings to revisit the evolving roles of planners in the era of AI. We then examine climate equity plans across the U.S. and apply ChatGPT to conduct semantic analysis, extracting policy, strategy, and action items related to transportation and energy. The methodological framework relied on a LangChain-native retrieval-augmented generation pipeline. Based on these extracted elements and their evaluated presence, we develop a content-based recommendation system to support cross-city policy comparison. The results indicate that, despite growing attention to AI, planning jobs largely retain their traditional domain emphases in transportation, environmental planning, housing, and land use. Communicative responsibilities remain central to planning practice. Climate equity plans commonly address transportation, environmental, and energy-related measures aimed at reducing greenhouse gas emissions and predominantly employ affirmative language. The demonstration of the recommendation system illustrates how planners can efficiently identify cities with similar policy practices, revealing patterns of geographic similarity in policy adoption. The study concludes by envisioning localized yet personalized AI-assisted systems that can be adapted within urban systems.",
      "url": "https://arxiv.org/pdf/2601.06703v1",
      "date": "2026-01-10T22:01:28+00:00"
    },
    {
      "id": "auth_seung_jun_choi",
      "name": "Seung Jun Choi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_policymaking",
      "name": "policymaking",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_city_policy",
      "name": "city policy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_extracting_policy",
      "name": "extracting policy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_planners",
      "name": "planners",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_envisioning_localized",
      "name": "envisioning localized",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06692v1",
      "name": "The Axiom of Consent: Friction Dynamics in Multi-Agent Coordination",
      "group": "paper",
      "val": 30,
      "abstract": "Multi-agent systems face a fundamental coordination problem: agents must coordinate despite heterogeneous preferences, asymmetric stakes, and imperfect information. When coordination fails, friction emerges: measurable resistance manifesting as deadlock, thrashing, communication overhead, or outright conflict. This paper derives a formal framework for analyzing coordination friction from a single axiom: actions affecting agents require authorization from those agents in proportion to stakes.   From this axiom of consent, we establish the kernel triple $(α, σ, ε)$ (alignment, stake, and entropy) characterizing any resource allocation configuration. The friction equation $F = σ (1 + ε)/(1 + α)$ predicts coordination difficulty as a function of preference alignment $α$, stake magnitude $σ$, and communication entropy $ε$. The Replicator-Optimization Mechanism (ROM) governs evolutionary selection over coordination strategies: configurations generating less friction persist longer, establishing consent-respecting arrangements as dynamical attractors rather than normative ideals.   We develop formal definitions for resource consent, coordination legitimacy, and friction-aware allocation in multi-agent systems. The framework yields testable predictions: MARL systems with higher reward alignment exhibit faster convergence; distributed allocations accounting for stake asymmetry generate lower coordination failure; AI systems with interpretability deficits produce friction proportional to the human-AI alignment gap. Applications to cryptocurrency governance and political systems demonstrate that the same equations govern friction dynamics across domains, providing a complexity science perspective on coordination under preference heterogeneity.",
      "url": "https://arxiv.org/pdf/2601.06692v1",
      "date": "2026-01-10T21:28:41+00:00"
    },
    {
      "id": "auth_murad_farzulla",
      "name": "Murad Farzulla",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_coordination_friction",
      "name": "coordination friction",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_fundamental_coordination",
      "name": "fundamental coordination",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_coordination_preference",
      "name": "coordination preference",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_analyzing_coordination",
      "name": "analyzing coordination",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_stake_entropy",
      "name": "stake entropy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06687v1",
      "name": "The Case for Strategic Data Stewardship: Re-imagining Data Governance to Make Responsible Data Re-use Possible",
      "group": "paper",
      "val": 30,
      "abstract": "As societal challenges grow more complex, access to data for public interest use is paradoxically becoming more constrained. This emerging data winter is not simply a matter of scarcity, but of shrinking legitimate and trusted pathways for responsible data reuse. Concerns over misuse, regulatory uncertainty, and the competitive race to train AI systems have concentrated data access among a few actors while raising costs and inhibiting collaboration. Prevailing data governance models, focused on compliance, risk management, and internal control, are necessary but insufficient. They often result in data that is technically available yet practically inaccessible, legally shareable yet institutionally unusable, or socially illegitimate to deploy. This paper proposes strategic data stewardship as a complementary institutional function designed to systematically, sustainably, and responsibly activate data for public value. Unlike traditional stewardship, which tends to be inwardlooking, strategic data stewardship focuses on enabling cross sector reuse, reducing missed opportunities, and building durable, ecosystem-level collaboration. It outlines core principles, functions, and competencies, and introduces a practical Data Stewardship Canvas to support adoption across contexts such as data collaboratives, data spaces, and data commons. Strategic data stewardship, the paper argues, is essential in the age of AI: it translates governance principles into practice, builds trust across data ecosystems, and ensures that data are not only governed, but meaningfully mobilized to serve society.",
      "url": "https://arxiv.org/pdf/2601.06687v1",
      "date": "2026-01-10T21:22:50+00:00"
    },
    {
      "id": "auth_stefaan_verhulst",
      "name": "Stefaan Verhulst",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_data_governance",
      "name": "data governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_data_stewardship",
      "name": "data stewardship",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_data_ecosystems",
      "name": "data ecosystems",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_responsible_data",
      "name": "responsible data",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_data_public",
      "name": "data public",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06634v1",
      "name": "A Framework for Kara-Kichwa Data Sovereignty in Latin America and the Caribbean",
      "group": "paper",
      "val": 30,
      "abstract": "In the high-altitude territories of the Andean-Amazonian-Atlantic pathway, data is not merely a digital resource but an extension of Khipu Panaka, the genealogical and relational memory of the Kara-Kichwa Republics. This perspective paper introduces the Kara-Kichwa Data Sovereignty Framework, a living instrument designed to counteract the \"intellectual gentrification\" and systemic invisibility of Andean Indigenous Peoples in global data ecosystems. Grounded in Indigenous legal systems thinking, the framework codifies five customary pillars, Kamachy (Self-determination), Ayllu-llaktapak kamachy (Collective Authority), Tantanakuy (Relational Accountability), Willay-panka-tantay (Ancestral Memory), and Sumak Kawsay (Biocultural Ethics), to govern the lifecycle of data from generation to expiration.",
      "url": "https://arxiv.org/pdf/2601.06634v1",
      "date": "2026-01-10T17:36:53+00:00"
    },
    {
      "id": "auth_warinkwi_k_flores",
      "name": "WariNkwi K. Flores",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kuntikzi_flores",
      "name": "KunTikzi Flores",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_rosa_m_panama",
      "name": "Rosa M. Panama",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kayakanti_alta",
      "name": "KayaKanti Alta",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_data_sovereignty",
      "name": "data sovereignty",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_indigenous_legal",
      "name": "indigenous legal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_indigenous_peoples",
      "name": "indigenous peoples",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_indigenous",
      "name": "indigenous",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_kichwa_data",
      "name": "kichwa data",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06500v1",
      "name": "The AI Pyramid A Conceptual Framework for Workforce Capability in the Age of AI",
      "group": "paper",
      "val": 30,
      "abstract": "Artificial intelligence (AI) represents a qualitative shift in technological change by extending cognitive labor itself rather than merely automating routine tasks. Recent evidence shows that generative AI disproportionately affects highly educated, white collar work, challenging existing assumptions about workforce vulnerability and rendering traditional approaches to digital or AI literacy insufficient. This paper introduces the concept of AI Nativity, the capacity to integrate AI fluidly into everyday reasoning, problem solving, and decision making, and proposes the AI Pyramid, a conceptual framework for organizing human capability in an AI mediated economy. The framework distinguishes three interdependent capability layers: AI Native capability as a universal baseline for participation in AI augmented environments; AI Foundation capability for building, integrating, and sustaining AI enabled systems; and AI Deep capability for advancing frontier AI knowledge and applications. Crucially, the pyramid is not a career ladder but a system level distribution of capabilities required at scale. Building on this structure, the paper argues that effective AI workforce development requires treating capability formation as infrastructure rather than episodic training, centered on problem based learning embedded in work contexts and supported by dynamic skill ontologies and competency based measurement. The framework has implications for organizations, education systems, and governments seeking to align learning, measurement, and policy with the evolving demands of AI mediated work, while addressing productivity, resilience, and inequality at societal scale.",
      "url": "https://arxiv.org/pdf/2601.06500v1",
      "date": "2026-01-10T09:27:56+00:00"
    },
    {
      "id": "auth_alok_khatri",
      "name": "Alok Khatri",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_bishesh_khanal",
      "name": "Bishesh Khanal",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_workforce",
      "name": "ai workforce",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_capability_ai",
      "name": "capability ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_literacy",
      "name": "ai literacy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sustaining_ai",
      "name": "sustaining ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_foundation",
      "name": "ai foundation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06477v1",
      "name": "IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments",
      "group": "paper",
      "val": 30,
      "abstract": "Warning: This paper consists of examples representing regional biases in Indian regions that might be offensive towards a particular region. While social biases corresponding to gender, race, socio-economic conditions, etc., have been extensively studied in the major applications of Natural Language Processing (NLP), biases corresponding to regions have garnered less attention. This is mainly because of (i) difficulty in the extraction of regional bias datasets, (ii) disagreements in annotation due to inherent human biases, and (iii) regional biases being studied in combination with other types of social biases and often being under-represented. This paper focuses on creating a dataset IndRegBias, consisting of regional biases in an Indian context reflected in users' comments on popular social media platforms, namely Reddit and YouTube. We carefully selected 25,000 comments appearing on various threads in Reddit and videos on YouTube discussing trending topics on regional issues in India. Furthermore, we propose a multilevel annotation strategy to annotate the comments describing the severity of regional biased statements. To detect the presence of regional bias and its severity in IndRegBias, we evaluate open-source Large Language Models (LLMs) and Indic Language Models (ILMs) using zero-shot, few-shot, and fine-tuning strategies. We observe that zero-shot and few-shot approaches show lower accuracy in detecting regional biases and severity in the majority of the LLMs and ILMs. However, the fine-tuning approach significantly enhances the performance of the LLM in detecting Indian regional bias along with its severity.",
      "url": "https://arxiv.org/pdf/2601.06477v1",
      "date": "2026-01-10T08:13:03+00:00"
    },
    {
      "id": "auth_debasmita_panda",
      "name": "Debasmita Panda",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_akash_anil",
      "name": "Akash Anil",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_neelesh_kumar_shukla",
      "name": "Neelesh Kumar Shukla",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_biases_indian",
      "name": "biases indian",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_nlp_biases",
      "name": "nlp biases",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regional_bias",
      "name": "regional bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regional_biases",
      "name": "regional biases",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regional_biased",
      "name": "regional biased",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06412v1",
      "name": "Brokerage in the Black Box: Swing States, Strategic Ambiguity, and the Global Politics of AI Governance",
      "group": "paper",
      "val": 30,
      "abstract": "The U.S. - China rivalry has placed frontier dual-use technologies, particularly Artificial Intelligence (AI), at the center of global power dynamics, as techno-nationalism, supply chain securitization, and competing standards deepen bifurcation within a weaponized interdependence that blurs civilian-military boundaries. Existing research, yet, mostly emphasizes superpower strategies and often overlooks the role of middle powers as autonomous actors shaping the techno-order. This study examines Technological Swing States (TSS), middle powers with both technological capacity and strategic flexibility, and their ability to navigate the frontier technologies' uncertainty and opacity to mediate great-power techno-competition regionally and globally. It reconceptualizes AI opacity not as a technical deficit, but as a structural feature and strategic resource, stemming from algorithmic complexity, political incentives that prioritize performance over explainability, and the limits of post-hoc interpretability. This structural opacity shifts authority from technical demands for explainability to institutional mechanisms, such as certification, auditing, and disclosure, converting technical constraints into strategic political opportunities. Drawing on case studies of South Korea, Singapore, and India, the paper theorizes how TSS exploit the interplay between opacity and institutional transparency through three strategies: (i) delay and hedging, (ii) selective alignment, and (iii) normative intermediation. These practices enable TSS to preserve strategic flexibility, build trust among diverse stakeholders, and broker convergence across competing governance regimes, thereby influencing institutional design, interstate bargaining, and policy outcomes in global AI governance.",
      "url": "https://arxiv.org/pdf/2601.06412v1",
      "date": "2026-01-10T03:19:48+00:00"
    },
    {
      "id": "auth_ha_chi_tran",
      "name": "Ha-Chi Tran",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_superpower_strategies",
      "name": "superpower strategies",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_powers_technological",
      "name": "powers technological",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_global_power",
      "name": "global power",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_china_rivalry",
      "name": "china rivalry",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_technological_capacity",
      "name": "technological capacity",
      "group": "topic",
      "val": 10
    }
  ],
  "links": [
    {
      "source": "2601.04175v1",
      "target": "auth_noam_kolt",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_nicholas_caputo",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jack_boeglin",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_cullen_o'keefe",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_rishi_bommasani",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_stephen_casper",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_mariano_florentino_cuéllar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noah_feldman",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_gillian_k_hadfield",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_lewis_hammond",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_peter_henderson",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_atoosa_kasirzadeh",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_seth_lazar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_anka_reuel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_kevin_l_wei",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jonathan_zittrain",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_alignment",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_compliance",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_exploring_legal",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_specifying_ai",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_ensuring_ai",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "auth_ruiyi_guo",
      "value": 5
    },
    {
      "source": "2601.04107v1",
      "target": "auth_bodong_zhang",
      "value": 5
    },
    {
      "source": "2601.04107v1",
      "target": "topic_institutional_logics",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_ai_governance",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_governs_ai",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_intelligence_governance",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_govern_ontologically",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "auth_tom_deckenbrunnen",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_alessio_buscemi",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_marco_almada",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_alfredo_capozucca",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_german_castignani",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "topic_eu_ai",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_ai_act",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_enforcement_macro",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_govern_ai",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_ai_technical",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "auth_anamaria_mojica_hanke",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_thomas_goger",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_svenja_wölfel",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_brian_valerius",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_steffen_herbold",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "topic_offense_individuals",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_criminal_liability",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_legal_consequences",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_criminal_legal",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_legal_systems",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "auth_xiaoxian_shen",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_yuhui_zhang",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_sahithi_ankireddy",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_xiaohan_wang",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_maya_varma",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_henry_guo",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_curtis_langlotz",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_serena_yeung_levy",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "topic_multimodal_reasoning",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_medical_ai",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_radiology_image",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_understanding_radiology",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_raddiff_multimodal",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "auth_sarah_spiekermann_hoff",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_marc_langheinrich",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_johannes_hoff",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_christiane_wendehorst",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_jürgen_pfeffer",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_thomas_fuchs",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_armin_grunwald",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital_world",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital_technologies",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_ethical_psychological",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_rules_digital",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "auth_junaid_qadir",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_muhammad_adil_attique",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_saleha_shoaib",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_syed_ibrahim_ghaznavi",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "topic_examines_ai",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_coaching_chatbot",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_ai_technical",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_intellectual",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_ai_chatbots",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "auth_kunpeng_wang",
      "value": 5
    },
    {
      "source": "2601.03547v1",
      "target": "auth_jiahui_hu",
      "value": 5
    },
    {
      "source": "2601.03547v1",
      "target": "topic_ai_china",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_ai_capital",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_china_economy",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_growth_ai",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_capital_equilibrium",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "auth_nadav_kunievsky",
      "value": 5
    },
    {
      "source": "2601.03469v1",
      "target": "auth_pedro_pertusi",
      "value": 5
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_variation",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_rewrites",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_persuasive_essays",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_writing_scores",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_content",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "auth_aron_gohr",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_marie_amelie_lawn",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_kevin_gao",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_inigo_serjeant",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_stephen_heslip",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "topic_tutoring_systems",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_intelligent_tutoring",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_tutoring",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_homework_platform",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_editable_programming",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "auth_mohamed_ouf",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_shayan_noei",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_zeph_van_iterson",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_mariam_guizani",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_ying_zou",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "topic_contributors_oss4sg",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_contributors_code",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_github",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_oss_communities",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_oss_projects",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "auth_jacob_erickson",
      "value": 5
    },
    {
      "source": "2601.03222v1",
      "target": "topic_trust_ai",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_autonomy_trust",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_conversational_ai",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_ai_agents",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_focusing_trust",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noam_kolt",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_nicholas_caputo",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jack_boeglin",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_cullen_o'keefe",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_rishi_bommasani",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_stephen_casper",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_mariano_florentino_cuéllar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noah_feldman",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_gillian_k_hadfield",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_lewis_hammond",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_peter_henderson",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_atoosa_kasirzadeh",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_seth_lazar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_anka_reuel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_kevin_l_wei",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jonathan_zittrain",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_alignment",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_compliance",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_exploring_legal",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_specifying_ai",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_ensuring_ai",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_gilda",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_dungarwal",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_a_thongkham",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_e_t_ajayi",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_s_choudhary",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_t_m_terol",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_c_lam",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_j_p_araujo",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_m_mcfadyen_mungalln",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_l_s_liebovitch",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_t_coleman",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_h_west",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_k_sieck",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_s_carter",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "topic_news_social",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_social_media",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_news_dataset",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_videos_social",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_media_youtube",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "auth_thomas_le_goff",
      "value": 5
    },
    {
      "source": "2601.04958v1",
      "target": "topic_sustainability_laws",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_regulation_assess",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_regulatory_landscape",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_sustainability_reporting",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_ai_environmental",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "auth_trevor_de_clark",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "auth_yulia_bobkova",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "auth_ajay_kumar_shrestha",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_usability",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_self",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_guidance",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_compliant_privacy",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_standards",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "auth_kenzo_soares_seto",
      "value": 5
    },
    {
      "source": "2601.05961v1",
      "target": "topic_examines_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_digital_sovereignty",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_sociotechnical_imaginaries",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_idea_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "auth_tianshi_li",
      "value": 5
    },
    {
      "source": "2601.05918v1",
      "target": "topic_interviewer_ai",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_identifying_interviewees",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_interviewees",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_anthropic_interviewer",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_search_agentic",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "auth_michael_henry_tessler",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_georgina_evans",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_michiel_a_bakker",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_sophie_bridgers",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_rishub_jain",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_raphael_koster",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_verena_rieser",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_anca_dragan",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_matthew_botvinick",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_christopher_summerfield",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "topic_mediated_deliberation",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_enhancing",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_political",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_meaningful_deliberation",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_augmentation",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "auth_jakub_harasta",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_matej_vasina",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_martin_kornel",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_tomas_foltynek",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_contexts",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_family_law",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_self",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_scenario_gendered",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_guidance",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "auth_íris_damião",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_joão_franco",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_mariana_silva",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_paulo_almeida",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_pedro_c_magalhães",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_joana_gonçalves_sá",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "topic_campaign_media",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_elections_analyzed",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_electoral_projections",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_different_political",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_media_influence",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "auth_yerin_kwak",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "auth_siddharth_adelkar",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "auth_zachary_a_pardos",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "topic_student_credit",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_course_articulation",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_articulations_institutions",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_ensuring_credits",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_candidate_articulations",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "auth_simon_chesterman",
      "value": 5
    },
    {
      "source": "2601.05574v1",
      "target": "auth_loy_hui_chieh",
      "value": 5
    },
    {
      "source": "2601.05574v1",
      "target": "topic_academic_authority",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_research_integrity",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_scholarly_labour",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_scholarly",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_institutional_capacity",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "auth_alessandro_bellina",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "auth_giordano_de_marzo",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "auth_david_garcia",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "topic_collective_ai",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_ai_agents",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_ai_agent",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_investigate_ai",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_reveal_ai",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "auth_tim_menzner",
      "value": 5
    },
    {
      "source": "2601.05358v1",
      "target": "auth_jochen_l_leidner",
      "value": 5
    },
    {
      "source": "2601.05358v1",
      "target": "topic_media_bias",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_bias_propaganda",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_bias_types",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_fact_bias",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_political_spectrum",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "auth_ritwik_gupta",
      "value": 5
    },
    {
      "source": "2601.05307v1",
      "target": "auth_andrew_w_reddie",
      "value": 5
    },
    {
      "source": "2601.05307v1",
      "target": "topic_ai_weaponization",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_ai_security",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_national_security",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_weaponization_compute",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_weaponizable_capabilities",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "auth_valdemar_švábenský",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "auth_conrad_borchers",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "auth_elvin_fortuna",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "auth_elizabeth_b_cloude",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "auth_dragan_gašević",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "topic_learning_analytics",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "topic_analytics_la",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "topic_learning_dashboards",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "topic_regulated_learning",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "topic_analytics",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "auth_jan_elfes",
      "value": 5
    },
    {
      "source": "2601.07398v1",
      "target": "auth_marco_bastos",
      "value": 5
    },
    {
      "source": "2601.07398v1",
      "target": "auth_luca_maria_aiello",
      "value": 5
    },
    {
      "source": "2601.07398v1",
      "target": "topic_narrative_polarisation",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "topic_polarised_narratives",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "topic_narratives_groups",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "topic_groups_narratives",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "topic_palestinian_conflict",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "auth_andrew_d_maynard",
      "value": 5
    },
    {
      "source": "2601.07085v1",
      "target": "topic_conversational_ai",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "topic_risk_conversational",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "topic_information_cognitive",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "topic_misinformation_persuasion",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "topic_preventing_deception",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "auth_fabian_walke",
      "value": 5
    },
    {
      "source": "2601.07016v1",
      "target": "auth_thaddäa_nürnberger",
      "value": 5
    },
    {
      "source": "2601.07016v1",
      "target": "topic_false_information",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "topic_fake_information",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "topic_undermine_trust",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "topic_risk_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "topic_misinformation_disinformation",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "auth_sonia_yeh",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "auth_christopher_dirzka",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "auth_aleksandr_kondratenko",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "auth_frans_libertson",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "auth_benedicte_madon",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "topic_port_governance",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "topic_landlord_governance",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "topic_tenants_infrastructures",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "topic_apply_governance",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "topic_sustained_stakeholder",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "auth_seung_jun_choi",
      "value": 5
    },
    {
      "source": "2601.06703v1",
      "target": "topic_policymaking",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "topic_city_policy",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "topic_extracting_policy",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "topic_planners",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "topic_envisioning_localized",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "auth_murad_farzulla",
      "value": 5
    },
    {
      "source": "2601.06692v1",
      "target": "topic_coordination_friction",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "topic_fundamental_coordination",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "topic_coordination_preference",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "topic_analyzing_coordination",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "topic_stake_entropy",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "auth_stefaan_verhulst",
      "value": 5
    },
    {
      "source": "2601.06687v1",
      "target": "topic_data_governance",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "topic_data_stewardship",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "topic_data_ecosystems",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "topic_responsible_data",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "topic_data_public",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "auth_warinkwi_k_flores",
      "value": 5
    },
    {
      "source": "2601.06634v1",
      "target": "auth_kuntikzi_flores",
      "value": 5
    },
    {
      "source": "2601.06634v1",
      "target": "auth_rosa_m_panama",
      "value": 5
    },
    {
      "source": "2601.06634v1",
      "target": "auth_kayakanti_alta",
      "value": 5
    },
    {
      "source": "2601.06634v1",
      "target": "topic_data_sovereignty",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "topic_indigenous_legal",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "topic_indigenous_peoples",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "topic_indigenous",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "topic_kichwa_data",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "auth_alok_khatri",
      "value": 5
    },
    {
      "source": "2601.06500v1",
      "target": "auth_bishesh_khanal",
      "value": 5
    },
    {
      "source": "2601.06500v1",
      "target": "topic_ai_workforce",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "topic_capability_ai",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "topic_ai_literacy",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "topic_sustaining_ai",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "topic_ai_foundation",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "auth_debasmita_panda",
      "value": 5
    },
    {
      "source": "2601.06477v1",
      "target": "auth_akash_anil",
      "value": 5
    },
    {
      "source": "2601.06477v1",
      "target": "auth_neelesh_kumar_shukla",
      "value": 5
    },
    {
      "source": "2601.06477v1",
      "target": "topic_biases_indian",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "topic_nlp_biases",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "topic_regional_bias",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "topic_regional_biases",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "topic_regional_biased",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "auth_ha_chi_tran",
      "value": 5
    },
    {
      "source": "2601.06412v1",
      "target": "topic_superpower_strategies",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "topic_powers_technological",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "topic_global_power",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "topic_china_rivalry",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "topic_technological_capacity",
      "value": 2
    }
  ]
}