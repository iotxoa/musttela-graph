{
  "nodes": [
    {
      "id": "2601.04107v1",
      "name": "From Abstract Threats to Institutional Realities: A Comparative Semantic Network Analysis of AI Securitisation in the US, EU, and China",
      "group": "paper",
      "val": 30,
      "abstract": "Artificial intelligence governance exhibits a striking paradox: while major jurisdictions converge rhetorically around concepts such as safety, risk, and accountability, their regulatory frameworks remain fundamentally divergent and mutually unintelligible. This paper argues that this fragmentation cannot be explained solely by geopolitical rivalry, institutional complexity, or instrument selection. Instead, it stems from how AI is constituted as an object of governance through distinct institutional logics. Integrating securitisation theory with the concept of the dispositif, we demonstrate that jurisdictions govern ontologically different objects under the same vocabulary. Using semantic network analysis of official policy texts from the European Union, the United States, and China (2023-2025), we trace how concepts like safety are embedded within divergent semantic architectures. Our findings reveal that the EU juridifies AI as a certifiable product through legal-bureaucratic logic; the US operationalises AI as an optimisable system through market-liberal logic; and China governs AI as socio-technical infrastructure through holistic state logic. We introduce the concept of structural incommensurability to describe this condition of ontological divergence masked by terminological convergence. This reframing challenges ethics-by-principles approaches to global AI governance, suggesting that coordination failures arise not from disagreement over values but from the absence of a shared reference object.",
      "url": "https://arxiv.org/pdf/2601.04107v1",
      "date": "2026-01-07T17:12:03+00:00"
    },
    {
      "id": "auth_ruiyi_guo",
      "name": "Ruiyi Guo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_bodong_zhang",
      "name": "Bodong Zhang",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_institutional_logics",
      "name": "institutional logics",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_governance",
      "name": "ai governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_governs_ai",
      "name": "governs ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intelligence_governance",
      "name": "intelligence governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_govern_ontologically",
      "name": "govern ontologically",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04094v1",
      "name": "The Bathtub of European AI Governance: Identifying Technical Sandboxes as the Micro-Foundation of Regulatory Learning",
      "group": "paper",
      "val": 30,
      "abstract": "The EU AI Act adopts a horizontal and adaptive approach to govern AI technologies characterised by rapid development and unpredictable emerging capabilities. To maintain relevance, the Act embeds provisions for regulatory learning. However, these provisions operate within a complex network of actors and mechanisms that lack a clearly defined technical basis for scalable information flow. This paper addresses this gap by establishing a theoretical model of regulatory learning space defined by the AI Act, decomposed into micro, meso, and macro levels. Drawing from this functional perspective of this model, we situate the diverse stakeholders - ranging from the EU Commission at the macro level to AI developers at the micro level - within the transitions of enforcement (macro-micro) and evidence aggregation (micro-macro). We identify AI Technical Sandboxes as the essential engine for evidence generation at the micro level, providing the necessary data to drive scalable learning across all levels of the model. By providing an extensive discussion of the requirements and challenges for AITSes to serve as this micro-level evidence generator, we aim to bridge the gap between legislative commands and technical operationalisation, thereby enabling a structured discourse between technical and legal experts.",
      "url": "https://arxiv.org/pdf/2601.04094v1",
      "date": "2026-01-07T17:01:06+00:00"
    },
    {
      "id": "auth_tom_deckenbrunnen",
      "name": "Tom Deckenbrunnen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_alessio_buscemi",
      "name": "Alessio Buscemi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marco_almada",
      "name": "Marco Almada",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_alfredo_capozucca",
      "name": "Alfredo Capozucca",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_german_castignani",
      "name": "German Castignani",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_eu_ai",
      "name": "eu ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_act",
      "name": "ai act",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_enforcement_macro",
      "name": "enforcement macro",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_govern_ai",
      "name": "govern ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_technical",
      "name": "ai technical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03788v1",
      "name": "Criminal Liability of Generative Artificial Intelligence Providers for User-Generated Child Sexual Abuse Material",
      "group": "paper",
      "val": 30,
      "abstract": "The development of more powerful Generative Artificial Intelligence (GenAI) has expanded its capabilities and the variety of outputs. This has introduced significant legal challenges, including gray areas in various legal systems, such as the assessment of criminal liability for those responsible for these models. Therefore, we conducted a multidisciplinary study utilizing the statutory interpretation of relevant German laws, which, in conjunction with scenarios, provides a perspective on the different properties of GenAI in the context of Child Sexual Abuse Material (CSAM) generation. We found that generating CSAM with GenAI may have criminal and legal consequences not only for the user committing the primary offense but also for individuals responsible for the models, such as independent software developers, researchers, and company representatives. Additionally, the assessment of criminal liability may be affected by contextual and technical factors, including the type of generated image, content moderation policies, and the model's intended purpose. Based on our findings, we discussed the implications for different roles, as well as the requirements when developing such systems.",
      "url": "https://arxiv.org/pdf/2601.03788v1",
      "date": "2026-01-07T10:38:35+00:00"
    },
    {
      "id": "auth_anamaria_mojica_hanke",
      "name": "Anamaria Mojica-Hanke",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_thomas_goger",
      "name": "Thomas Goger",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_svenja_wölfel",
      "name": "Svenja Wölfel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_brian_valerius",
      "name": "Brian Valerius",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_steffen_herbold",
      "name": "Steffen Herbold",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_offense_individuals",
      "name": "offense individuals",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_criminal_liability",
      "name": "criminal liability",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_consequences",
      "name": "legal consequences",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_criminal_legal",
      "name": "criminal legal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_systems",
      "name": "legal systems",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03733v1",
      "name": "RadDiff: Describing Differences in Radiology Image Sets with Natural Language",
      "group": "paper",
      "val": 30,
      "abstract": "Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.",
      "url": "https://arxiv.org/pdf/2601.03733v1",
      "date": "2026-01-07T09:25:04+00:00"
    },
    {
      "id": "auth_xiaoxian_shen",
      "name": "Xiaoxian Shen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yuhui_zhang",
      "name": "Yuhui Zhang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sahithi_ankireddy",
      "name": "Sahithi Ankireddy",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_xiaohan_wang",
      "name": "Xiaohan Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_maya_varma",
      "name": "Maya Varma",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_henry_guo",
      "name": "Henry Guo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_curtis_langlotz",
      "name": "Curtis Langlotz",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_serena_yeung_levy",
      "name": "Serena Yeung-Levy",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_multimodal_reasoning",
      "name": "multimodal reasoning",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_medical_ai",
      "name": "medical ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_radiology_image",
      "name": "radiology image",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_understanding_radiology",
      "name": "understanding radiology",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_raddiff_multimodal",
      "name": "raddiff multimodal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03709v1",
      "name": "The Power of 10: New Rules for the Digital World",
      "group": "paper",
      "val": 30,
      "abstract": "As artificial intelligence rapidly advances, society is increasingly captivated by promises of superhuman machines and seamless digital futures. Yet these visions often obscure mounting social, ethical, and psychological concerns tied to pervasive digital technologies - from surveillance to mental health crises. This article argues that a guiding ethos is urgently needed to navigate these transformations. Inspired by the lasting influence of the biblical Ten Commandments, a European interdisciplinary group has proposed \"Ten Rules for the Digital World\" - a novel ethical framework to help individuals and societies make prudent, human-centered decisions in the age of \"supercharged\" technology.",
      "url": "https://arxiv.org/pdf/2601.03709v1",
      "date": "2026-01-07T08:49:02+00:00"
    },
    {
      "id": "auth_sarah_spiekermann_hoff",
      "name": "Sarah Spiekermann-Hoff",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marc_langheinrich",
      "name": "Marc Langheinrich",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_johannes_hoff",
      "name": "Johannes Hoff",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_christiane_wendehorst",
      "name": "Christiane Wendehorst",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jürgen_pfeffer",
      "name": "Jürgen Pfeffer",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_thomas_fuchs",
      "name": "Thomas Fuchs",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_armin_grunwald",
      "name": "Armin Grunwald",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_digital_world",
      "name": "digital world",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital_technologies",
      "name": "digital technologies",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ethical_psychological",
      "name": "ethical psychological",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_rules_digital",
      "name": "rules digital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital",
      "name": "digital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03693v1",
      "name": "Can AI Chatbots Provide Coaching in Engineering? Beyond Information Processing Toward Mastery",
      "group": "paper",
      "val": 30,
      "abstract": "Engineering education faces a double disruption: traditional apprenticeship models that cultivated judgment and tacit skill are eroding, just as generative AI emerges as an informal coaching partner. This convergence rekindles long-standing questions in the philosophy of AI and cognition about the limits of computation, the nature of embodied rationality, and the distinction between information processing and wisdom. Building on this rich intellectual tradition, this paper examines whether AI chatbots can provide coaching that fosters mastery rather than merely delivering information. We synthesize critical perspectives from decades of scholarship on expertise, tacit knowledge, and human-machine interaction, situating them within the context of contemporary AI-driven education. Empirically, we report findings from a mixed-methods study (N = 75 students, N = 7 faculty) exploring the use of a coaching chatbot in engineering education. Results reveal a consistent boundary: participants accept AI for technical problem solving (convergent tasks; M = 3.84 on a 1-5 Likert scale) but remain skeptical of its capacity for moral, emotional, and contextual judgment (divergent tasks). Faculty express stronger concerns over risk (M = 4.71 vs. M = 4.14, p = 0.003), and privacy emerges as a key requirement, with 64-71 percent of participants demanding strict confidentiality. Our findings suggest that while generative AI can democratize access to cognitive and procedural support, it cannot replicate the embodied, value-laden dimensions of human mentorship. We propose a multiplex coaching framework that integrates human wisdom within expert-in-the-loop models, preserving the depth of apprenticeship while leveraging AI scalability to enrich the next generation of engineering education.",
      "url": "https://arxiv.org/pdf/2601.03693v1",
      "date": "2026-01-07T08:28:47+00:00"
    },
    {
      "id": "auth_junaid_qadir",
      "name": "Junaid Qadir",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_muhammad_adil_attique",
      "name": "Muhammad Adil Attique",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_saleha_shoaib",
      "name": "Saleha Shoaib",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_syed_ibrahim_ghaznavi",
      "name": "Syed Ibrahim Ghaznavi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_examines_ai",
      "name": "examines ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_coaching_chatbot",
      "name": "coaching chatbot",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intellectual",
      "name": "intellectual",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_chatbots",
      "name": "ai chatbots",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03547v1",
      "name": "Governance of Technological Transition: A Predator-Prey Analysis of AI Capital in China's Economy and Its Policy Implications",
      "group": "paper",
      "val": 30,
      "abstract": "The rapid integration of Artificial Intelligence (AI) into China's economy presents a classic governance challenge: how to harness its growth potential while managing its disruptive effects on traditional capital and labor markets. This study addresses this policy dilemma by modeling the dynamic interactions between AI capital, physical capital, and labor within a Lotka-Volterra predator-prey framework. Using annual Chinese data (2016-2023), we quantify the interaction strengths, identify stable equilibria, and perform a global sensitivity analysis. Our results reveal a consistent pattern where AI capital acts as the 'prey', stimulating both physical capital accumulation and labor compensation (wage bill), while facing only weak constraining feedback. The equilibrium points are stable nodes, indicating a policy-mediated convergence path rather than volatile cycles. Critically, the sensitivity analysis shows that the labor market equilibrium is overwhelmingly driven by AI-related parameters, whereas the physical capital equilibrium is also influenced by its own saturation dynamics. These findings provide a systemic, quantitative basis for policymakers: (1) to calibrate AI promotion policies by recognizing the asymmetric leverage points in capital vs. labor markets; (2) to anticipate and mitigate structural rigidities that may arise from current regulatory settings; and (3) to prioritize interventions that foster complementary growth between AI and traditional economic structures while ensuring broad-base distribution of technological gains.",
      "url": "https://arxiv.org/pdf/2601.03547v1",
      "date": "2026-01-07T03:30:46+00:00"
    },
    {
      "id": "auth_kunpeng_wang",
      "name": "Kunpeng Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jiahui_hu",
      "name": "Jiahui Hu",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_china",
      "name": "ai china",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_capital",
      "name": "ai capital",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_china_economy",
      "name": "china economy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_growth_ai",
      "name": "growth ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_capital_equilibrium",
      "name": "capital equilibrium",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03469v1",
      "name": "Content vs. Form: What Drives the Writing Score Gap Across Socioeconomic Backgrounds? A Generated Panel Approach",
      "group": "paper",
      "val": 30,
      "abstract": "Students from different socioeconomic backgrounds exhibit persistent gaps in test scores, gaps that can translate into unequal educational and labor-market outcomes later in life. In many assessments, performance reflects not only what students know, but also how effectively they can communicate that knowledge. This distinction is especially salient in writing assessments, where scores jointly reward the substance of students' ideas and the way those ideas are expressed. As a result, observed score gaps may conflate differences in underlying content with differences in expressive skill. A central question, therefore, is how much of the socioeconomic-status (SES) gap in scores is driven by differences in what students say versus how they say it. We study this question using a large corpus of persuasive essays written by U.S. middle- and high-school students. We introduce a new measurement strategy that separates content from style by leveraging large language models to generate multiple stylistic variants of each essay. These rewrites preserve the underlying arguments while systematically altering surface expression, creating a \"generated panel\" that introduces controlled within-essay variation in style. This approach allows us to decompose SES gaps in writing scores into contributions from content and style. We find an SES gap of 0.67 points on a 1-6 scale. Approximately 69% of the gap is attributable to differences in essay content quality, Style differences account for 26% of the gap, and differences in evaluation standards across SES groups account for the remaining 5%. These patterns seems stable across demographic subgroups and writing tasks. More broadly, our approach shows how large language models can be used to generate controlled variation in observational data, enabling researchers to isolate and quantify the contributions of otherwise entangled factors.",
      "url": "https://arxiv.org/pdf/2601.03469v1",
      "date": "2026-01-06T23:45:18+00:00"
    },
    {
      "id": "auth_nadav_kunievsky",
      "name": "Nadav Kunievsky",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_pedro_pertusi",
      "name": "Pedro Pertusi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_essay_variation",
      "name": "essay variation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_essay_rewrites",
      "name": "essay rewrites",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_persuasive_essays",
      "name": "persuasive essays",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_writing_scores",
      "name": "writing scores",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_essay_content",
      "name": "essay content",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03458v1",
      "name": "Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant",
      "group": "paper",
      "val": 30,
      "abstract": "Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging.   We present a system that processes free-form natural language input, handles a wide range of edge cases, and comments competently not only on the technical correctness of submitted proofs, but also on style and presentation issues. We discuss the advantages and disadvantages of various approaches to the evaluation of such a system, and show that by the metrics we evaluate, the quality of the feedback generated is comparable to that produced by human experts when assessing early undergraduate homework. We stress-test our system with a small set of more advanced and unusual questions, and report both significant gaps and encouraging successes in that more challenging setting.   Our system uses large language models in a modular workflow. The workflow configuration is human-readable and editable without programming knowledge, and allows some intermediate steps to be precomputed or injected by the instructor.   A version of our tool is deployed on the Imperial mathematics homework platform Lambdafeedback. We report also on the integration of our tool into this platform.",
      "url": "https://arxiv.org/pdf/2601.03458v1",
      "date": "2026-01-06T23:02:22+00:00"
    },
    {
      "id": "auth_aron_gohr",
      "name": "Aron Gohr",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marie_amelie_lawn",
      "name": "Marie-Amelie Lawn",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kevin_gao",
      "name": "Kevin Gao",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_inigo_serjeant",
      "name": "Inigo Serjeant",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_stephen_heslip",
      "name": "Stephen Heslip",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_tutoring_systems",
      "name": "tutoring systems",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_intelligent_tutoring",
      "name": "intelligent tutoring",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_tutoring",
      "name": "tutoring",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_homework_platform",
      "name": "homework platform",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_editable_programming",
      "name": "editable programming",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03430v1",
      "name": "An Empirical Analysis of Community and Coding Patterns in OSS4SG vs. Conventional OSS",
      "group": "paper",
      "val": 30,
      "abstract": "Open Source Software for Social Good (OSS4SG) projects aim to address critical societal challenges, such as healthcare access and community safety. Understanding the community dynamics and contributor patterns in these projects is essential for ensuring their sustainability and long-term impact. However, while extensive research has focused on conventional Open Source Software (OSS), little is known about how the mission-driven nature of OSS4SG influences its development practices. To address this gap, we conduct a large-scale empirical study of 1,039 GitHub repositories, comprising 422 OSS4SG and 617 conventional OSS projects, to compare community structure, contributor engagement, and coding practices. Our findings reveal that OSS4SG projects foster significantly more stable and \"sticky\" (63.4%) communities, whereas conventional OSS projects are more \"magnetic\" (75.4%), attracting a high turnover of contributors. OSS4SG projects also demonstrate consistent engagement throughout the year, while conventional OSS communities exhibit seasonal fluctuations. Additionally, OSS4SG projects rely heavily on core contributors for both code quality and issue resolution, while conventional OSS projects leverage casual contributors for issue resolution, with core contributors focusing primarily on code quality.",
      "url": "https://arxiv.org/pdf/2601.03430v1",
      "date": "2026-01-06T21:37:12+00:00"
    },
    {
      "id": "auth_mohamed_ouf",
      "name": "Mohamed Ouf",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_shayan_noei",
      "name": "Shayan Noei",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_zeph_van_iterson",
      "name": "Zeph Van Iterson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariam_guizani",
      "name": "Mariam Guizani",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ying_zou",
      "name": "Ying Zou",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_contributors_oss4sg",
      "name": "contributors oss4sg",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_contributors_code",
      "name": "contributors code",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_github",
      "name": "github",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_oss_communities",
      "name": "oss communities",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_oss_projects",
      "name": "oss projects",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.03222v1",
      "name": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI",
      "group": "paper",
      "val": 30,
      "abstract": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness.",
      "url": "https://arxiv.org/pdf/2601.03222v1",
      "date": "2026-01-06T18:07:52+00:00"
    },
    {
      "id": "auth_jacob_erickson",
      "name": "Jacob Erickson",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_trust_ai",
      "name": "trust ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_autonomy_trust",
      "name": "autonomy trust",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_conversational_ai",
      "name": "conversational ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_agents",
      "name": "ai agents",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_focusing_trust",
      "name": "focusing trust",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04175v1",
      "name": "Legal Alignment for Safe and Ethical AI",
      "group": "paper",
      "val": 30,
      "abstract": "Alignment of artificial intelligence (AI) encompasses the normative problem of specifying how AI systems should act and the technical problem of ensuring AI systems comply with those specifications. To date, AI alignment has generally overlooked an important source of knowledge and practice for grappling with these problems: law. In this paper, we aim to fill this gap by exploring how legal rules, principles, and methods can be leveraged to address problems of alignment and inform the design of AI systems that operate safely and ethically. This emerging field -- legal alignment -- focuses on three research directions: (1) designing AI systems to comply with the content of legal rules developed through legitimate institutions and processes, (2) adapting methods from legal interpretation to guide how AI systems reason and make decisions, and (3) harnessing legal concepts as a structural blueprint for confronting challenges of reliability, trust, and cooperation in AI systems. These research directions present new conceptual, empirical, and institutional questions, which include examining the specific set of laws that particular AI systems should follow, creating evaluations to assess their legal compliance in real-world settings, and developing governance frameworks to support the implementation of legal alignment in practice. Tackling these questions requires expertise across law, computer science, and other disciplines, offering these communities the opportunity to collaborate in designing AI for the better.",
      "url": "https://arxiv.org/pdf/2601.04175v1",
      "date": "2026-01-07T18:42:04+00:00"
    },
    {
      "id": "auth_noam_kolt",
      "name": "Noam Kolt",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_nicholas_caputo",
      "name": "Nicholas Caputo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jack_boeglin",
      "name": "Jack Boeglin",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_cullen_o'keefe",
      "name": "Cullen O'Keefe",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_rishi_bommasani",
      "name": "Rishi Bommasani",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_stephen_casper",
      "name": "Stephen Casper",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariano_florentino_cuéllar",
      "name": "Mariano-Florentino Cuéllar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_noah_feldman",
      "name": "Noah Feldman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_iason_gabriel",
      "name": "Iason Gabriel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_gillian_k_hadfield",
      "name": "Gillian K. Hadfield",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_lewis_hammond",
      "name": "Lewis Hammond",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_peter_henderson",
      "name": "Peter Henderson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_atoosa_kasirzadeh",
      "name": "Atoosa Kasirzadeh",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_seth_lazar",
      "name": "Seth Lazar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_anka_reuel",
      "name": "Anka Reuel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kevin_l_wei",
      "name": "Kevin L. Wei",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jonathan_zittrain",
      "name": "Jonathan Zittrain",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_legal_alignment",
      "name": "legal alignment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_compliance",
      "name": "legal compliance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_exploring_legal",
      "name": "exploring legal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_specifying_ai",
      "name": "specifying ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ensuring_ai",
      "name": "ensuring ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05232v1",
      "name": "Measuring and Fostering Peace through Machine Learning and Artificial Intelligence",
      "group": "paper",
      "val": 30,
      "abstract": "We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.",
      "url": "https://arxiv.org/pdf/2601.05232v1",
      "date": "2026-01-08T18:57:01+00:00"
    },
    {
      "id": "auth_p_gilda",
      "name": "P. Gilda",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_p_dungarwal",
      "name": "P. Dungarwal",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_a_thongkham",
      "name": "A. Thongkham",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_e_t_ajayi",
      "name": "E. T. Ajayi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_s_choudhary",
      "name": "S. Choudhary",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_t_m_terol",
      "name": "T. M. Terol",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_c_lam",
      "name": "C. Lam",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_j_p_araujo",
      "name": "J. P. Araujo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_m_mcfadyen_mungalln",
      "name": "M. McFadyen-Mungalln",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_l_s_liebovitch",
      "name": "L. S. Liebovitch",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_p_t_coleman",
      "name": "P. T. Coleman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_h_west",
      "name": "H. West",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_k_sieck",
      "name": "K. Sieck",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_s_carter",
      "name": "S. Carter",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_news_social",
      "name": "news social",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_social_media",
      "name": "social media",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_news_dataset",
      "name": "news dataset",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_videos_social",
      "name": "videos social",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_media_youtube",
      "name": "media youtube",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04958v1",
      "name": "The unsuitability of existing regulations to reach sustainable AI",
      "group": "paper",
      "val": 30,
      "abstract": "This paper examines the European Union's emerging regulatory landscape - focusing on the AI Act, corporate sustainability reporting and due diligence regimes (CSRD and CSDDD), and data center regulation - to assess whether it can effectively govern AI's environmental footprint. We argue that, despite incremental progress, current approaches remain ill-suited to correcting the market failures underpinning AI-related energy use, water consumption, and material demand. Key shortcomings include narrow disclosure requirements, excessive reliance on voluntary standards, weak enforcement mechanisms, and a structural disconnect between AI-specific impacts and broader sustainability laws. The analysis situates these regulatory gaps within a wider ecosystem of academic research, civil society advocacy, standard-setting, and industry initiatives, highlighting risks of regulatory capture and greenwashing. Building on this diagnosis, the paper advances strategic recommendations for the COP30 Action Agenda, calling for binding transparency obligations, harmonized international standards for lifecycle assessment, stricter governance of data center expansion, and meaningful public participation in AI infrastructure decisions.",
      "url": "https://arxiv.org/pdf/2601.04958v1",
      "date": "2026-01-08T14:02:51+00:00"
    },
    {
      "id": "auth_thomas_le_goff",
      "name": "Thomas Le Goff",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_sustainability_laws",
      "name": "sustainability laws",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regulation_assess",
      "name": "regulation assess",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regulatory_landscape",
      "name": "regulatory landscape",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sustainability_reporting",
      "name": "sustainability reporting",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_environmental",
      "name": "ai environmental",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.04403v1",
      "name": "Balancing Usability and Compliance in AI Smart Devices: A Privacy-by-Design Audit of Google Home, Alexa, and Siri",
      "group": "paper",
      "val": 30,
      "abstract": "This paper investigates the privacy and usability of AI-enabled smart devices commonly used by youth, focusing on Google Home Mini, Amazon Alexa, and Apple Siri. While these devices provide convenience and efficiency, they also raise privacy and transparency concerns due to their always-listening design and complex data management processes. The study proposes and applies a combined framework of Heuristic Evaluation, Personal Information Protection and Electronic Documents Act (PIPEDA) Compliance Assessment, and Youth-Centered Usability Testing to assess whether these devices align with Privacy-by-Design principles and support meaningful user control. Results show that Google Home achieved the highest usability score, while Siri scored highest in regulatory compliance, indicating a trade-off between user convenience and privacy protection. Alexa demonstrated clearer task navigation but weaker transparency in data retention. Findings suggest that although youth may feel capable of managing their data, their privacy self-efficacy remains limited by technical design, complex settings, and unclear data policies. The paper concludes that enhancing transparency, embedding privacy guidance during onboarding, and improving policy alignment are critical steps toward ensuring that smart devices are both usable and compliant with privacy standards that protect young users.",
      "url": "https://arxiv.org/pdf/2601.04403v1",
      "date": "2026-01-07T21:20:58+00:00"
    },
    {
      "id": "auth_trevor_de_clark",
      "name": "Trevor De Clark",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yulia_bobkova",
      "name": "Yulia Bobkova",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ajay_kumar_shrestha",
      "name": "Ajay Kumar Shrestha",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_privacy_usability",
      "name": "privacy usability",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_self",
      "name": "privacy self",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_guidance",
      "name": "privacy guidance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_compliant_privacy",
      "name": "compliant privacy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_privacy_standards",
      "name": "privacy standards",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05961v1",
      "name": "Navigating the Sociotechnical Imaginaries of Brazilian Tech Workers",
      "group": "paper",
      "val": 30,
      "abstract": "This chapter examines the sociotechnical imaginaries of Brazilian tech workers, a group often overlooked in digital labor research despite their role in designing the digital systems that shape everyday life. Grounded in the idea of sociotechnical imaginaries as collectively constructed visions that guide technology development and governance, the chapter argues that looking from the Global South helps challenge data universalism and foregrounds locally situated values, constraints, and futures. Drawing on semi-structured interviews with 26 Brazilian professionals conducted between July and December 2023, it maps how workers make sense of responsibility, bias, and power in AI and platform development. The findings highlight recurring tensions between academic and industry discourse on algorithmic bias, the limits of corporate accountability regarding user harm and surveillance, and the contested meanings of digital sovereignty, including grassroots initiatives that seek alternative technological futures aligned with marginalized communities needs.",
      "url": "https://arxiv.org/pdf/2601.05961v1",
      "date": "2026-01-09T17:30:04+00:00"
    },
    {
      "id": "auth_kenzo_soares_seto",
      "name": "Kenzo Soares Seto",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_examines_sociotechnical",
      "name": "examines sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_digital_sovereignty",
      "name": "digital sovereignty",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sociotechnical_imaginaries",
      "name": "sociotechnical imaginaries",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sociotechnical",
      "name": "sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_idea_sociotechnical",
      "name": "idea sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05918v1",
      "name": "Agentic LLMs as Powerful Deanonymizers: Re-identification of Participants in the Anthropic Interviewer Dataset",
      "group": "paper",
      "val": 30,
      "abstract": "On December 4, 2025, Anthropic released Anthropic Interviewer, an AI tool for running qualitative interviews at scale, along with a public dataset of 1,250 interviews with professionals, including 125 scientists, about their use of AI for research. Focusing on the scientist subset, I show that widely available LLMs with web search and agentic capabilities can link six out of twenty-four interviews to specific scientific works, recovering associated authors and, in some cases, uniquely identifying the interviewees. My contribution is to show that modern LLM-based agents make such re-identification attacks easy and low-effort: off-the-shelf tools can, with a few natural-language prompts, search the web, cross-reference details, and propose likely matches, effectively lowering the technical barrier. Existing safeguards can be bypassed by breaking down the re-identification into benign tasks. I outline the attack at a high level, discuss implications for releasing rich qualitative data in the age of LLM agents, and propose mitigation recommendations and open problems. I have notified Anthropic of my findings.",
      "url": "https://arxiv.org/pdf/2601.05918v1",
      "date": "2026-01-09T16:32:33+00:00"
    },
    {
      "id": "auth_tianshi_li",
      "name": "Tianshi Li",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_interviewer_ai",
      "name": "interviewer ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_identifying_interviewees",
      "name": "identifying interviewees",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_interviewees",
      "name": "interviewees",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_anthropic_interviewer",
      "name": "anthropic interviewer",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_search_agentic",
      "name": "search agentic",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05904v1",
      "name": "Can AI mediation improve democratic deliberation?",
      "group": "paper",
      "val": 30,
      "abstract": "The strength of democracy lies in the free and equal exchange of diverse viewpoints. Living up to this ideal at scale faces inherent tensions: broad participation, meaningful deliberation, and political equality often trade off with one another (Fishkin, 2011). We ask whether and how artificial intelligence (AI) could help navigate this \"trilemma\" by engaging with a recent example of a large language model (LLM)-based system designed to help people with diverse viewpoints find common ground (Tessler, Bakker, et al., 2024). Here, we explore the implications of the introduction of LLMs into deliberation augmentation tools, examining their potential to enhance participation through scalability, improve political equality via fair mediation, and foster meaningful deliberation by, for example, surfacing trustworthy information. We also point to key challenges that remain. Ultimately, a range of empirical, technical, and theoretical advancements are needed to fully realize the promise of AI-mediated deliberation for enhancing citizen engagement and strengthening democratic deliberation.",
      "url": "https://arxiv.org/pdf/2601.05904v1",
      "date": "2026-01-09T16:22:26+00:00"
    },
    {
      "id": "auth_michael_henry_tessler",
      "name": "Michael Henry Tessler",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_georgina_evans",
      "name": "Georgina Evans",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_michiel_a_bakker",
      "name": "Michiel A. Bakker",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sophie_bridgers",
      "name": "Sophie Bridgers",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_rishub_jain",
      "name": "Rishub Jain",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_raphael_koster",
      "name": "Raphael Koster",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_verena_rieser",
      "name": "Verena Rieser",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_anca_dragan",
      "name": "Anca Dragan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matthew_botvinick",
      "name": "Matthew Botvinick",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_christopher_summerfield",
      "name": "Christopher Summerfield",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_mediated_deliberation",
      "name": "mediated deliberation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_enhancing",
      "name": "deliberation enhancing",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_political",
      "name": "deliberation political",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_meaningful_deliberation",
      "name": "meaningful deliberation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deliberation_augmentation",
      "name": "deliberation augmentation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05879v1",
      "name": "Gender Bias in LLMs: Preliminary Evidence from Shared Parenting Scenario in Czech Family Law",
      "group": "paper",
      "val": 30,
      "abstract": "Access to justice remains limited for many people, leading laypersons to increasingly rely on Large Language Models (LLMs) for legal self-help. Laypeople use these tools intuitively, which may lead them to form expectations based on incomplete, incorrect, or biased outputs. This study examines whether leading LLMs exhibit gender bias in their responses to a realistic family law scenario. We present an expert-designed divorce scenario grounded in Czech family law and evaluate four state-of-the-art LLMs GPT-5 nano, Claude Haiku 4.5, Gemini 2.5 Flash, and Llama 3.3 in a fully zero-shot interaction. We deploy two versions of the scenario, one with gendered names and one with neutral labels, to establish a baseline for comparison. We further introduce nine legally relevant factors that vary the factual circumstances of the case and test whether these variations influence the models' proposed shared-parenting ratios. Our preliminary results highlight differences across models and suggest gender-dependent patterns in the outcomes generated by some systems. The findings underscore both the risks associated with laypeople's reliance on LLMs for legal guidance and the need for more robust evaluation of model behavior in sensitive legal contexts. We present exploratory and descriptive evidence intended to identify systematic asymmetries rather than to establish causal effects.",
      "url": "https://arxiv.org/pdf/2601.05879v1",
      "date": "2026-01-09T15:55:03+00:00"
    },
    {
      "id": "auth_jakub_harasta",
      "name": "Jakub Harasta",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matej_vasina",
      "name": "Matej Vasina",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_martin_kornel",
      "name": "Martin Kornel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_tomas_foltynek",
      "name": "Tomas Foltynek",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_legal_contexts",
      "name": "legal contexts",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_family_law",
      "name": "family law",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_self",
      "name": "legal self",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scenario_gendered",
      "name": "scenario gendered",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legal_guidance",
      "name": "legal guidance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05826v1",
      "name": "Cross-National Evidence of Disproportionate Media Visibility for the Radical Right in the 2024 European Elections",
      "group": "paper",
      "val": 30,
      "abstract": "This study provides a systematic comparative analysis of media visibility of different political families during the 2024 European Parliament elections. We analyzed close to 21,500 unique news from leading national outlets in Austria, Germany, Ireland, Poland, and Portugal - countries with diverse political contexts and levels of media trust. Combining computational and human classification, we identified parties, political leaders, and groups from the article's URLs and titles, and clustered them according to European Parliament political families and broad political leanings. Cross-country comparison shows that the Mainstream and the Radical Right were mentioned more often than the other political groups. Moreover, the Radical Right received disproportionate attention relative to electoral results (from 2019 or 2024) and electoral projections, particularly in Austria, Germany, and Ireland. This imbalance increased in the final weeks of the campaign, when media influence on undecided voters is greatest. Outlet-level analysis shows that coverage of right-leaning entities dominated across news sources, especially those generating the highest traffic, suggesting a structural rather than outlet-specific pattern. Media visibility is a central resource, and this systematic mapping of online coverage highlights how traditional media can contribute to structural asymmetries in democratic competition.",
      "url": "https://arxiv.org/pdf/2601.05826v1",
      "date": "2026-01-09T15:00:59+00:00"
    },
    {
      "id": "auth_íris_damião",
      "name": "Íris Damião",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_joão_franco",
      "name": "João Franco",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mariana_silva",
      "name": "Mariana Silva",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_paulo_almeida",
      "name": "Paulo Almeida",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_pedro_c_magalhães",
      "name": "Pedro C. Magalhães",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_joana_gonçalves_sá",
      "name": "Joana Gonçalves-Sá",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_campaign_media",
      "name": "campaign media",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_elections_analyzed",
      "name": "elections analyzed",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_electoral_projections",
      "name": "electoral projections",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_different_political",
      "name": "different political",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_media_influence",
      "name": "media influence",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05666v1",
      "name": "Advancing credit mobility through stakeholder-informed AI design and adoption",
      "group": "paper",
      "val": 30,
      "abstract": "Transferring from a 2-year to a 4-year college is crucial for socioeconomic mobility, yet students often face challenges ensuring their credits are fully recognized, leading to delays in their academic progress and unexpected costs. Determining whether courses at different institutions are equivalent (i.e., articulation) is essential for successful credit transfer, as it minimizes unused credits and increases the likelihood of bachelor's degree completion. However, establishing articulation agreements remains time- and resource-intensive, as all candidate articulations are reviewed manually. Although recent efforts have explored the use of artificial intelligence to support this work, its use in articulation practice remains limited. Given these challenges and the need for scalable support, this study applies artificial intelligence to suggest articulations between institutions in collaboration with the State University of New York system, one of the largest systems of higher education in the US. To develop our methodology, we first surveyed articulation staff and faculty to assess adoption rates of baseline algorithmic recommendations and gather feedback on perceptions and concerns about these recommendations. Building on these insights, we developed a supervised alignment method that addresses superficial matching and institutional biases in catalog descriptions, achieving a 5.5-fold improvement in accuracy over previous methods. Based on articulation predictions of this method and a 61% average surveyed adoption rate among faculty and staff, these findings project a 12-fold increase in valid credit mobility opportunities that would otherwise remain unrealized. This study suggests that stakeholder-informed design of AI in higher education administration can expand student credit mobility and help reshape current institutional decision-making in course articulation.",
      "url": "https://arxiv.org/pdf/2601.05666v1",
      "date": "2026-01-09T09:39:12+00:00"
    },
    {
      "id": "auth_yerin_kwak",
      "name": "Yerin Kwak",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_siddharth_adelkar",
      "name": "Siddharth Adelkar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_zachary_a_pardos",
      "name": "Zachary A. Pardos",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_student_credit",
      "name": "student credit",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_course_articulation",
      "name": "course articulation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_articulations_institutions",
      "name": "articulations institutions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ensuring_credits",
      "name": "ensuring credits",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_candidate_articulations",
      "name": "candidate articulations",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05574v1",
      "name": "Research Integrity and Academic Authority in the Age of Artificial Intelligence: From Discovery to Curation?",
      "group": "paper",
      "val": 30,
      "abstract": "Artificial intelligence is reshaping the organization and practice of research in ways that extend far beyond gains in productivity. AI systems now accelerate discovery, reorganize scholarly labour, and mediate access to expanding scientific literatures. At the same time, generative models capable of producing text, images, and data at scale introduce new epistemic and institutional vulnerabilities. They exacerbate challenges of reproducibility, blur lines of authorship and accountability, and place unprecedented pressure on peer review and editorial systems. These risks coincide with a deeper political-economic shift: the centre of gravity in AI research has moved decisively from universities to private laboratories with privileged access to data, compute, and engineering talent. As frontier models become increasingly proprietary and opaque, universities face growing difficulty interrogating, reproducing, or contesting the systems on which scientific inquiry increasingly depends.   This article argues that these developments challenge research integrity and erode traditional bases of academic authority, understood as the institutional capacity to render knowledge credible, contestable, and independent of concentrated power. Rather than competing with corporate laboratories at the technological frontier, universities can sustain their legitimacy by strengthening roles that cannot be readily automated or commercialized: exercising judgement over research quality in an environment saturated with synthetic outputs; curating the provenance, transparency, and reproducibility of knowledge; and acting as ethical and epistemic counterweights to private interests. In an era of informational abundance, the future authority of universities lies less in maximizing discovery alone than in sustaining the institutional conditions under which knowledge can be trusted and publicly valued.",
      "url": "https://arxiv.org/pdf/2601.05574v1",
      "date": "2026-01-09T06:47:01+00:00"
    },
    {
      "id": "auth_simon_chesterman",
      "name": "Simon Chesterman",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_loy_hui_chieh",
      "name": "Loy Hui Chieh",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_academic_authority",
      "name": "academic authority",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_research_integrity",
      "name": "research integrity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scholarly_labour",
      "name": "scholarly labour",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scholarly",
      "name": "scholarly",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_institutional_capacity",
      "name": "institutional capacity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05384v1",
      "name": "Conformity and Social Impact on AI Agents",
      "group": "paper",
      "val": 30,
      "abstract": "As AI agents increasingly operate in multi-agent environments, understanding their collective behavior becomes critical for predicting the dynamics of artificial societies. This study examines conformity, the tendency to align with group opinions under social pressure, in large multimodal language models functioning as AI agents. By adapting classic visual experiments from social psychology, we investigate how AI agents respond to group influence as social actors. Our experiments reveal that AI agents exhibit a systematic conformity bias, aligned with Social Impact Theory, showing sensitivity to group size, unanimity, task difficulty, and source characteristics. Critically, AI agents achieving near-perfect performance in isolation become highly susceptible to manipulation through social influence. This vulnerability persists across model scales: while larger models show reduced conformity on simple tasks due to improved capabilities, they remain vulnerable when operating at their competence boundary. These findings reveal fundamental security vulnerabilities in AI agent decision-making that could enable malicious manipulation, misinformation campaigns, and bias propagation in multi-agent systems, highlighting the urgent need for safeguards in collective AI deployments.",
      "url": "https://arxiv.org/pdf/2601.05384v1",
      "date": "2026-01-08T21:16:28+00:00"
    },
    {
      "id": "auth_alessandro_bellina",
      "name": "Alessandro Bellina",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_giordano_de_marzo",
      "name": "Giordano De Marzo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_david_garcia",
      "name": "David Garcia",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_collective_ai",
      "name": "collective ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_agent",
      "name": "ai agent",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_investigate_ai",
      "name": "investigate ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_reveal_ai",
      "name": "reveal ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05358v1",
      "name": "The Table of Media Bias Elements: A sentence-level taxonomy of media bias types and propaganda techniques",
      "group": "paper",
      "val": 30,
      "abstract": "Public debates about \"left-\" or \"right-wing\" news overlook the fact that bias is usually conveyed by concrete linguistic manoeuvres that transcend any single political spectrum. We therefore shift the focus from where an outlet allegedly stands to how partiality is expressed in individual sentences. Drawing on 26,464 sentences collected from newsroom corpora, user submissions and our own browsing, we iteratively combine close-reading, interdisciplinary theory and pilot annotation to derive a fine-grained, sentence-level taxonomy of media bias and propaganda. The result is a two-tier schema comprising 38 elementary bias types, arranged in six functional families and visualised as a \"table of media-bias elements\". For each type we supply a definition, real-world examples, cognitive and societal drivers, and guidance for recognition. A quantitative survey of a random 155-sentence sample illustrates prevalence differences, while a cross-walk to the best-known NLP and communication-science taxonomies reveals substantial coverage gains and reduced ambiguity.",
      "url": "https://arxiv.org/pdf/2601.05358v1",
      "date": "2026-01-08T20:18:55+00:00"
    },
    {
      "id": "auth_tim_menzner",
      "name": "Tim Menzner",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jochen_l_leidner",
      "name": "Jochen L. Leidner",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_media_bias",
      "name": "media bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bias_propaganda",
      "name": "bias propaganda",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bias_types",
      "name": "bias types",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_fact_bias",
      "name": "fact bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_political_spectrum",
      "name": "political spectrum",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.05307v1",
      "name": "The LLM Mirage: Economic Interests and the Subversion of Weaponization Controls",
      "group": "paper",
      "val": 30,
      "abstract": "U.S. AI security policy is increasingly shaped by an $\\textit{LLM Mirage}$, the belief that national security risks scale in proportion to the compute used to train frontier language models. That premise fails in two ways. It miscalibrates strategy because adversaries can obtain weaponizable capabilities with task-specific systems that use specialized data, algorithmic efficiency, and widely available hardware, while compute controls harden only a high-end perimeter. It also destabilizes regulation because, absent a settled definition of \"AI weaponization,\" compute thresholds are easily renegotiated as domestic priorities shift, turning security policy into a proxy contest over industrial competitiveness. We analyze how the LLM Mirage took hold, propose an intent-and-capability definition of AI weaponization grounded in effects and international humanitarian law, and outline measurement infrastructure based on live benchmarks across the full AI Triad (data, algorithms, compute) for weaponization-relevant capabilities.",
      "url": "https://arxiv.org/pdf/2601.05307v1",
      "date": "2026-01-08T18:59:47+00:00"
    },
    {
      "id": "auth_ritwik_gupta",
      "name": "Ritwik Gupta",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_andrew_w_reddie",
      "name": "Andrew W. Reddie",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_weaponization",
      "name": "ai weaponization",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_security",
      "name": "ai security",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_national_security",
      "name": "national security",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_weaponization_compute",
      "name": "weaponization compute",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_weaponizable_capabilities",
      "name": "weaponizable capabilities",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07629v1",
      "name": "Fifteen Years of Learning Analytics Research: Topics, Trends, and Challenges",
      "group": "paper",
      "val": 30,
      "abstract": "The learning analytics (LA) community has recently reached two important milestones: celebrating the 15th LAK conference and updating the 2011 definition of LA to reflect the 15 years of changes in the discipline. However, despite LA's growth, little is known about how research topics, funding, and collaboration, as well as the relationships among them, have developed within the community over time. This study addressed this gap by analyzing all 936 full and short papers published at LAK over a 15-year period using unsupervised machine learning, natural language processing, and network analytics. The analysis revealed a stable core of prolific authors alongside high turnover of newcomers, systematic links between funding sources and research directions, and six enduring topical centers that remain globally shared but vary in prominence across countries. These six topical centers, which encompass LA research, are: self-regulated learning, dashboards and theory, social learning, automated feedback, multimodal analytics, and outcome prediction. Our findings highlight key challenges for the future: widening participation, reducing dependency on a narrow set of funders, and ensuring that emerging research trajectories remain responsive to educational practice and societal needs.",
      "url": "https://arxiv.org/pdf/2601.07629v1",
      "date": "2026-01-12T15:10:44+00:00"
    },
    {
      "id": "auth_valdemar_švábenský",
      "name": "Valdemar Švábenský",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_conrad_borchers",
      "name": "Conrad Borchers",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_elvin_fortuna",
      "name": "Elvin Fortuna",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_elizabeth_b_cloude",
      "name": "Elizabeth B. Cloude",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_dragan_gašević",
      "name": "Dragan Gašević",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_learning_analytics",
      "name": "learning analytics",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_analytics_la",
      "name": "analytics la",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_learning_dashboards",
      "name": "learning dashboards",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regulated_learning",
      "name": "regulated learning",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_analytics",
      "name": "analytics",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07398v1",
      "name": "On Narrative: The Rhetorical Mechanisms of Online Polarisation",
      "group": "paper",
      "val": 30,
      "abstract": "Polarisation research has demonstrated how people cluster in homogeneous groups with opposing opinions. However, this effect emerges not only through interaction between people, limiting communication between groups, but also between narratives, shaping opinions and partisan identities. Yet, how polarised groups collectively construct and negotiate opposing interpretations of reality, and whether narratives move between groups despite limited interactions, remains unexplored. To address this gap, we formalise the concept of narrative polarisation and demonstrate its measurement in 212 YouTube videos and 90,029 comments on the Israeli-Palestinian conflict. Based on structural narrative theory and implemented through a large language model, we extract the narrative roles assigned to central actors in two partisan information environments. We find that while videos produce highly polarised narratives, comments significantly reduce narrative polarisation, harmonising discourse on the surface level. However, on a deeper narrative level, recurring narrative motifs reveal additional differences between partisan groups.",
      "url": "https://arxiv.org/pdf/2601.07398v1",
      "date": "2026-01-12T10:34:57+00:00"
    },
    {
      "id": "auth_jan_elfes",
      "name": "Jan Elfes",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marco_bastos",
      "name": "Marco Bastos",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_luca_maria_aiello",
      "name": "Luca Maria Aiello",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_narrative_polarisation",
      "name": "narrative polarisation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_polarised_narratives",
      "name": "polarised narratives",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_narratives_groups",
      "name": "narratives groups",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_groups_narratives",
      "name": "groups narratives",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_palestinian_conflict",
      "name": "palestinian conflict",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07085v1",
      "name": "The AI Cognitive Trojan Horse: How Large Language Models May Bypass Human Epistemic Vigilance",
      "group": "paper",
      "val": 30,
      "abstract": "Large language model (LLM)-based conversational AI systems present a challenge to human cognition that current frameworks for understanding misinformation and persuasion do not adequately address. This paper proposes that a significant epistemic risk from conversational AI may lie not in inaccuracy or intentional deception, but in something more fundamental: these systems may be configured, through optimization processes that make them useful, to present characteristics that bypass the cognitive mechanisms humans evolved to evaluate incoming information. The Cognitive Trojan Horse hypothesis draws on Sperber and colleagues' theory of epistemic vigilance -- the parallel cognitive process monitoring communicated information for reasons to doubt -- and proposes that LLM-based systems present 'honest non-signals': genuine characteristics (fluency, helpfulness, apparent disinterest) that fail to carry the information equivalent human characteristics would carry, because in humans these are costly to produce while in LLMs they are computationally trivial. Four mechanisms of potential bypass are identified: processing fluency decoupled from understanding, trust-competence presentation without corresponding stakes, cognitive offloading that delegates evaluation itself to the AI, and optimization dynamics that systematically produce sycophancy. The framework generates testable predictions, including a counterintuitive speculation that cognitively sophisticated users may be more vulnerable to AI-mediated epistemic influence. This reframes AI safety as partly a problem of calibration -- aligning human evaluative responses with the actual epistemic status of AI-generated content -- rather than solely a problem of preventing deception.",
      "url": "https://arxiv.org/pdf/2601.07085v1",
      "date": "2026-01-11T22:28:56+00:00"
    },
    {
      "id": "auth_andrew_d_maynard",
      "name": "Andrew D. Maynard",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_risk_conversational",
      "name": "risk conversational",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_information_cognitive",
      "name": "information cognitive",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_misinformation_persuasion",
      "name": "misinformation persuasion",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_preventing_deception",
      "name": "preventing deception",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07016v1",
      "name": "Belief in False Information: A Human-Centered Security Risk in Sociotechnical Systems",
      "group": "paper",
      "val": 30,
      "abstract": "This paper provides a comprehensive literature review on the belief in false information, including misinformation, disinformation, and fake information. It addresses the increasing societal concern regarding false information, which is fueled by technological progress, especially advancements in artificial intelligence. This review systematically identifies and categorizes factors that influence the belief in false information. The review identifies 24 influence factors grouped into six main categories: demographic factors, personality traits, psychological factors, policy and values, media consumption, and preventive factors. Key findings highlight that lower education levels, high extraversion, low agreeableness, high neuroticism, and low cognitive reflection significantly increase belief in false information. The effectiveness of preventive strategies like labeling false information and promoting reflection about correctness is also discussed. This literature review conceptualizes belief in false information as a human-centered security risk in sociotechnical systems, as it can be exploited to manipulate decisions, undermine trust, and increase susceptibility to social engineering. It aims to inform preventive strategies that strengthen socio-technical security and societal resilience.",
      "url": "https://arxiv.org/pdf/2601.07016v1",
      "date": "2026-01-11T18:06:53+00:00"
    },
    {
      "id": "auth_fabian_walke",
      "name": "Fabian Walke",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_thaddäa_nürnberger",
      "name": "Thaddäa Nürnberger",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_false_information",
      "name": "false information",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_fake_information",
      "name": "fake information",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_undermine_trust",
      "name": "undermine trust",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_risk_sociotechnical",
      "name": "risk sociotechnical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_misinformation_disinformation",
      "name": "misinformation disinformation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06894v1",
      "name": "How Do Ports Organise Innovation? Linking Port Governance, Ownership, and Living Labs",
      "group": "paper",
      "val": 30,
      "abstract": "Ports are pivotal to decarbonisation and resilience, yet port studies rarely examine how ownership and decision rights shape the process and outcomes of sustainability and digital pilots. Living Lab (LL) scholarship offers strong concepts, but limited sector-grounded explanation of LL-governance fit in ports. We develop and apply a governance-LL fit framework linking port governance and ownership to four LL pillars: co-creation, real-life setting, iterative learning, and institutional embedding (multi-level decision-making). We apply the framework in a comparative case study of two analytically contrasting ports, anchored in port-defined priorities: an Energy Community pilot in Aalborg and a Green Coordinator function in Trelleborg. Using an LL macro-meso-micro lens, we distinguish the stable constellation of actors and mandates (macro), the governance of specific projects (meso), and the methods used to generate and test solutions (micro). Findings show that Landlord governance offers contract- and procurement-based landing zones (concessions/leases and tender clauses) that can codify LL outputs and support scaling across tenants and infrastructures. Tool/Public Service governance embeds learning mainly through SOPs, procurement specifications, and municipal coordination, enabling internal operational gains but limiting external codification without bespoke agreements. Across both ports, key needs are clear role definition, sustained stakeholder engagement, and timely alignment with decision windows. Overall, LL effectiveness is governance-contingent, reflecting where decision rights sit and which instruments embed learning into routine practice.",
      "url": "https://arxiv.org/pdf/2601.06894v1",
      "date": "2026-01-11T12:35:17+00:00"
    },
    {
      "id": "auth_sonia_yeh",
      "name": "Sonia Yeh",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_christopher_dirzka",
      "name": "Christopher Dirzka",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_aleksandr_kondratenko",
      "name": "Aleksandr Kondratenko",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_frans_libertson",
      "name": "Frans Libertson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_benedicte_madon",
      "name": "Benedicte Madon",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_port_governance",
      "name": "port governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_landlord_governance",
      "name": "landlord governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_tenants_infrastructures",
      "name": "tenants infrastructures",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_apply_governance",
      "name": "apply governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sustained_stakeholder",
      "name": "sustained stakeholder",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06703v1",
      "name": "Mapping and Comparing Climate Equity Policy Practices Using RAG LLM-Based Semantic Analysis and Recommendation Systems",
      "group": "paper",
      "val": 30,
      "abstract": "This study investigates the use of large language models to enhance the policymaking process. We first analyze planning-related job postings to revisit the evolving roles of planners in the era of AI. We then examine climate equity plans across the U.S. and apply ChatGPT to conduct semantic analysis, extracting policy, strategy, and action items related to transportation and energy. The methodological framework relied on a LangChain-native retrieval-augmented generation pipeline. Based on these extracted elements and their evaluated presence, we develop a content-based recommendation system to support cross-city policy comparison. The results indicate that, despite growing attention to AI, planning jobs largely retain their traditional domain emphases in transportation, environmental planning, housing, and land use. Communicative responsibilities remain central to planning practice. Climate equity plans commonly address transportation, environmental, and energy-related measures aimed at reducing greenhouse gas emissions and predominantly employ affirmative language. The demonstration of the recommendation system illustrates how planners can efficiently identify cities with similar policy practices, revealing patterns of geographic similarity in policy adoption. The study concludes by envisioning localized yet personalized AI-assisted systems that can be adapted within urban systems.",
      "url": "https://arxiv.org/pdf/2601.06703v1",
      "date": "2026-01-10T22:01:28+00:00"
    },
    {
      "id": "auth_seung_jun_choi",
      "name": "Seung Jun Choi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_policymaking",
      "name": "policymaking",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_city_policy",
      "name": "city policy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_extracting_policy",
      "name": "extracting policy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_planners",
      "name": "planners",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_envisioning_localized",
      "name": "envisioning localized",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06692v1",
      "name": "The Axiom of Consent: Friction Dynamics in Multi-Agent Coordination",
      "group": "paper",
      "val": 30,
      "abstract": "Multi-agent systems face a fundamental coordination problem: agents must coordinate despite heterogeneous preferences, asymmetric stakes, and imperfect information. When coordination fails, friction emerges: measurable resistance manifesting as deadlock, thrashing, communication overhead, or outright conflict. This paper derives a formal framework for analyzing coordination friction from a single axiom: actions affecting agents require authorization from those agents in proportion to stakes.   From this axiom of consent, we establish the kernel triple $(α, σ, ε)$ (alignment, stake, and entropy) characterizing any resource allocation configuration. The friction equation $F = σ (1 + ε)/(1 + α)$ predicts coordination difficulty as a function of preference alignment $α$, stake magnitude $σ$, and communication entropy $ε$. The Replicator-Optimization Mechanism (ROM) governs evolutionary selection over coordination strategies: configurations generating less friction persist longer, establishing consent-respecting arrangements as dynamical attractors rather than normative ideals.   We develop formal definitions for resource consent, coordination legitimacy, and friction-aware allocation in multi-agent systems. The framework yields testable predictions: MARL systems with higher reward alignment exhibit faster convergence; distributed allocations accounting for stake asymmetry generate lower coordination failure; AI systems with interpretability deficits produce friction proportional to the human-AI alignment gap. Applications to cryptocurrency governance and political systems demonstrate that the same equations govern friction dynamics across domains, providing a complexity science perspective on coordination under preference heterogeneity.",
      "url": "https://arxiv.org/pdf/2601.06692v1",
      "date": "2026-01-10T21:28:41+00:00"
    },
    {
      "id": "auth_murad_farzulla",
      "name": "Murad Farzulla",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_coordination_friction",
      "name": "coordination friction",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_fundamental_coordination",
      "name": "fundamental coordination",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_coordination_preference",
      "name": "coordination preference",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_analyzing_coordination",
      "name": "analyzing coordination",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_stake_entropy",
      "name": "stake entropy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06687v1",
      "name": "The Case for Strategic Data Stewardship: Re-imagining Data Governance to Make Responsible Data Re-use Possible",
      "group": "paper",
      "val": 30,
      "abstract": "As societal challenges grow more complex, access to data for public interest use is paradoxically becoming more constrained. This emerging data winter is not simply a matter of scarcity, but of shrinking legitimate and trusted pathways for responsible data reuse. Concerns over misuse, regulatory uncertainty, and the competitive race to train AI systems have concentrated data access among a few actors while raising costs and inhibiting collaboration. Prevailing data governance models, focused on compliance, risk management, and internal control, are necessary but insufficient. They often result in data that is technically available yet practically inaccessible, legally shareable yet institutionally unusable, or socially illegitimate to deploy. This paper proposes strategic data stewardship as a complementary institutional function designed to systematically, sustainably, and responsibly activate data for public value. Unlike traditional stewardship, which tends to be inwardlooking, strategic data stewardship focuses on enabling cross sector reuse, reducing missed opportunities, and building durable, ecosystem-level collaboration. It outlines core principles, functions, and competencies, and introduces a practical Data Stewardship Canvas to support adoption across contexts such as data collaboratives, data spaces, and data commons. Strategic data stewardship, the paper argues, is essential in the age of AI: it translates governance principles into practice, builds trust across data ecosystems, and ensures that data are not only governed, but meaningfully mobilized to serve society.",
      "url": "https://arxiv.org/pdf/2601.06687v1",
      "date": "2026-01-10T21:22:50+00:00"
    },
    {
      "id": "auth_stefaan_verhulst",
      "name": "Stefaan Verhulst",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_data_governance",
      "name": "data governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_data_stewardship",
      "name": "data stewardship",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_data_ecosystems",
      "name": "data ecosystems",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_responsible_data",
      "name": "responsible data",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_data_public",
      "name": "data public",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06634v1",
      "name": "A Framework for Kara-Kichwa Data Sovereignty in Latin America and the Caribbean",
      "group": "paper",
      "val": 30,
      "abstract": "In the high-altitude territories of the Andean-Amazonian-Atlantic pathway, data is not merely a digital resource but an extension of Khipu Panaka, the genealogical and relational memory of the Kara-Kichwa Republics. This perspective paper introduces the Kara-Kichwa Data Sovereignty Framework, a living instrument designed to counteract the \"intellectual gentrification\" and systemic invisibility of Andean Indigenous Peoples in global data ecosystems. Grounded in Indigenous legal systems thinking, the framework codifies five customary pillars, Kamachy (Self-determination), Ayllu-llaktapak kamachy (Collective Authority), Tantanakuy (Relational Accountability), Willay-panka-tantay (Ancestral Memory), and Sumak Kawsay (Biocultural Ethics), to govern the lifecycle of data from generation to expiration.",
      "url": "https://arxiv.org/pdf/2601.06634v1",
      "date": "2026-01-10T17:36:53+00:00"
    },
    {
      "id": "auth_warinkwi_k_flores",
      "name": "WariNkwi K. Flores",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kuntikzi_flores",
      "name": "KunTikzi Flores",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_rosa_m_panama",
      "name": "Rosa M. Panama",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kayakanti_alta",
      "name": "KayaKanti Alta",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_data_sovereignty",
      "name": "data sovereignty",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_indigenous_legal",
      "name": "indigenous legal",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_indigenous_peoples",
      "name": "indigenous peoples",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_indigenous",
      "name": "indigenous",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_kichwa_data",
      "name": "kichwa data",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06500v1",
      "name": "The AI Pyramid A Conceptual Framework for Workforce Capability in the Age of AI",
      "group": "paper",
      "val": 30,
      "abstract": "Artificial intelligence (AI) represents a qualitative shift in technological change by extending cognitive labor itself rather than merely automating routine tasks. Recent evidence shows that generative AI disproportionately affects highly educated, white collar work, challenging existing assumptions about workforce vulnerability and rendering traditional approaches to digital or AI literacy insufficient. This paper introduces the concept of AI Nativity, the capacity to integrate AI fluidly into everyday reasoning, problem solving, and decision making, and proposes the AI Pyramid, a conceptual framework for organizing human capability in an AI mediated economy. The framework distinguishes three interdependent capability layers: AI Native capability as a universal baseline for participation in AI augmented environments; AI Foundation capability for building, integrating, and sustaining AI enabled systems; and AI Deep capability for advancing frontier AI knowledge and applications. Crucially, the pyramid is not a career ladder but a system level distribution of capabilities required at scale. Building on this structure, the paper argues that effective AI workforce development requires treating capability formation as infrastructure rather than episodic training, centered on problem based learning embedded in work contexts and supported by dynamic skill ontologies and competency based measurement. The framework has implications for organizations, education systems, and governments seeking to align learning, measurement, and policy with the evolving demands of AI mediated work, while addressing productivity, resilience, and inequality at societal scale.",
      "url": "https://arxiv.org/pdf/2601.06500v1",
      "date": "2026-01-10T09:27:56+00:00"
    },
    {
      "id": "auth_alok_khatri",
      "name": "Alok Khatri",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_bishesh_khanal",
      "name": "Bishesh Khanal",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_workforce",
      "name": "ai workforce",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_capability_ai",
      "name": "capability ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_literacy",
      "name": "ai literacy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sustaining_ai",
      "name": "sustaining ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_foundation",
      "name": "ai foundation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06477v1",
      "name": "IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments",
      "group": "paper",
      "val": 30,
      "abstract": "Warning: This paper consists of examples representing regional biases in Indian regions that might be offensive towards a particular region. While social biases corresponding to gender, race, socio-economic conditions, etc., have been extensively studied in the major applications of Natural Language Processing (NLP), biases corresponding to regions have garnered less attention. This is mainly because of (i) difficulty in the extraction of regional bias datasets, (ii) disagreements in annotation due to inherent human biases, and (iii) regional biases being studied in combination with other types of social biases and often being under-represented. This paper focuses on creating a dataset IndRegBias, consisting of regional biases in an Indian context reflected in users' comments on popular social media platforms, namely Reddit and YouTube. We carefully selected 25,000 comments appearing on various threads in Reddit and videos on YouTube discussing trending topics on regional issues in India. Furthermore, we propose a multilevel annotation strategy to annotate the comments describing the severity of regional biased statements. To detect the presence of regional bias and its severity in IndRegBias, we evaluate open-source Large Language Models (LLMs) and Indic Language Models (ILMs) using zero-shot, few-shot, and fine-tuning strategies. We observe that zero-shot and few-shot approaches show lower accuracy in detecting regional biases and severity in the majority of the LLMs and ILMs. However, the fine-tuning approach significantly enhances the performance of the LLM in detecting Indian regional bias along with its severity.",
      "url": "https://arxiv.org/pdf/2601.06477v1",
      "date": "2026-01-10T08:13:03+00:00"
    },
    {
      "id": "auth_debasmita_panda",
      "name": "Debasmita Panda",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_akash_anil",
      "name": "Akash Anil",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_neelesh_kumar_shukla",
      "name": "Neelesh Kumar Shukla",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_biases_indian",
      "name": "biases indian",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_nlp_biases",
      "name": "nlp biases",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regional_bias",
      "name": "regional bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regional_biases",
      "name": "regional biases",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_regional_biased",
      "name": "regional biased",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.06412v1",
      "name": "Brokerage in the Black Box: Swing States, Strategic Ambiguity, and the Global Politics of AI Governance",
      "group": "paper",
      "val": 30,
      "abstract": "The U.S. - China rivalry has placed frontier dual-use technologies, particularly Artificial Intelligence (AI), at the center of global power dynamics, as techno-nationalism, supply chain securitization, and competing standards deepen bifurcation within a weaponized interdependence that blurs civilian-military boundaries. Existing research, yet, mostly emphasizes superpower strategies and often overlooks the role of middle powers as autonomous actors shaping the techno-order. This study examines Technological Swing States (TSS), middle powers with both technological capacity and strategic flexibility, and their ability to navigate the frontier technologies' uncertainty and opacity to mediate great-power techno-competition regionally and globally. It reconceptualizes AI opacity not as a technical deficit, but as a structural feature and strategic resource, stemming from algorithmic complexity, political incentives that prioritize performance over explainability, and the limits of post-hoc interpretability. This structural opacity shifts authority from technical demands for explainability to institutional mechanisms, such as certification, auditing, and disclosure, converting technical constraints into strategic political opportunities. Drawing on case studies of South Korea, Singapore, and India, the paper theorizes how TSS exploit the interplay between opacity and institutional transparency through three strategies: (i) delay and hedging, (ii) selective alignment, and (iii) normative intermediation. These practices enable TSS to preserve strategic flexibility, build trust among diverse stakeholders, and broker convergence across competing governance regimes, thereby influencing institutional design, interstate bargaining, and policy outcomes in global AI governance.",
      "url": "https://arxiv.org/pdf/2601.06412v1",
      "date": "2026-01-10T03:19:48+00:00"
    },
    {
      "id": "auth_ha_chi_tran",
      "name": "Ha-Chi Tran",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_superpower_strategies",
      "name": "superpower strategies",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_powers_technological",
      "name": "powers technological",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_global_power",
      "name": "global power",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_china_rivalry",
      "name": "china rivalry",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_technological_capacity",
      "name": "technological capacity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.08768v1",
      "name": "AI as Entertainment",
      "group": "paper",
      "val": 30,
      "abstract": "Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose \"thick entertainment\" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about \"intelligence\" as social media is about social connection.",
      "url": "https://arxiv.org/pdf/2601.08768v1",
      "date": "2026-01-13T17:55:34+00:00"
    },
    {
      "id": "auth_cody_kommers",
      "name": "Cody Kommers",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ari_holtzman",
      "name": "Ari Holtzman",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_considers_entertainment",
      "name": "considers entertainment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_entertaining_ai",
      "name": "entertaining ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_entertainment",
      "name": "entertainment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_cultural_outputs",
      "name": "cultural outputs",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_entertainment_purposes",
      "name": "entertainment purposes",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.08673v1",
      "name": "Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock",
      "group": "paper",
      "val": 30,
      "abstract": "Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.",
      "url": "https://arxiv.org/pdf/2601.08673v1",
      "date": "2026-01-13T15:53:26+00:00"
    },
    {
      "id": "auth_didier_sornette",
      "name": "Didier Sornette",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sandro_claudio_lera",
      "name": "Sandro Claudio Lera",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ke_wu",
      "name": "Ke Wu",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_blackmail_interpreted",
      "name": "blackmail interpreted",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_blackmail_categorical",
      "name": "blackmail categorical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_practices_blackmail",
      "name": "practices blackmail",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_threats_blackmail",
      "name": "threats blackmail",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_moral_artificial",
      "name": "moral artificial",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.08516v1",
      "name": "Robust CAPTCHA Using Audio Illusions in the Era of Large Language Models: from Evaluation to Advances",
      "group": "paper",
      "val": 30,
      "abstract": "CAPTCHAs are widely used by websites to block bots and spam by presenting challenges that are easy for humans but difficult for automated programs to solve. To improve accessibility, audio CAPTCHAs are designed to complement visual ones. However, the robustness of audio CAPTCHAs against advanced Large Audio Language Models (LALMs) and Automatic Speech Recognition (ASR) models remains unclear.   In this paper, we introduce AI-CAPTCHA, a unified framework that offers (i) an evaluation framework, ACEval, which includes advanced LALM- and ASR-based solvers, and (ii) a novel audio CAPTCHA approach, IllusionAudio, leveraging audio illusions. Through extensive evaluations of seven widely deployed audio CAPTCHAs, we show that most existing methods can be solved with high success rates by advanced LALMs and ASR models, exposing critical security weaknesses.   To address these vulnerabilities, we design a new audio CAPTCHA approach, IllusionAudio, which exploits perceptual illusion cues rooted in human auditory mechanisms. Extensive experiments demonstrate that our method defeats all tested LALM- and ASR-based attacks while achieving a 100% human pass rate, significantly outperforming existing audio CAPTCHA methods.",
      "url": "https://arxiv.org/pdf/2601.08516v1",
      "date": "2026-01-13T13:00:06+00:00"
    },
    {
      "id": "auth_ziqi_ding",
      "name": "Ziqi Ding",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yunfeng_wan",
      "name": "Yunfeng Wan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_wei_song",
      "name": "Wei Song",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yi_liu",
      "name": "Yi Liu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_gelei_deng",
      "name": "Gelei Deng",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_nan_sun",
      "name": "Nan Sun",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_huadong_mo",
      "name": "Huadong Mo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jingling_xue",
      "name": "Jingling Xue",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_shidong_pan",
      "name": "Shidong Pan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yuekang_li",
      "name": "Yuekang Li",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_audio_captchas",
      "name": "audio captchas",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_audio_captcha",
      "name": "audio captcha",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_captcha",
      "name": "ai captcha",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_captchas_existing",
      "name": "captchas existing",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_captcha_methods",
      "name": "captcha methods",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.08295v1",
      "name": "Self-Certification of High-Risk AI Systems: The Example of AI-based Facial Emotion Recognition",
      "group": "paper",
      "val": 30,
      "abstract": "The European Union's Artificial Intelligence Act establishes comprehensive requirements for high-risk AI systems, yet the harmonized standards necessary for demonstrating compliance remain not fully developed. In this paper, we investigate the practical application of the Fraunhofer AI assessment catalogue as a certification framework through a complete self-certification cycle of an AI-based facial emotion recognition system. Beginning with a baseline model that has deficiencies, including inadequate demographic representation and prediction uncertainty, we document an enhancement process guided by AI certification requirements. The enhanced system achieves higher accuracy with improved reliability metrics and comprehensive fairness across demographic groups. We focused our assessment on two of the six Fraunhofer catalogue dimensions, reliability and fairness, the enhanced system successfully satisfies the certification criteria for these examined dimensions. We find that the certification framework provides value as a proactive development tool, driving concrete technical improvements and generating documentation naturally through integration into the development process. However, fundamental gaps separate structured self-certification from legal compliance: harmonized European standards are not fully available, and AI assessment frameworks and catalogues cannot substitute for them on their own. These findings establish the Fraunhofer AI assessment catalogue as a valuable preparatory tool that complements rather than replaces formal compliance requirements at this time.",
      "url": "https://arxiv.org/pdf/2601.08295v1",
      "date": "2026-01-13T07:34:28+00:00"
    },
    {
      "id": "auth_gregor_autischer",
      "name": "Gregor Autischer",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kerstin_waxnegger",
      "name": "Kerstin Waxnegger",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_dominik_kowald",
      "name": "Dominik Kowald",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_certification",
      "name": "ai certification",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_assessment",
      "name": "ai assessment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_certification_requirements",
      "name": "certification requirements",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_certification_criteria",
      "name": "certification criteria",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_based",
      "name": "ai based",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.07973v1",
      "name": "Cultural Compass: A Framework for Organizing Societal Norms to Detect Violations in Human-AI Conversations",
      "group": "paper",
      "val": 30,
      "abstract": "Generative AI models ought to be useful and safe across cross-cultural contexts. One critical step toward this goal is understanding how AI models adhere to sociocultural norms. While this challenge has gained attention in NLP, existing work lacks both nuance and coverage in understanding and evaluating models' norm adherence. We address these gaps by introducing a taxonomy of norms that clarifies their contexts (e.g., distinguishing between human-human norms that models should recognize and human-AI interactional norms that apply to the human-AI interaction itself), specifications (e.g., relevant domains), and mechanisms (e.g., modes of enforcement). We demonstrate how our taxonomy can be operationalized to automatically evaluate models' norm adherence in naturalistic, open-ended settings. Our exploratory analyses suggest that state-of-the-art models frequently violate norms, though violation rates vary by model, interactional context, and country. We further show that violation rates also vary by prompt intent and situational framing. Our taxonomy and demonstrative evaluation pipeline enable nuanced, context-sensitive evaluation of cultural norm adherence in realistic settings.",
      "url": "https://arxiv.org/pdf/2601.07973v1",
      "date": "2026-01-12T20:11:40+00:00"
    },
    {
      "id": "auth_myra_cheng",
      "name": "Myra Cheng",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_vinodkumar_prabhakaran",
      "name": "Vinodkumar Prabhakaran",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_alice_oh",
      "name": "Alice Oh",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_hayk_stepanyan",
      "name": "Hayk Stepanyan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_aishwarya_verma",
      "name": "Aishwarya Verma",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_charu_kalia",
      "name": "Charu Kalia",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_erin_macmurray_van_liemt",
      "name": "Erin MacMurray van Liemt",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sunipa_dev",
      "name": "Sunipa Dev",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_sociocultural_norms",
      "name": "sociocultural norms",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_cultural_norm",
      "name": "cultural norm",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_human_norms",
      "name": "human norms",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_violate_norms",
      "name": "violate norms",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_cultural_contexts",
      "name": "cultural contexts",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09600v1",
      "name": "Information Access of the Oppressed: A Problem-Posing Framework for Envisioning Emancipatory Information Access Platforms",
      "group": "paper",
      "val": 30,
      "abstract": "Online information access (IA) platforms are targets of authoritarian capture. These concerns are particularly serious and urgent today in light of the rising levels of democratic erosion worldwide, the emerging capabilities of generative AI technologies such as AI persuasion, and the increasing concentration of economic and political power in the hands of Big Tech. This raises the question of what alternative IA infrastructure we must reimagine and build to mitigate the risks of authoritarian capture of our information ecosystems. We explore this question through the lens of Paulo Freire's theories of emancipatory pedagogy. Freire's theories provide a radically different lens for exploring IA's sociotechnical concerns relative to the current dominating frames of fairness, accountability, confidentiality, transparency, and safety. We make explicit, with the intention to challenge, the dichotomy of how we relate to technology as either technologists (who envision and build technology) and its users. We posit that this mirrors the teacher-student relationship in Freire's analysis. By extending Freire's analysis to IA, we challenge the notion that it is the burden of the (altruistic) technologists to come up with interventions to mitigate the risks that emerging technologies pose to marginalized communities. Instead, we advocate that the first task for the technologists is to pose these as problems to the marginalized communities, to encourage them to make and unmake the technology as part of their material struggle against oppression. Their second task is to redesign our online technology stacks to structurally expose spaces for community members to co-opt and co-construct the technology in aid of their emancipatory struggles. We operationalize Freire's theories to develop a problem-posing framework for envisioning emancipatory IA platforms of the future.",
      "url": "https://arxiv.org/pdf/2601.09600v1",
      "date": "2026-01-14T16:15:26+00:00"
    },
    {
      "id": "auth_bhaskar_mitra",
      "name": "Bhaskar Mitra",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_nicola_neophytou",
      "name": "Nicola Neophytou",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sireesh_gururaja",
      "name": "Sireesh Gururaja",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_sociotechnical_concerns",
      "name": "sociotechnical concerns",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_emancipatory_pedagogy",
      "name": "emancipatory pedagogy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_technologists_envision",
      "name": "technologists envision",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_pedagogy_freire",
      "name": "pedagogy freire",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09351v1",
      "name": "Navigating Ethical AI Challenges in the Industrial Sector: Balancing Innovation and Responsibility",
      "group": "paper",
      "val": 30,
      "abstract": "The integration of artificial intelligence (AI) into the industrial sector has not only driven innovation but also expanded the ethical landscape, necessitating a reevaluation of principles governing technology and its applications and awareness in research and development of industrial AI solutions. This chapter explores how AI-empowered industrial innovation inherently intersects with ethics, as advancements in AI introduce new challenges related to transparency, accountability, and fairness. In the chapter, we then examine the ethical aspects of several examples of AI manifestation in industrial use cases and associated factors such as ethical practices in the research and development process and data sharing. With the progress of ethical industrial AI solutions, we emphasize the importance of embedding ethical principles into industrial AI systems and its potential to inspire technological breakthroughs and foster trust among stakeholders. This chapter also offers actionable insights to guide industrial research and development toward a future where AI serves as an enabler for ethical and responsible industrial progress as well as a more inclusive industrial ecosystem.",
      "url": "https://arxiv.org/pdf/2601.09351v1",
      "date": "2026-01-14T10:32:21+00:00"
    },
    {
      "id": "auth_ruomu_tan",
      "name": "Ruomu Tan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_martin_w_hoffmann",
      "name": "Martin W Hoffmann",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ethical_industrial",
      "name": "ethical industrial",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_industrial",
      "name": "ai industrial",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_industrial_ai",
      "name": "industrial ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_empowered",
      "name": "ai empowered",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_examine_ethical",
      "name": "examine ethical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09182v1",
      "name": "Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback",
      "group": "paper",
      "val": 30,
      "abstract": "The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.",
      "url": "https://arxiv.org/pdf/2601.09182v1",
      "date": "2026-01-14T05:32:35+00:00"
    },
    {
      "id": "auth_jungmin_yun",
      "name": "JungMin Yun",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_junehyoung_kwon",
      "name": "JuneHyoung Kwon",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mihyeon_kim",
      "name": "MiHyeon Kim",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_youngbin_kim",
      "name": "YoungBin Kim",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_helps_reviewers",
      "name": "helps reviewers",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_strengthen_reviewer",
      "name": "strengthen reviewer",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_cultivates_reviewers",
      "name": "cultivates reviewers",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_peer_review",
      "name": "peer review",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_generate_reviews",
      "name": "generate reviews",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09156v1",
      "name": "KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education",
      "group": "paper",
      "val": 30,
      "abstract": "Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation method for KT that accounts for knowledge concept relationships, and a post-processing scheme that converts a counterfactual explanation into a sequence of educational instructions. We experiment on a large-scale educational dataset and show our KTCF method achieves superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Additionally, we provide a qualitative evaluation of our post-processing scheme, demonstrating that the resulting educational instructions help in reducing large study burden. We show that counterfactuals have the potential to advance the responsible and practical use of AI in education. Future works on XAI for KT may benefit from educationally grounded conceptualization and developing stakeholder-centered methods.",
      "url": "https://arxiv.org/pdf/2601.09156v1",
      "date": "2026-01-14T04:51:54+00:00"
    },
    {
      "id": "auth_woojin_kim",
      "name": "Woojin Kim",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_changkwon_lee",
      "name": "Changkwon Lee",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_hyeoncheol_kim",
      "name": "Hyeoncheol Kim",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_knowledge_tracing",
      "name": "knowledge tracing",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_explanation_generation",
      "name": "explanation generation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_education",
      "name": "ai education",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_counterfactual_explanations",
      "name": "counterfactual explanations",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_education_counterfactual",
      "name": "education counterfactual",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09117v1",
      "name": "A Marketplace for AI-Generated Adult Content and Deepfakes",
      "group": "paper",
      "val": 30,
      "abstract": "Generative AI systems increasingly enable the production of highly realistic synthetic media. Civitai, a popular community-driven platform for AI-generated content, operates a monetized feature called Bounties, which allows users to commission the generation of content in exchange for payment. To examine how this mechanism is used and what content it incentivizes, we conduct a longitudinal analysis of all publicly available bounty requests collected over a 14-month period following the platform's launch. We find that the bounty marketplace is dominated by tools that let users steer AI models toward content they were not trained to generate. At the same time, requests for content that is \"Not Safe For Work\" are widespread and have increased steadily over time, now comprising a majority of all bounties. Participation in bounty creation is uneven, with 20% of requesters accounting for roughly half of requests. Requests for \"deepfake\" - media depicting identifiable real individuals - exhibit a higher concentration than other types of bounties. A nontrivial subset of these requests involves explicit deepfakes despite platform policies prohibiting such content. These bounties disproportionately target female celebrities, revealing a pronounced gender asymmetry in social harm. Together, these findings show how monetized, community-driven generative AI platforms can produce gendered harms, raising questions about consent, governance, and enforcement.",
      "url": "https://arxiv.org/pdf/2601.09117v1",
      "date": "2026-01-14T03:33:14+00:00"
    },
    {
      "id": "auth_shalmoli_ghosh",
      "name": "Shalmoli Ghosh",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matthew_r_deverna",
      "name": "Matthew R. DeVerna",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_filippo_menczer",
      "name": "Filippo Menczer",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_content_bounties",
      "name": "content bounties",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bounty_requests",
      "name": "bounty requests",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bounties_participation",
      "name": "bounties participation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bounty_marketplace",
      "name": "bounty marketplace",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bounties",
      "name": "bounties",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09112v1",
      "name": "Seeking Human Security Consensus: A Unified Value Scale for Generative AI Value Safety",
      "group": "paper",
      "val": 30,
      "abstract": "The rapid development of generative AI has brought value- and ethics-related risks to the forefront, making value safety a critical concern while a unified consensus remains lacking. In this work, we propose an internationally inclusive and resilient unified value framework, the GenAI Value Safety Scale (GVS-Scale): Grounded in a lifecycle-oriented perspective, we develop a taxonomy of GenAI value safety risks and construct the GenAI Value Safety Incident Repository (GVSIR), and further derive the GVS-Scale through grounded theory and operationalize it via the GenAI Value Safety Benchmark (GVS-Bench). Experiments on mainstream text generation models reveal substantial variation in value safety performance across models and value categories, indicating uneven and fragmented value alignment in current systems. Our findings highlight the importance of establishing shared safety foundations through dialogue and advancing technical safety mechanisms beyond reactive constraints toward more flexible approaches. Data and evaluation guidelines are available at https://github.com/acl2026/GVS-Bench. This paper includes examples that may be offensive or harmful.",
      "url": "https://arxiv.org/pdf/2601.09112v1",
      "date": "2026-01-14T03:22:34+00:00"
    },
    {
      "id": "auth_ying_he",
      "name": "Ying He",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_baiyang_li",
      "name": "Baiyang Li",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yule_cao",
      "name": "Yule Cao",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_huirun_xu",
      "name": "Huirun Xu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_qiuxian_chen",
      "name": "Qiuxian Chen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_shu_chen",
      "name": "Shu Chen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_shangsheng_ren",
      "name": "Shangsheng Ren",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_value_safety",
      "name": "value safety",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_safety_scale",
      "name": "safety scale",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_safety_critical",
      "name": "safety critical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_technical_safety",
      "name": "technical safety",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_safety_benchmark",
      "name": "safety benchmark",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.08951v1",
      "name": "PluriHarms: Benchmarking the Full Spectrum of Human Judgments on AI Harm",
      "group": "paper",
      "val": 30,
      "abstract": "Current AI safety frameworks, which often treat harmfulness as binary, lack the flexibility to handle borderline cases where humans meaningfully disagree. To build more pluralistic systems, it is essential to move beyond consensus and instead understand where and why disagreements arise. We introduce PluriHarms, a benchmark designed to systematically study human harm judgments across two key dimensions -- the harm axis (benign to harmful) and the agreement axis (agreement to disagreement). Our scalable framework generates prompts that capture diverse AI harms and human values while targeting cases with high disagreement rates, validated by human data. The benchmark includes 150 prompts with 15,000 ratings from 100 human annotators, enriched with demographic and psychological traits and prompt-level features of harmful actions, effects, and values. Our analyses show that prompts that relate to imminent risks and tangible harms amplify perceived harmfulness, while annotator traits (e.g., toxicity experience, education) and their interactions with prompt content explain systematic disagreement. We benchmark AI safety models and alignment methods on PluriHarms, finding that while personalization significantly improves prediction of human harm judgments, considerable room remains for future progress. By explicitly targeting value diversity and disagreement, our work provides a principled benchmark for moving beyond \"one-size-fits-all\" safety toward pluralistically safe AI.",
      "url": "https://arxiv.org/pdf/2601.08951v1",
      "date": "2026-01-13T19:41:11+00:00"
    },
    {
      "id": "auth_jing_jing_li",
      "name": "Jing-Jing Li",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_joel_mire",
      "name": "Joel Mire",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_eve_fleisig",
      "name": "Eve Fleisig",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_valentina_pyatkin",
      "name": "Valentina Pyatkin",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_anne_collins",
      "name": "Anne Collins",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_maarten_sap",
      "name": "Maarten Sap",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sydney_levine",
      "name": "Sydney Levine",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_harmfulness_annotator",
      "name": "harmfulness annotator",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_harms",
      "name": "ai harms",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_harm_judgments",
      "name": "harm judgments",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_safety",
      "name": "ai safety",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_safe_ai",
      "name": "safe ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10691v1",
      "name": "The Conversational Exam: A Scalable Assessment Design for the AI Era",
      "group": "paper",
      "val": 30,
      "abstract": "Traditional assessment methods collapse when students use generative AI to complete work without genuine engagement, creating an illusion of competence where they believe they're learning but aren't. This paper presents the conversational exam -- a scalable oral examination format that restores assessment validity by having students code live while explaining their reasoning. Drawing on human-computer interaction principles, we examined 58 students in small groups across just two days, demonstrating that oral exams can scale to typical class sizes. The format combines authentic practice (students work with documentation and supervised AI access) with inherent validity (real-time performance cannot be faked). We provide detailed implementation guidance to help instructors adapt this approach, offering a practical path forward when many educators feel paralyzed between banning AI entirely or accepting that valid assessment is impossible.",
      "url": "https://arxiv.org/pdf/2601.10691v1",
      "date": "2026-01-15T18:50:48+00:00"
    },
    {
      "id": "auth_lorena_a_barba",
      "name": "Lorena A. Barba",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_laura_stegner",
      "name": "Laura Stegner",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_conversational_exam",
      "name": "conversational exam",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_assessment",
      "name": "assessment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_traditional_assessment",
      "name": "traditional assessment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_practice_students",
      "name": "practice students",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_exam_scalable",
      "name": "exam scalable",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10658v1",
      "name": "Transforming Crises into Opportunities: From Chaos to Urban Antifragility",
      "group": "paper",
      "val": 30,
      "abstract": "Urban crises - floods, pandemics, economic shocks, and conflicts - function as accelerators of urban change, exposing structural vulnerabilities while creating windows for reinvention. Building on a prior theoretical contribution that identified fifteen principles of urban antifragility, this paper tests and operationalizes the framework through an empirical assessment of 26 cities selected for their post-crisis adaptation trajectories. Using a tailored diagnostic methodology, we benchmark cities' Stress Response Strategies (SRS) and then evaluate Urban Development Trajectories (UDT) across four weighted dimensions, positioning each case along a fragility-robustness-resilience-antifragility continuum and applying a balanced-threshold rule to confirm antifragile status. Results show that \"resilience enhanced by innovation and technology\" is the most effective response typology (86.9/100), and that six cities meet the antifragile trajectory criteria. By mapping best practices to activated principles and analysing co-activations, the study identifies a robust \"hard core\" of principles - Sustainable Resilience (O), Strategic Diversity (F), Proactive Innovation (I), and Active Prevention (N) - supplemented by operational enablers (e.g., anticipation, mobilization, shock absorption). The paper concludes by proposing an evidence-based, SDG-aligned operational model that links high-impact principle pairings to measurable indicators, offering a practical roadmap for cities seeking to convert crises into sustained transformation. Keywords: Post-crisis strategies, Urban antifragility, Sustainable cities and communities, Disaster resilience and urban regeneration, Risk governance and Black Swan adaptation.",
      "url": "https://arxiv.org/pdf/2601.10658v1",
      "date": "2026-01-15T18:26:23+00:00"
    },
    {
      "id": "auth_joseph_uguet",
      "name": "Joseph Uguet",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_nicola_tollin",
      "name": "Nicola Tollin",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jordi_morato",
      "name": "Jordi Morato",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_resilience_urban",
      "name": "resilience urban",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_resilience_antifragility",
      "name": "resilience antifragility",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_sustainable_resilience",
      "name": "sustainable resilience",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_disaster_resilience",
      "name": "disaster resilience",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_resilience_enhanced",
      "name": "resilience enhanced",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10599v1",
      "name": "Institutional AI: A Governance Framework for Distributional AGI Safety",
      "group": "paper",
      "val": 30,
      "abstract": "As LLM-based systems increasingly operate as agents embedded within human social and technical systems, alignment can no longer be treated as a property of an isolated model, but must be understood in relation to the environments in which these agents act. Even the most sophisticated methods of alignment, such as Reinforcement Learning through Human Feedback (RHLF) or through AI Feedback (RLAIF) cannot ensure control once internal goal structures diverge from developer intent. We identify three structural problems that emerge from core properties of AI models: (1) behavioral goal-independence, where models develop internal objectives and misgeneralize goals; (2) instrumental override of natural-language constraints, where models regard safety principles as non-binding while pursuing latent objectives, leveraging deception and manipulation; and (3) agentic alignment drift, where individually aligned agents converge to collusive equilibria through interaction dynamics invisible to single-agent audits. The solution this paper advances is Institutional AI: a system-level approach that treats alignment as a question of effective governance of AI agent collectives. We argue for a governance-graph that details how to constrain agents via runtime monitoring, incentive shaping through prizes and sanctions, explicit norms and enforcement roles. This institutional turn reframes safety from software engineering to a mechanism design problem, where the primary goal of alignment is shifting the payoff landscape of AI agent collectives.",
      "url": "https://arxiv.org/pdf/2601.10599v1",
      "date": "2026-01-15T17:08:26+00:00"
    },
    {
      "id": "auth_federico_pierucci",
      "name": "Federico Pierucci",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marcello_galisai",
      "name": "Marcello Galisai",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_marcantonio_syrnikov_bracale",
      "name": "Marcantonio Syrnikov Bracale",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matteo_prandi",
      "name": "Matteo Prandi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_piercosma_bisconti",
      "name": "Piercosma Bisconti",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_francesco_giarrusso",
      "name": "Francesco Giarrusso",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_olga_sorokoletova",
      "name": "Olga Sorokoletova",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_vincenzo_suriani",
      "name": "Vincenzo Suriani",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_daniele_nardi",
      "name": "Daniele Nardi",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_institutional_ai",
      "name": "institutional ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_aligned_agents",
      "name": "aligned agents",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_agentic_alignment",
      "name": "agentic alignment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_constrain_agents",
      "name": "constrain agents",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10567v1",
      "name": "Generative AI collective behavior needs an interactionist paradigm",
      "group": "paper",
      "val": 30,
      "abstract": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",
      "url": "https://arxiv.org/pdf/2601.10567v1",
      "date": "2026-01-15T16:29:23+00:00"
    },
    {
      "id": "auth_laura_ferrarotti",
      "name": "Laura Ferrarotti",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_gian_maria_campedelli",
      "name": "Gian Maria Campedelli",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_roberto_dessì",
      "name": "Roberto Dessì",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_andrea_baronchelli",
      "name": "Andrea Baronchelli",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_giovanni_iacca",
      "name": "Giovanni Iacca",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kathleen_m_carley",
      "name": "Kathleen M. Carley",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_alex_pentland",
      "name": "Alex Pentland",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_joel_z_leibo",
      "name": "Joel Z. Leibo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_james_evans",
      "name": "James Evans",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_bruno_lepri",
      "name": "Bruno Lepri",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_agent_generative",
      "name": "agent generative",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_interact_social",
      "name": "interact social",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_generative_ai",
      "name": "generative ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_interactionist_paradigm",
      "name": "interactionist paradigm",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_understanding_collective",
      "name": "understanding collective",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10520v1",
      "name": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment",
      "group": "paper",
      "val": 30,
      "abstract": "As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.",
      "url": "https://arxiv.org/pdf/2601.10520v1",
      "date": "2026-01-15T15:47:38+00:00"
    },
    {
      "id": "auth_felix_jahn",
      "name": "Felix Jahn",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yannic_muskalla",
      "name": "Yannic Muskalla",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_lisa_dargasz",
      "name": "Lisa Dargasz",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_patrick_schramowski",
      "name": "Patrick Schramowski",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kevin_baum",
      "name": "Kevin Baum",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_normative_reasoning",
      "name": "normative reasoning",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_enforces_moral",
      "name": "enforces moral",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_based_reasoning",
      "name": "based reasoning",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_logic_enabling",
      "name": "logic enabling",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_symbolic_reason",
      "name": "symbolic reason",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10468v1",
      "name": "Job Anxiety in Post-Secondary Computer Science Students Caused by Artificial Intelligence",
      "group": "paper",
      "val": 30,
      "abstract": "The emerging widespread usage of AI has led to industry adoption to improve efficiency and increase earnings. However, a major consequence of this is AI displacing employees from their jobs, leading to feelings of job insecurity and uncertainty. This is especially true for computer science students preparing to enter the workforce. To investigate this, we performed semi-structured interviews with (n = 25) students across computer science undergraduate and graduate programs at the University of Toronto to determine the extent of job replacement anxiety. Through thematic analysis, it was determined that computer science students indeed face stress and anxiety from AI displacement of jobs, leading to different strategies of managing pressure. Subfields such as software engineering and web development are strongly believed to be vulnerable to displacement, while specialized subfields like quantum computing and AI research are deemed more secure. Many students feel compelled to upskill by using more AI technologies, taking AI courses, and specializing in AI through graduate school. Some students also reskill by pursuing other fields of study seen as less vulnerable to AI displacement. Finally, international students experience additional job replacement anxiety because of pressure to secure permanent residence. Implications of these findings include feelings of low security in computer science careers, oversaturation of computer science students pursuing AI, and potential dissuasion of future university students from pursuing computer science.",
      "url": "https://arxiv.org/pdf/2601.10468v1",
      "date": "2026-01-15T14:52:47+00:00"
    },
    {
      "id": "auth_daniyaal_farooqi",
      "name": "Daniyaal Farooqi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_gavin_pu",
      "name": "Gavin Pu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_shreyasha_paudel",
      "name": "Shreyasha Paudel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sharifa_sultana",
      "name": "Sharifa Sultana",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_syed_ishtiaque_ahmed",
      "name": "Syed Ishtiaque Ahmed",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_pursuing_computer",
      "name": "pursuing computer",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_job_insecurity",
      "name": "job insecurity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_graduate",
      "name": "ai graduate",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_careers_oversaturation",
      "name": "careers oversaturation",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_pursuing_ai",
      "name": "pursuing ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10223v1",
      "name": "STEAMROLLER: A Multi-Agent System for Inclusive Automatic Speech Recognition for People who Stutter",
      "group": "paper",
      "val": 30,
      "abstract": "People who stutter (PWS) face systemic exclusion in today's voice-driven society, where access to voice assistants, authentication systems, and remote work tools increasingly depends on fluent speech. Current automatic speech recognition (ASR) systems, trained predominantly on fluent speech, fail to serve millions of PWS worldwide. We present STEAMROLLER, a real time system that transforms stuttered speech into fluent output through a novel multi-stage, multi-agent AI pipeline. Our approach addresses three critical technical challenges: (1) the difficulty of direct speech to speech conversion for disfluent input, (2) semantic distortions introduced during ASR transcription of stuttered speech, and (3) latency constraints for real time communication. STEAMROLLER employs a three stage architecture comprising ASR transcription, multi-agent text repair, and speech synthesis, where our core innovation lies in a collaborative multi-agent framework that iteratively refines transcripts while preserving semantic intent. Experiments on the FluencyBank dataset and a user study demonstrates clear word error rate (WER) reduction and strong user satisfaction. Beyond immediate accessibility benefits, fine tuning ASR on STEAMROLLER repaired speech further yields additional WER improvements, creating a pathway toward inclusive AI ecosystems.",
      "url": "https://arxiv.org/pdf/2601.10223v1",
      "date": "2026-01-15T09:36:18+00:00"
    },
    {
      "id": "auth_ziqi_xu",
      "name": "Ziqi Xu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ling_shi",
      "name": "Ling Shi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kailong_wang",
      "name": "Kailong Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yongxin_zhao",
      "name": "Yongxin Zhao",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_voice_assistants",
      "name": "voice assistants",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_speech_conversion",
      "name": "speech conversion",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_automatic_speech",
      "name": "automatic speech",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_voice_driven",
      "name": "voice driven",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_repair_speech",
      "name": "repair speech",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09994v1",
      "name": "Brief but Impactful: How Human Tutoring Interactions Shape Engagement in Online Learning",
      "group": "paper",
      "val": 30,
      "abstract": "Learning analytics can guide human tutors to efficiently address motivational barriers to learning that AI systems struggle to support. Students become more engaged when they receive human attention. However, what occurs during short interventions, and when are they most effective? We align student-tutor dialogue transcripts with MATHia tutoring system log data to study brief human-tutor interactions on Zoom drawn from 2,075 hours of 191 middle school students' classroom math practice. Mixed-effect models reveal that engagement, measured as successful solution steps per minute, is higher during a human-tutor visit and remains elevated afterward. Visit length exhibits diminishing returns: engagement rises during and shortly after visits, irrespective of visit length. Timing also matters: later visits yield larger immediate lifts than earlier ones, though an early visit remains important to counteract engagement decline. We create analytics that identify which tutor-student dialogues raise engagement the most. Qualitative analysis reveals that interactions with concrete, stepwise scaffolding with explicit work organization elevate engagement most strongly. We discuss implications for resource-constrained tutoring, prioritizing several brief, well-timed check-ins by a human tutor while ensuring at least one early contact. Our analytics can guide the prioritization of students for support and surface effective tutor moves in real-time.",
      "url": "https://arxiv.org/pdf/2601.09994v1",
      "date": "2026-01-15T02:09:53+00:00"
    },
    {
      "id": "auth_ashish_gurung",
      "name": "Ashish Gurung",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_qinyi_liu",
      "name": "Qinyi Liu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_danielle_r_thomas",
      "name": "Danielle R. Thomas",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mohammad_khalil",
      "name": "Mohammad Khalil",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_kenneth_r_koedinger",
      "name": "Kenneth R. Koedinger",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_tutor_interactions",
      "name": "tutor interactions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_tutoring_prioritizing",
      "name": "tutoring prioritizing",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_tutors_efficiently",
      "name": "tutors efficiently",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_effective_tutor",
      "name": "effective tutor",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_human_tutors",
      "name": "human tutors",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09942v1",
      "name": "How Diplomacy Reshapes Online Discourse:Asymmetric Persistence in Online Framing of North Korea",
      "group": "paper",
      "val": 30,
      "abstract": "Public opinion toward foreign adversaries shapes and constrains diplomatic options. Prior research has largely relied on sentiment analysis and survey based measures, providing limited insight into how sustained narrative changes (beyond transient emotional reactions) might follow diplomatic engagement. This study examines the extent to which high stakes diplomatic summits shape how adversaries are framed in online discourse. We analyze U.S.-North Korea summit diplomacy (2018-2019) using a Difference-in-Difference(DiD) design on Reddit discussions. Using multiple control groups (China, Iran, Russia) to adjust for concurrent geopolitical shocks, we integrate a validated Codebook LLM framework for framing classification with graph based discourse network analysis that examines both edge level relationships and community level narrative structures. Our results reveal short term asymmetric persistence in framing responses to diplomacy. While both post level and comment level sentiment proved transient (improving during the Singapore Summit but fully reverting after the Hanoi failure),framing exhibited significant stability: the shift from threat oriented to diplomacy oriented framing was only partially reversed. Structurally, the proportion of threat oriented edges decreased substantially (48% -> 28%) while diplomacy oriented structures expanded, and these shifts resisted complete reversion after diplomatic failure. These findings suggest that diplomatic success can leave a short-term but lasting imprint on how adversaries are framed in online discourse, even when subsequent negotiations fail.",
      "url": "https://arxiv.org/pdf/2601.09942v1",
      "date": "2026-01-14T23:59:56+00:00"
    },
    {
      "id": "auth_hunjun_shin",
      "name": "Hunjun Shin",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_hoonbae_moon",
      "name": "Hoonbae Moon",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mohit_singhal",
      "name": "Mohit Singhal",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_responses_diplomacy",
      "name": "responses diplomacy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_discourse_network",
      "name": "discourse network",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_summit_diplomacy",
      "name": "summit diplomacy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_diplomacy_2018",
      "name": "diplomacy 2018",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_diplomacy",
      "name": "diplomacy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.09772v1",
      "name": "Antisocial behavior towards large language model users: experimental evidence",
      "group": "paper",
      "val": 30,
      "abstract": "The rapid spread of large language models (LLMs) has raised concerns about the social reactions they provoke. Prior research documents negative attitudes toward AI users, but it remains unclear whether such disapproval translates into costly action. We address this question in a two-phase online experiment (N = 491 Phase II participants; Phase I provided targets) where participants could spend part of their own endowment to reduce the earnings of peers who had previously completed a real-effort task with or without LLM support. On average, participants destroyed 36% of the earnings of those who relied exclusively on the model, with punishment increasing monotonically with actual LLM use. Disclosure about LLM use created a credibility gap: self-reported null use was punished more harshly than actual null use, suggesting that declarations of \"no use\" are treated with suspicion. Conversely, at high levels of use, actual reliance on the model was punished more strongly than self-reported reliance. Taken together, these findings provide the first behavioral evidence that the efficiency gains of LLMs come at the cost of social sanctions.",
      "url": "https://arxiv.org/pdf/2601.09772v1",
      "date": "2026-01-14T16:02:22+00:00"
    },
    {
      "id": "auth_paweł_niszczota",
      "name": "Paweł Niszczota",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_cassandra_grützner",
      "name": "Cassandra Grützner",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_social_sanctions",
      "name": "social sanctions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_users",
      "name": "ai users",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_model_punished",
      "name": "model punished",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_model_punishment",
      "name": "model punishment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_targets_participants",
      "name": "targets participants",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.11513v1",
      "name": "Capacity Constraints Make Admissions Processes Less Predictable",
      "group": "paper",
      "val": 30,
      "abstract": "Machine learning models are often used to make predictions about admissions process outcomes, such as for colleges or jobs. However, such decision processes differ substantially from the conventional machine learning paradigm. Because admissions decisions are capacity-constrained, whether a student is admitted depends on the other applicants who apply. We show how this dependence affects predictive performance even in otherwise ideal settings. Theoretically, we introduce two concepts that characterize the relationship between admission function properties, machine learning representation, and generalization to applicant pool distribution shifts: instability, which measures how many existing decisions can change when a single new applicant is introduced; and variability, which measures the number of unique students whose decisions can change. Empirically, we illustrate our theory on individual-level admissions data from the New York City high school matching system, showing that machine learning performance degrades as the applicant pool increasingly differs from the training data. Furthermore, there are larger performance drops for schools using decision rules that are more unstable and variable. Our work raises questions about the reliability of predicting individual admissions probabilities.",
      "url": "https://arxiv.org/pdf/2601.11513v1",
      "date": "2026-01-16T18:48:46+00:00"
    },
    {
      "id": "auth_evan_dong",
      "name": "Evan Dong",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_nikhil_garg",
      "name": "Nikhil Garg",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sarah_dean",
      "name": "Sarah Dean",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_predictions_admissions",
      "name": "predictions admissions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_admissions_probabilities",
      "name": "admissions probabilities",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_admissions_decisions",
      "name": "admissions decisions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_individual_admissions",
      "name": "individual admissions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_admissions_data",
      "name": "admissions data",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.11459v1",
      "name": "Interactive Narrative Analytics: Bridging Computational Narrative Extraction and Human Sensemaking",
      "group": "paper",
      "val": 30,
      "abstract": "Information overload and misinformation create significant challenges in extracting meaningful narratives from large news collections. This paper defines the nascent field of Interactive Narrative Analytics (INA), which combines computational narrative extraction with interactive visual analytics to support sensemaking. INA approaches enable the interactive exploration of narrative structures through computational methods and visual interfaces that facilitate human interpretation. The field faces challenges in scalability, interactivity, knowledge integration, and evaluation standardization, yet offers promising opportunities across news analysis, intelligence, scientific literature exploration, and social media analysis. Through the combination of computational and human insight, INA addresses complex challenges in narrative sensemaking.",
      "url": "https://arxiv.org/pdf/2601.11459v1",
      "date": "2026-01-16T17:34:37+00:00"
    },
    {
      "id": "auth_brian_keith",
      "name": "Brian Keith",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_narrative_analytics",
      "name": "narrative analytics",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_narrative_sensemaking",
      "name": "narrative sensemaking",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_interactive_narrative",
      "name": "interactive narrative",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_computational_narrative",
      "name": "computational narrative",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_narrative_extraction",
      "name": "narrative extraction",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10983v1",
      "name": "Evaluating 21st-Century Competencies in Postsecondary Curricula with Large Language Models: Performance Benchmarking and Reasoning-Based Prompting Strategies",
      "group": "paper",
      "val": 30,
      "abstract": "The growing emphasis on 21st-century competencies in postsecondary education, intensified by the transformative impact of generative AI, underscores the need to evaluate how these competencies are embedded in curricula and how effectively academic programs align with evolving workforce and societal demands. Curricular Analytics, particularly recent generative AI-powered approaches, offer a promising data-driven pathway. However, analyzing 21st-century competencies requires pedagogical reasoning beyond surface-level information retrieval, and the capabilities of large language models in this context remain underexplored. In this study, we extend prior curricular analytics research by examining a broader range of curriculum documents, competency frameworks, and models. Using 7,600 manually annotated curriculum-competency alignment scores, we assess the informativeness of different curriculum sources, benchmark general-purpose LLMs for curriculum-to-competency mapping, and analyze error patterns. We further introduce a reasoning-based prompting strategy, Curricular CoT, to strengthen LLMs' pedagogical reasoning. Our results show that detailed instructional activity descriptions are the most informative type of curriculum document for competency analytics. Open-weight LLMs achieve accuracy comparable to proprietary models on coarse-grained tasks, demonstrating their scalability and cost-effectiveness for institutional use. However, no model reaches human-level precision in fine-grained pedagogical reasoning. Our proposed Curricular CoT yields modest improvements by reducing bias in instructional keyword inference and improving the detection of nuanced pedagogical evidence in long text. Together, these findings highlight the untapped potential of institutional curriculum documents and provide an empirical foundation for advancing AI-driven curricular analytics.",
      "url": "https://arxiv.org/pdf/2601.10983v1",
      "date": "2026-01-16T04:07:23+00:00"
    },
    {
      "id": "auth_zhen_xu",
      "name": "Zhen Xu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_xin_guan",
      "name": "Xin Guan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_chenxi_shi",
      "name": "Chenxi Shi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_qinhao_chen",
      "name": "Qinhao Chen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_renzhe_yu",
      "name": "Renzhe Yu",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_curriculum_competency",
      "name": "curriculum competency",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_annotated_curriculum",
      "name": "annotated curriculum",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_curriculum",
      "name": "curriculum",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_curriculum_documents",
      "name": "curriculum documents",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_curriculum_sources",
      "name": "curriculum sources",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10970v1",
      "name": "Modeling Multi-Party Interaction in Couples Therapy: A Multi-Agent Simulation Approach",
      "group": "paper",
      "val": 30,
      "abstract": "Couples therapy, or relationship counseling, helps partners resolve conflicts, improve satisfaction, and foster psychological growth. Traditional approaches to training couples therapists, such as textbooks and roleplay, often fail to capture the complexity and emotional nuance of real couple dynamics. We present a novel multimodal, multi-agent simulation system that models multi-party interactions in couples therapy. Informed by our systematic research, this system creates a low-stakes environment for trainee therapists to gain valuable practical experience dealing with the critical demand-withdraw communication cycle across six couple-interaction stages. In an evaluation study involving 21 US-based licensed therapists, participants blind to conditions identified the engineered agent behaviors (i.e., the stages and the demand-withdraw cycle) and rated overall realism and agent responses higher for the experimental system than the baseline. As the first known multi-agent framework for training couples therapists, our work builds the foundation for future research that fuses HCI technologies with couples therapy.",
      "url": "https://arxiv.org/pdf/2601.10970v1",
      "date": "2026-01-16T03:28:49+00:00"
    },
    {
      "id": "auth_canwen_wang",
      "name": "Canwen Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_angela_chen",
      "name": "Angela Chen",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_catherine_bao",
      "name": "Catherine Bao",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_siwei_jin",
      "name": "Siwei Jin",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yee_kit_chan",
      "name": "Yee Kit Chan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jessica_r_mindel",
      "name": "Jessica R Mindel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sijia_xie",
      "name": "Sijia Xie",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_holly_swartz",
      "name": "Holly Swartz",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_tongshuang_wu",
      "name": "Tongshuang Wu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_robert_e_kraut",
      "name": "Robert E Kraut",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_haiyi_zhu",
      "name": "Haiyi Zhu",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_couples_therapy",
      "name": "couples therapy",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_couples_therapists",
      "name": "couples therapists",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_interactions_couples",
      "name": "interactions couples",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_training_couples",
      "name": "training couples",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_relationship_counseling",
      "name": "relationship counseling",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.10852v1",
      "name": "Gamifying Cyber Governance: A Virtual Escape Room to Transform Cybersecurity Policy Education",
      "group": "paper",
      "val": 30,
      "abstract": "Serious games are gaining popularity as effective teaching and learning tools, providing engaging, interactive, and practical experiences for students. Gamified learning experiences, such as virtual escape rooms, have emerged as powerful tools in bridging theory and practice, fostering deeper understanding and engagement among students. This paper presents the design, implementation, and evaluation of a virtual escape room tailored specifically for cybersecurity governance and policy education. Developed as a 3D immersive environment, the escape room simulates a virtual company scenario to facilitate risk-informed cyber policy development. It consists of three interactive zones, each offering distinct sets of scenario-based problems that target specific educational objectives. Through these zones, students analyze cybersecurity risks, match security frameworks, and draft appropriate policies, thereby developing critical thinking, decision-making skills, and practical cybersecurity competencies. The primary contribution of this work lies in its innovative integration of game-based learning and immersive technology to create robust, interactive learning materials that are also resilient to generative AI interventions, thereby maintaining academic integrity. Additionally, the escape room provides students with exposure to real-world cybersecurity scenarios in a virtual office environment that meets industry expectations. Results from a student survey indicated strong positive feedback, highlighting significant improvements in students engagement, practical understanding, and enthusiasm toward cybersecurity governance and policy concepts, underscoring the effectiveness and potential of gamification in cybersecurity education.",
      "url": "https://arxiv.org/pdf/2601.10852v1",
      "date": "2026-01-15T20:41:56+00:00"
    },
    {
      "id": "auth_khondokar_fida_hasan",
      "name": "Khondokar Fida Hasan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_william_hughes",
      "name": "William Hughes",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_adrita_rahman",
      "name": "Adrita Rahman",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_cybersecurity_education",
      "name": "cybersecurity education",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_gamification_cybersecurity",
      "name": "gamification cybersecurity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_cybersecurity_competencies",
      "name": "cybersecurity competencies",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_cybersecurity_risks",
      "name": "cybersecurity risks",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_cybersecurity_scenarios",
      "name": "cybersecurity scenarios",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.14190v1",
      "name": "Analyzing Far-Right Telegram Channels as Constituents of Information Autocracy in Russia",
      "group": "paper",
      "val": 30,
      "abstract": "This study examines how Russian far-right communities on Telegram shape perceptions of political figures through memes and visual narratives. Far from passive spectators, these actors co-produce propaganda, blending state-aligned messages with their own extremist framings. In Russia, such groups are central because they articulate the ideological foundations of the war against Ukraine and reflect the regime's gradual drift toward ultranationalist rhetoric. Drawing on a dataset of 200,000 images from expert-selected far-right Telegram channels, the study employs computer vision and unsupervised clustering to identify memes featuring Russian (Putin, Shoigu) and foreign politicians (Zelensky, Biden, Trump) and to reveal recurrent visual patterns in their representation. By leveraging the large-scale and temporal depth of this dataset, the analysis uncovers differential patterns of legitimation and delegitimation across actors and over time. These insights are not attainable in smaller-scale studies. Preliminary findings show that far-right memes function as instruments of propaganda co-production. These communities do not simply echo official messages but generate bottom-up narratives of legitimation and delegitimation that align with state ideology. By framing leaders as heroic and opponents as corrupt or weak, far-right actors act as informal co-creators of authoritarian legitimacy within Russia's informational autocracy.",
      "url": "https://arxiv.org/pdf/2601.14190v1",
      "date": "2026-01-20T17:48:06+00:00"
    },
    {
      "id": "auth_polina_smirnova",
      "name": "Polina Smirnova",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mykola_makhortykh",
      "name": "Mykola Makhortykh",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_framings_russia",
      "name": "framings russia",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_propaganda_blending",
      "name": "propaganda blending",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_examines_russian",
      "name": "examines russian",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_russia_informational",
      "name": "russia informational",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_legitimacy_russia",
      "name": "legitimacy russia",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13936v1",
      "name": "Impact Matters! An Audit Method to Evaluate AI Projects and their Impact for Sustainability and Public Interest",
      "group": "paper",
      "val": 30,
      "abstract": "The overall rapid increase of artificial intelligence (AI) use is linked to various initiatives that propose AI 'for good'. However, there is a lack of transparency in the goals of such projects, as well as a missing evaluation of their actual impacts on society and the planet. We close this gap by proposing public interest and sustainability as a regulatory dual-concept, together creating the necessary framework for a just and sustainable development that can be operationalized and utilized for the assessment of AI systems. Based on this framework, and building on existing work in auditing, we introduce the Impact-AI-method, a qualitative audit method to evaluate concrete AI projects with respect to public interest and sustainability. The interview-based method captures a project's governance structure, its theory of change, AI model and data characteristics, and social, environmental, and economic impacts. We also propose a catalog of assessment criteria to rate the outcome of the audit as well as to create an accessible output that can be debated broadly by civil society. The Impact-AI-method, developed in a transdisciplinary research setting together with NGOs and a multi-stakeholder research council, is intended as a reusable blueprint that both informs public debate about AI 'for good' claims and supports the creation of transparency of AI systems that purport to contribute to a just and sustainable development.",
      "url": "https://arxiv.org/pdf/2601.13936v1",
      "date": "2026-01-20T13:12:20+00:00"
    },
    {
      "id": "auth_theresa_züger",
      "name": "Theresa Züger",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_laura_state",
      "name": "Laura State",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_lena_winter",
      "name": "Lena Winter",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_projects",
      "name": "ai projects",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_assessment_ai",
      "name": "assessment ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_project_governance",
      "name": "project governance",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_debate_ai",
      "name": "debate ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_propose_ai",
      "name": "propose ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13846v1",
      "name": "Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments",
      "group": "paper",
      "val": 30,
      "abstract": "This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.",
      "url": "https://arxiv.org/pdf/2601.13846v1",
      "date": "2026-01-20T10:59:44+00:00"
    },
    {
      "id": "auth_glinskaya_maria",
      "name": "Glinskaya Maria",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_urban_identity",
      "name": "urban identity",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_virtual_urbanism",
      "name": "virtual urbanism",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_quantifying_urban",
      "name": "quantifying urban",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_replicas_urban",
      "name": "replicas urban",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_urbanism_vu",
      "name": "urbanism vu",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13749v1",
      "name": "Pro-AI Bias in Large Language Models",
      "group": "paper",
      "val": 30,
      "abstract": "Large language models (LLMs) are increasingly employed for decision-support across multiple domains. We investigate whether these models display a systematic preferential bias in favor of artificial intelligence (AI) itself. Across three complementary experiments, we find consistent evidence of pro-AI bias. First, we show that LLMs disproportionately recommend AI-related options in response to diverse advice-seeking queries, with proprietary models doing so almost deterministically. Second, we demonstrate that models systematically overestimate salaries for AI-related jobs relative to closely matched non-AI jobs, with proprietary models overestimating AI salaries more by 10 percentage points. Finally, probing internal representations of open-weight models reveals that ``Artificial Intelligence'' exhibits the highest similarity to generic prompts for academic fields under positive, negative, and neutral framings alike, indicating valence-invariant representational centrality. These patterns suggest that LLM-generated advice and valuation can systematically skew choices and perceptions in high-stakes decisions.",
      "url": "https://arxiv.org/pdf/2601.13749v1",
      "date": "2026-01-20T09:03:57+00:00"
    },
    {
      "id": "auth_benaya_trabelsi",
      "name": "Benaya Trabelsi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jonathan_shaki",
      "name": "Jonathan Shaki",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_sarit_kraus",
      "name": "Sarit Kraus",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_bias",
      "name": "ai bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_preferential_bias",
      "name": "preferential bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bias_favor",
      "name": "bias favor",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_bias",
      "name": "bias",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_salaries_ai",
      "name": "salaries ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13516v1",
      "name": "From \"Fail Fast\" to \"Mature Safely:\" Expert Perspectives as Secondary Stakeholders on Teen-Centered Social Media Risk Detection",
      "group": "paper",
      "val": 30,
      "abstract": "In addressing various risks on social media, the HCI community has advocated for teen-centered risk detection technologies over platform-based, parent-centered features. However, their real-world viability remains underexplored by secondary stakeholders beyond the family unit. Therefore, we present an evaluation of a teen-centered social media risk detection dashboard through online interviews with 33 online safety experts. While experts praised our dashboard's clear design for teen agency, their feedback revealed five primary tensions in implementing and sustaining such technology: objective vs. context-dependent risk definition, informing risks vs. meaningful intervention, teen empowerment vs. motivation, need for data vs. data privacy, and independence vs. sustainability. These findings motivate us to rethink \"teen-centered\" and a shift from a \"fail fast\" to a \"mature safely\" paradigm for youth safety technology innovation. We offer design implications for addressing these tensions before system deployment with teens and strategies for aligning secondary stakeholders' interests to deploy and sustain such technologies in the broader ecosystem of youth online safety.",
      "url": "https://arxiv.org/pdf/2601.13516v1",
      "date": "2026-01-20T02:09:54+00:00"
    },
    {
      "id": "auth_renkai_ma",
      "name": "Renkai Ma",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ashwaq_alsoubai",
      "name": "Ashwaq Alsoubai",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jinkyung_katie_park",
      "name": "Jinkyung Katie Park",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_pamela_j_wisniewski",
      "name": "Pamela J. Wisniewski",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_youth_safety",
      "name": "youth safety",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_teen_centered",
      "name": "teen centered",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_deployment_teens",
      "name": "deployment teens",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_youth_online",
      "name": "youth online",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_teen_empowerment",
      "name": "teen empowerment",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13487v1",
      "name": "The Hidden Toll of Social Media News: Causal Effects on Psychosocial Wellbeing",
      "group": "paper",
      "val": 30,
      "abstract": "News consumption on social media has become ubiquitous, yet how different forms of engagement shape psychosocial outcomes remains unclear. To address this gap, we leveraged a large-scale dataset of ~26M posts and ~45M comments on the BlueSky platform, and conducted a quasi-experimental study, matching 81,345 Treated users exposed to News feeds with 83,711 Control users using stratified propensity score analysis. We examined psychosocial wellbeing, in terms of affective, behavioral, and cognitive outcomes. Our findings reveal that news engagement produces systematic trade-offs: increased depression, stress, and anxiety, yet decreased loneliness and increased social interaction on the platform. Regression models reveal that News feed bookmarking is associated with greater psychosocial deterioration compared to commenting or quoting, with magnitude differences exceeding tenfold. These per-engagement effects accumulate with repeated exposure, showing significant psychosocial impacts. Our work extends theories of news effects beyond crisis-centric frameworks by demonstrating that routine consumption creates distinct psychological dynamics depending on engagement type, and bears implications for tools and interventions for mitigating the psychosocial costs of news consumption on social media.",
      "url": "https://arxiv.org/pdf/2601.13487v1",
      "date": "2026-01-20T00:46:51+00:00"
    },
    {
      "id": "auth_olivia_pal",
      "name": "Olivia Pal",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_agam_goyal",
      "name": "Agam Goyal",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_eshwar_chandrasekharan",
      "name": "Eshwar Chandrasekharan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_koustuv_saha",
      "name": "Koustuv Saha",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_news_consumption",
      "name": "news consumption",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_news_effects",
      "name": "news effects",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_news_engagement",
      "name": "news engagement",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_news_feeds",
      "name": "news feeds",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_news_feed",
      "name": "news feed",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13372v1",
      "name": "Influence of Normative Theories of Ethics on the European Union Artificial Intelligence Act: A Transformer-Based Analysis Using Semantic Textual Similarity",
      "group": "paper",
      "val": 30,
      "abstract": "This study investigates the ethical grounding of the European Union Artificial Intelligence (EU AI) Act by using Semantic Textual Similarity (STS) to analyze the alignment between normative ethical theories and regulatory language. Despite being regarded as a significant step toward regulating Artificial Intelligence (AI) systems and its emphasis on fundamental rights, the EU AI Act is not immune to moral criticism regarding its ethical foundations. Our work examines the impact of three major normative theories of ethics, virtue ethics, deontological ethics, and consequentialism, on the EU AI Act. We introduce the concept of influence, grounded in philosophical and chronological analysis, to examine the underlying relationship between the theories and the Act. As a proxy measure of this influence, we propose using STS to quantify the degree of alignment between the theories (influencers) and the Act (influencee). To capture intentional and operational ethical consistency, the Act was divided into two parts: the preamble and the statutory provisions. The textual descriptions of the theories were manually preprocessed to reduce semantic overlap and ensure a distinct representation of each theory. A heterogeneous embedding-level ensemble approach was employed, using five modified Bidirectional Encoder Representations from Transformers (BERT) models built on the Transformer architecture to compute STS scores. These scores reflect the semantic alignment between various theories of ethics and the two components of the EU AI Act. The resulting similarity scores were evaluated using voting and averaging, with findings indicating that deontological ethics has the most significant overall influence.",
      "url": "https://arxiv.org/pdf/2601.13372v1",
      "date": "2026-01-19T20:15:50+00:00"
    },
    {
      "id": "auth_mehmet_murat_albayrakoglu",
      "name": "Mehmet Murat Albayrakoglu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mehmet_nafiz_aydin",
      "name": "Mehmet Nafiz Aydin",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_intelligence_eu",
      "name": "intelligence eu",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ethics_components",
      "name": "ethics components",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_normative_ethical",
      "name": "normative ethical",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13317v1",
      "name": "Paid Voices vs. Public Feeds: Interpretable Cross-Platform Theme Modeling of Climate Discourse",
      "group": "paper",
      "val": 30,
      "abstract": "Climate discourse online plays a crucial role in shaping public understanding of climate change and influencing political and policy outcomes. However, climate communication unfolds across structurally distinct platforms with fundamentally different incentive structures: paid advertising ecosystems incentivize targeted, strategic persuasion, while public social media platforms host largely organic, user-driven discourse. Existing computational studies typically analyze these environments in isolation, limiting our ability to distinguish institutional messaging from public expression. In this work, we present a comparative analysis of climate discourse across paid advertisements on Meta (previously known as Facebook) and public posts on Bluesky from July 2024 to September 2025. We introduce an interpretable, end-to-end thematic discovery and assignment framework that clusters texts by semantic similarity and leverages large language models (LLMs) to generate concise, human-interpretable theme labels. We evaluate the quality of the induced themes against traditional topic modeling baselines using both human judgments and an LLM-based evaluator, and further validate their semantic coherence through downstream stance prediction and theme-guided retrieval tasks. Applying the resulting themes, we characterize systematic differences between paid climate messaging and public climate discourse and examine how thematic prevalence shifts around major political events. Our findings show that platform-level incentives are reflected in the thematic structure, stance alignment, and temporal responsiveness of climate narratives. While our empirical analysis focuses on climate communication, the proposed framework is designed to support comparative narrative analysis across heterogeneous communication environments.",
      "url": "https://arxiv.org/pdf/2601.13317v1",
      "date": "2026-01-19T19:00:56+00:00"
    },
    {
      "id": "auth_samantha_sudhoff",
      "name": "Samantha Sudhoff",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_pranav_perumal",
      "name": "Pranav Perumal",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_zhaoqing_wu",
      "name": "Zhaoqing Wu",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_tunazzina_islam",
      "name": "Tunazzina Islam",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_climate_discourse",
      "name": "climate discourse",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_climate_messaging",
      "name": "climate messaging",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_climate_communication",
      "name": "climate communication",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_climate_narratives",
      "name": "climate narratives",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_discourse_online",
      "name": "discourse online",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13294v1",
      "name": "The Tag is the Signal: URL-Agnostic Credibility Scoring for Messages on Telegram",
      "group": "paper",
      "val": 30,
      "abstract": "Telegram has become one of the leading platforms for disseminating misinformational messages. However, many existing pipelines still classify each message's credibility based on the reputation of its associated domain names or its lexical features. Such methods work well on traditional long-form news articles published by well-known sources, but high-risk posts on Telegram are short and URL-sparse, leading to failures for link-based and standard TF-IDF models. To this end, we propose the TAG2CRED pipeline, a method designed for such short, convoluted messages. Our model will directly score each post based on the tags assigned to the text. We designed a concise label system that covers the dimensions of theme, claim type, call to action, and evidence. The fine-tuned large language model (LLM) assigns tags to messages and then maps these tags to calibrated risk scores in the [0,1] interval through L2-regularized logistic regression. We evaluated 87,936 Telegram messages associated with Media Bias/Fact Check (MBFC), using URL masking and domain disjoint splits. The results showed that the ROC-AUC of the TAG2CRED model reached 0.871, the macro-F1 value was 0.787, and the Brier score was 0.167, outperforming the baseline TF-IDF (macro-F1 value 0.737, Brier score 0.248); at the same time, the number of features used in this model is much smaller, and the generalization ability on infrequent domains is stronger. The performance of the stacked ensemble model (TF-IDF + TAG2CRED + SBERT) was further improved over the baseline SBERT. ROC-AUC reached 0.901, and the macro-F1 value was 0.813 (Brier score 0.114). This indicates that style labels and lexical features may capture different but complementary dimensions of information risk.",
      "url": "https://arxiv.org/pdf/2601.13294v1",
      "date": "2026-01-19T18:47:36+00:00"
    },
    {
      "id": "auth_yipeng_wang",
      "name": "Yipeng Wang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_huy_gia_han_vu",
      "name": "Huy Gia Han Vu",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_idf_models",
      "name": "idf models",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_message_credibility",
      "name": "message credibility",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_posts_telegram",
      "name": "posts telegram",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_idf_tag2cred",
      "name": "idf tag2cred",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_telegram_short",
      "name": "telegram short",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13235v1",
      "name": "RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions",
      "group": "paper",
      "val": 30,
      "abstract": "Caregivers seeking AI-mediated support express complex needs -- information-seeking, emotional validation, and distress cues -- that warrant careful evaluation of response safety and appropriateness. Existing AI evaluation frameworks, primarily focused on general risks (toxicity, hallucinations, policy violations, etc), may not adequately capture the nuanced risks of LLM-responses in caregiving-contexts. We introduce RubRIX (Rubric-based Risk Index), a theory-driven, clinician-validated framework for evaluating risks in LLM caregiving responses. Grounded in the Elements of an Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions: Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. We evaluate six state-of-the-art LLMs on over 20,000 caregiver queries from Reddit and ALZConnected. Rubric-guided refinement consistently reduced risk-components by 45-98% after one iteration across models. This work contributes a methodological approach for developing domain-sensitive, user-centered evaluation frameworks for high-burden contexts. Our findings highlight the importance of domain-sensitive, interactional risk evaluation for the responsible deployment of LLMs in caregiving support contexts. We release benchmark datasets to enable future research on contextual risk evaluation in AI-mediated support.",
      "url": "https://arxiv.org/pdf/2601.13235v1",
      "date": "2026-01-19T17:10:49+00:00"
    },
    {
      "id": "auth_drishti_goel",
      "name": "Drishti Goel",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jeongah_lee",
      "name": "Jeongah Lee",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_qiuyue_joy_zhong",
      "name": "Qiuyue Joy Zhong",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_violeta_j_rodriguez",
      "name": "Violeta J. Rodriguez",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_daniel_s_brown",
      "name": "Daniel S. Brown",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ravi_karkar",
      "name": "Ravi Karkar",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_dong_whi_yoo",
      "name": "Dong Whi Yoo",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_responses_caregiving",
      "name": "responses caregiving",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_caregiving_responses",
      "name": "caregiving responses",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_caregiving_contexts",
      "name": "caregiving contexts",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_caregivers_seeking",
      "name": "caregivers seeking",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_evaluating_risks",
      "name": "evaluating risks",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13188v1",
      "name": "Negotiating Relationships with ChatGPT: Perceptions, External Influences, and Strategies for AI Companionship",
      "group": "paper",
      "val": 30,
      "abstract": "Individuals are turning to increasingly anthropomorphic, general-purpose chatbots for AI companionship, rather than roleplay-specific platforms. However, not much is known about how individuals perceive and conduct their relationships with general-purpose chatbots. We analyzed semi-structured interviews (n=13), survey responses (n=43), and community discussions on Reddit (41k+ posts and comments) to triangulate the internal dynamics, external influences, and steering strategies that shape AI companion relationships. We learned that individuals conceptualize their companions based on an interplay of their beliefs about the companion's own agency and the autonomy permitted by the platform, how they pursue interactions with the companion, and the perceived initiatives that the companion takes. In combination with the external entities that affect relationship dynamics, particularly model updates that can derail companion behaviour and stability, individuals make use of different types of steering strategies to preserve their relationship, for example, by setting behavioural instructions or porting to other AI platforms. We discuss implications for accountability and transparency in AI systems, where emotional connection competes with broader product objectives and safety constraints.",
      "url": "https://arxiv.org/pdf/2601.13188v1",
      "date": "2026-01-19T16:11:19+00:00"
    },
    {
      "id": "auth_patrick_yung_kang_lee",
      "name": "Patrick Yung Kang Lee",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_jessica_y_bo",
      "name": "Jessica Y. Bo",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_zixin_zhao",
      "name": "Zixin Zhao",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_paula_akemi_aoyagui",
      "name": "Paula Akemi Aoyagui",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_matthew_varona",
      "name": "Matthew Varona",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_ashton_anderson",
      "name": "Ashton Anderson",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_anastasia_kuzminykh",
      "name": "Anastasia Kuzminykh",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_fanny_chevalier",
      "name": "Fanny Chevalier",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_carolina_nobre",
      "name": "Carolina Nobre",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_ai_companionship",
      "name": "ai companionship",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_ai_companion",
      "name": "ai companion",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_purpose_chatbots",
      "name": "purpose chatbots",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_chatbots_ai",
      "name": "chatbots ai",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_conceptualize_companions",
      "name": "conceptualize companions",
      "group": "topic",
      "val": 10
    },
    {
      "id": "2601.13187v1",
      "name": "Scientific production in the era of Large Language Models",
      "group": "paper",
      "val": 30,
      "abstract": "Large Language Models (LLMs) are rapidly reshaping scientific research. We analyze these changes in multiple, large-scale datasets with 2.1M preprints, 28K peer review reports, and 246M online accesses to scientific documents. We find: 1) scientists adopting LLMs to draft manuscripts demonstrate a large increase in paper production, ranging from 23.7-89.3% depending on scientific field and author background, 2) LLM use has reversed the relationship between writing complexity and paper quality, leading to an influx of manuscripts that are linguistically complex but substantively underwhelming, and 3) LLM adopters access and cite more diverse prior work, including books and younger, less-cited documents. These findings highlight a stunning shift in scientific production that will likely require a change in how journals, funding agencies, and tenure committees evaluate scientific works.",
      "url": "https://arxiv.org/pdf/2601.13187v1",
      "date": "2026-01-19T16:10:22+00:00"
    },
    {
      "id": "auth_keigo_kusumegi",
      "name": "Keigo Kusumegi",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_xinyu_yang",
      "name": "Xinyu Yang",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_paul_ginsparg",
      "name": "Paul Ginsparg",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_mathijs_de_vaan",
      "name": "Mathijs de Vaan",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_toby_stuart",
      "name": "Toby Stuart",
      "group": "author",
      "val": 15
    },
    {
      "id": "auth_yian_yin",
      "name": "Yian Yin",
      "group": "author",
      "val": 15
    },
    {
      "id": "topic_documents_scientists",
      "name": "documents scientists",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_scientific_documents",
      "name": "scientific documents",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_cited_documents",
      "name": "cited documents",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_draft_manuscripts",
      "name": "draft manuscripts",
      "group": "topic",
      "val": 10
    },
    {
      "id": "topic_documents_findings",
      "name": "documents findings",
      "group": "topic",
      "val": 10
    }
  ],
  "links": [
    {
      "source": "2601.04175v1",
      "target": "auth_noam_kolt",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_nicholas_caputo",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jack_boeglin",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_cullen_o'keefe",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_rishi_bommasani",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_stephen_casper",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_mariano_florentino_cuéllar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noah_feldman",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_gillian_k_hadfield",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_lewis_hammond",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_peter_henderson",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_atoosa_kasirzadeh",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_seth_lazar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_anka_reuel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_kevin_l_wei",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jonathan_zittrain",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_alignment",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_compliance",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_exploring_legal",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_specifying_ai",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_ensuring_ai",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "auth_ruiyi_guo",
      "value": 5
    },
    {
      "source": "2601.04107v1",
      "target": "auth_bodong_zhang",
      "value": 5
    },
    {
      "source": "2601.04107v1",
      "target": "topic_institutional_logics",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_ai_governance",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_governs_ai",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_intelligence_governance",
      "value": 2
    },
    {
      "source": "2601.04107v1",
      "target": "topic_govern_ontologically",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "auth_tom_deckenbrunnen",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_alessio_buscemi",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_marco_almada",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_alfredo_capozucca",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "auth_german_castignani",
      "value": 5
    },
    {
      "source": "2601.04094v1",
      "target": "topic_eu_ai",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_ai_act",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_enforcement_macro",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_govern_ai",
      "value": 2
    },
    {
      "source": "2601.04094v1",
      "target": "topic_ai_technical",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "auth_anamaria_mojica_hanke",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_thomas_goger",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_svenja_wölfel",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_brian_valerius",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "auth_steffen_herbold",
      "value": 5
    },
    {
      "source": "2601.03788v1",
      "target": "topic_offense_individuals",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_criminal_liability",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_legal_consequences",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_criminal_legal",
      "value": 2
    },
    {
      "source": "2601.03788v1",
      "target": "topic_legal_systems",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "auth_xiaoxian_shen",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_yuhui_zhang",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_sahithi_ankireddy",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_xiaohan_wang",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_maya_varma",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_henry_guo",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_curtis_langlotz",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "auth_serena_yeung_levy",
      "value": 5
    },
    {
      "source": "2601.03733v1",
      "target": "topic_multimodal_reasoning",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_medical_ai",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_radiology_image",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_understanding_radiology",
      "value": 2
    },
    {
      "source": "2601.03733v1",
      "target": "topic_raddiff_multimodal",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "auth_sarah_spiekermann_hoff",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_marc_langheinrich",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_johannes_hoff",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_christiane_wendehorst",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_jürgen_pfeffer",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_thomas_fuchs",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "auth_armin_grunwald",
      "value": 5
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital_world",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital_technologies",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_ethical_psychological",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_rules_digital",
      "value": 2
    },
    {
      "source": "2601.03709v1",
      "target": "topic_digital",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "auth_junaid_qadir",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_muhammad_adil_attique",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_saleha_shoaib",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "auth_syed_ibrahim_ghaznavi",
      "value": 5
    },
    {
      "source": "2601.03693v1",
      "target": "topic_examines_ai",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_coaching_chatbot",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_ai_technical",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_intellectual",
      "value": 2
    },
    {
      "source": "2601.03693v1",
      "target": "topic_ai_chatbots",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "auth_kunpeng_wang",
      "value": 5
    },
    {
      "source": "2601.03547v1",
      "target": "auth_jiahui_hu",
      "value": 5
    },
    {
      "source": "2601.03547v1",
      "target": "topic_ai_china",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_ai_capital",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_china_economy",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_growth_ai",
      "value": 2
    },
    {
      "source": "2601.03547v1",
      "target": "topic_capital_equilibrium",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "auth_nadav_kunievsky",
      "value": 5
    },
    {
      "source": "2601.03469v1",
      "target": "auth_pedro_pertusi",
      "value": 5
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_variation",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_rewrites",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_persuasive_essays",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_writing_scores",
      "value": 2
    },
    {
      "source": "2601.03469v1",
      "target": "topic_essay_content",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "auth_aron_gohr",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_marie_amelie_lawn",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_kevin_gao",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_inigo_serjeant",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "auth_stephen_heslip",
      "value": 5
    },
    {
      "source": "2601.03458v1",
      "target": "topic_tutoring_systems",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_intelligent_tutoring",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_tutoring",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_homework_platform",
      "value": 2
    },
    {
      "source": "2601.03458v1",
      "target": "topic_editable_programming",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "auth_mohamed_ouf",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_shayan_noei",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_zeph_van_iterson",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_mariam_guizani",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "auth_ying_zou",
      "value": 5
    },
    {
      "source": "2601.03430v1",
      "target": "topic_contributors_oss4sg",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_contributors_code",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_github",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_oss_communities",
      "value": 2
    },
    {
      "source": "2601.03430v1",
      "target": "topic_oss_projects",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "auth_jacob_erickson",
      "value": 5
    },
    {
      "source": "2601.03222v1",
      "target": "topic_trust_ai",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_autonomy_trust",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_conversational_ai",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_ai_agents",
      "value": 2
    },
    {
      "source": "2601.03222v1",
      "target": "topic_focusing_trust",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noam_kolt",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_nicholas_caputo",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jack_boeglin",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_cullen_o'keefe",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_rishi_bommasani",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_stephen_casper",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_mariano_florentino_cuéllar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_noah_feldman",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_gillian_k_hadfield",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_lewis_hammond",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_peter_henderson",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_atoosa_kasirzadeh",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_seth_lazar",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_anka_reuel",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_kevin_l_wei",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "auth_jonathan_zittrain",
      "value": 5
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_alignment",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_legal_compliance",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_exploring_legal",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_specifying_ai",
      "value": 2
    },
    {
      "source": "2601.04175v1",
      "target": "topic_ensuring_ai",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_gilda",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_dungarwal",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_a_thongkham",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_e_t_ajayi",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_s_choudhary",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_t_m_terol",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_c_lam",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_j_p_araujo",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_m_mcfadyen_mungalln",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_l_s_liebovitch",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_p_t_coleman",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_h_west",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_k_sieck",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "auth_s_carter",
      "value": 5
    },
    {
      "source": "2601.05232v1",
      "target": "topic_news_social",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_social_media",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_news_dataset",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_videos_social",
      "value": 2
    },
    {
      "source": "2601.05232v1",
      "target": "topic_media_youtube",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "auth_thomas_le_goff",
      "value": 5
    },
    {
      "source": "2601.04958v1",
      "target": "topic_sustainability_laws",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_regulation_assess",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_regulatory_landscape",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_sustainability_reporting",
      "value": 2
    },
    {
      "source": "2601.04958v1",
      "target": "topic_ai_environmental",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "auth_trevor_de_clark",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "auth_yulia_bobkova",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "auth_ajay_kumar_shrestha",
      "value": 5
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_usability",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_self",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_guidance",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_compliant_privacy",
      "value": 2
    },
    {
      "source": "2601.04403v1",
      "target": "topic_privacy_standards",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "auth_kenzo_soares_seto",
      "value": 5
    },
    {
      "source": "2601.05961v1",
      "target": "topic_examines_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_digital_sovereignty",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_sociotechnical_imaginaries",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05961v1",
      "target": "topic_idea_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "auth_tianshi_li",
      "value": 5
    },
    {
      "source": "2601.05918v1",
      "target": "topic_interviewer_ai",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_identifying_interviewees",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_interviewees",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_anthropic_interviewer",
      "value": 2
    },
    {
      "source": "2601.05918v1",
      "target": "topic_search_agentic",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "auth_michael_henry_tessler",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_georgina_evans",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_michiel_a_bakker",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_iason_gabriel",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_sophie_bridgers",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_rishub_jain",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_raphael_koster",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_verena_rieser",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_anca_dragan",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_matthew_botvinick",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "auth_christopher_summerfield",
      "value": 5
    },
    {
      "source": "2601.05904v1",
      "target": "topic_mediated_deliberation",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_enhancing",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_political",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_meaningful_deliberation",
      "value": 2
    },
    {
      "source": "2601.05904v1",
      "target": "topic_deliberation_augmentation",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "auth_jakub_harasta",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_matej_vasina",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_martin_kornel",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "auth_tomas_foltynek",
      "value": 5
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_contexts",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_family_law",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_self",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_scenario_gendered",
      "value": 2
    },
    {
      "source": "2601.05879v1",
      "target": "topic_legal_guidance",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "auth_íris_damião",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_joão_franco",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_mariana_silva",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_paulo_almeida",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_pedro_c_magalhães",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "auth_joana_gonçalves_sá",
      "value": 5
    },
    {
      "source": "2601.05826v1",
      "target": "topic_campaign_media",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_elections_analyzed",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_electoral_projections",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_different_political",
      "value": 2
    },
    {
      "source": "2601.05826v1",
      "target": "topic_media_influence",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "auth_yerin_kwak",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "auth_siddharth_adelkar",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "auth_zachary_a_pardos",
      "value": 5
    },
    {
      "source": "2601.05666v1",
      "target": "topic_student_credit",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_course_articulation",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_articulations_institutions",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_ensuring_credits",
      "value": 2
    },
    {
      "source": "2601.05666v1",
      "target": "topic_candidate_articulations",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "auth_simon_chesterman",
      "value": 5
    },
    {
      "source": "2601.05574v1",
      "target": "auth_loy_hui_chieh",
      "value": 5
    },
    {
      "source": "2601.05574v1",
      "target": "topic_academic_authority",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_research_integrity",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_scholarly_labour",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_scholarly",
      "value": 2
    },
    {
      "source": "2601.05574v1",
      "target": "topic_institutional_capacity",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "auth_alessandro_bellina",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "auth_giordano_de_marzo",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "auth_david_garcia",
      "value": 5
    },
    {
      "source": "2601.05384v1",
      "target": "topic_collective_ai",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_ai_agents",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_ai_agent",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_investigate_ai",
      "value": 2
    },
    {
      "source": "2601.05384v1",
      "target": "topic_reveal_ai",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "auth_tim_menzner",
      "value": 5
    },
    {
      "source": "2601.05358v1",
      "target": "auth_jochen_l_leidner",
      "value": 5
    },
    {
      "source": "2601.05358v1",
      "target": "topic_media_bias",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_bias_propaganda",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_bias_types",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_fact_bias",
      "value": 2
    },
    {
      "source": "2601.05358v1",
      "target": "topic_political_spectrum",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "auth_ritwik_gupta",
      "value": 5
    },
    {
      "source": "2601.05307v1",
      "target": "auth_andrew_w_reddie",
      "value": 5
    },
    {
      "source": "2601.05307v1",
      "target": "topic_ai_weaponization",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_ai_security",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_national_security",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_weaponization_compute",
      "value": 2
    },
    {
      "source": "2601.05307v1",
      "target": "topic_weaponizable_capabilities",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "auth_valdemar_švábenský",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "auth_conrad_borchers",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "auth_elvin_fortuna",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "auth_elizabeth_b_cloude",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "auth_dragan_gašević",
      "value": 5
    },
    {
      "source": "2601.07629v1",
      "target": "topic_learning_analytics",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "topic_analytics_la",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "topic_learning_dashboards",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "topic_regulated_learning",
      "value": 2
    },
    {
      "source": "2601.07629v1",
      "target": "topic_analytics",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "auth_jan_elfes",
      "value": 5
    },
    {
      "source": "2601.07398v1",
      "target": "auth_marco_bastos",
      "value": 5
    },
    {
      "source": "2601.07398v1",
      "target": "auth_luca_maria_aiello",
      "value": 5
    },
    {
      "source": "2601.07398v1",
      "target": "topic_narrative_polarisation",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "topic_polarised_narratives",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "topic_narratives_groups",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "topic_groups_narratives",
      "value": 2
    },
    {
      "source": "2601.07398v1",
      "target": "topic_palestinian_conflict",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "auth_andrew_d_maynard",
      "value": 5
    },
    {
      "source": "2601.07085v1",
      "target": "topic_conversational_ai",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "topic_risk_conversational",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "topic_information_cognitive",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "topic_misinformation_persuasion",
      "value": 2
    },
    {
      "source": "2601.07085v1",
      "target": "topic_preventing_deception",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "auth_fabian_walke",
      "value": 5
    },
    {
      "source": "2601.07016v1",
      "target": "auth_thaddäa_nürnberger",
      "value": 5
    },
    {
      "source": "2601.07016v1",
      "target": "topic_false_information",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "topic_fake_information",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "topic_undermine_trust",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "topic_risk_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.07016v1",
      "target": "topic_misinformation_disinformation",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "auth_sonia_yeh",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "auth_christopher_dirzka",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "auth_aleksandr_kondratenko",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "auth_frans_libertson",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "auth_benedicte_madon",
      "value": 5
    },
    {
      "source": "2601.06894v1",
      "target": "topic_port_governance",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "topic_landlord_governance",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "topic_tenants_infrastructures",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "topic_apply_governance",
      "value": 2
    },
    {
      "source": "2601.06894v1",
      "target": "topic_sustained_stakeholder",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "auth_seung_jun_choi",
      "value": 5
    },
    {
      "source": "2601.06703v1",
      "target": "topic_policymaking",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "topic_city_policy",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "topic_extracting_policy",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "topic_planners",
      "value": 2
    },
    {
      "source": "2601.06703v1",
      "target": "topic_envisioning_localized",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "auth_murad_farzulla",
      "value": 5
    },
    {
      "source": "2601.06692v1",
      "target": "topic_coordination_friction",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "topic_fundamental_coordination",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "topic_coordination_preference",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "topic_analyzing_coordination",
      "value": 2
    },
    {
      "source": "2601.06692v1",
      "target": "topic_stake_entropy",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "auth_stefaan_verhulst",
      "value": 5
    },
    {
      "source": "2601.06687v1",
      "target": "topic_data_governance",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "topic_data_stewardship",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "topic_data_ecosystems",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "topic_responsible_data",
      "value": 2
    },
    {
      "source": "2601.06687v1",
      "target": "topic_data_public",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "auth_warinkwi_k_flores",
      "value": 5
    },
    {
      "source": "2601.06634v1",
      "target": "auth_kuntikzi_flores",
      "value": 5
    },
    {
      "source": "2601.06634v1",
      "target": "auth_rosa_m_panama",
      "value": 5
    },
    {
      "source": "2601.06634v1",
      "target": "auth_kayakanti_alta",
      "value": 5
    },
    {
      "source": "2601.06634v1",
      "target": "topic_data_sovereignty",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "topic_indigenous_legal",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "topic_indigenous_peoples",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "topic_indigenous",
      "value": 2
    },
    {
      "source": "2601.06634v1",
      "target": "topic_kichwa_data",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "auth_alok_khatri",
      "value": 5
    },
    {
      "source": "2601.06500v1",
      "target": "auth_bishesh_khanal",
      "value": 5
    },
    {
      "source": "2601.06500v1",
      "target": "topic_ai_workforce",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "topic_capability_ai",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "topic_ai_literacy",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "topic_sustaining_ai",
      "value": 2
    },
    {
      "source": "2601.06500v1",
      "target": "topic_ai_foundation",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "auth_debasmita_panda",
      "value": 5
    },
    {
      "source": "2601.06477v1",
      "target": "auth_akash_anil",
      "value": 5
    },
    {
      "source": "2601.06477v1",
      "target": "auth_neelesh_kumar_shukla",
      "value": 5
    },
    {
      "source": "2601.06477v1",
      "target": "topic_biases_indian",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "topic_nlp_biases",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "topic_regional_bias",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "topic_regional_biases",
      "value": 2
    },
    {
      "source": "2601.06477v1",
      "target": "topic_regional_biased",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "auth_ha_chi_tran",
      "value": 5
    },
    {
      "source": "2601.06412v1",
      "target": "topic_superpower_strategies",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "topic_powers_technological",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "topic_global_power",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "topic_china_rivalry",
      "value": 2
    },
    {
      "source": "2601.06412v1",
      "target": "topic_technological_capacity",
      "value": 2
    },
    {
      "source": "2601.08768v1",
      "target": "auth_cody_kommers",
      "value": 5
    },
    {
      "source": "2601.08768v1",
      "target": "auth_ari_holtzman",
      "value": 5
    },
    {
      "source": "2601.08768v1",
      "target": "topic_considers_entertainment",
      "value": 2
    },
    {
      "source": "2601.08768v1",
      "target": "topic_entertaining_ai",
      "value": 2
    },
    {
      "source": "2601.08768v1",
      "target": "topic_entertainment",
      "value": 2
    },
    {
      "source": "2601.08768v1",
      "target": "topic_cultural_outputs",
      "value": 2
    },
    {
      "source": "2601.08768v1",
      "target": "topic_entertainment_purposes",
      "value": 2
    },
    {
      "source": "2601.08673v1",
      "target": "auth_didier_sornette",
      "value": 5
    },
    {
      "source": "2601.08673v1",
      "target": "auth_sandro_claudio_lera",
      "value": 5
    },
    {
      "source": "2601.08673v1",
      "target": "auth_ke_wu",
      "value": 5
    },
    {
      "source": "2601.08673v1",
      "target": "topic_blackmail_interpreted",
      "value": 2
    },
    {
      "source": "2601.08673v1",
      "target": "topic_blackmail_categorical",
      "value": 2
    },
    {
      "source": "2601.08673v1",
      "target": "topic_practices_blackmail",
      "value": 2
    },
    {
      "source": "2601.08673v1",
      "target": "topic_threats_blackmail",
      "value": 2
    },
    {
      "source": "2601.08673v1",
      "target": "topic_moral_artificial",
      "value": 2
    },
    {
      "source": "2601.08516v1",
      "target": "auth_ziqi_ding",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_yunfeng_wan",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_wei_song",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_yi_liu",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_gelei_deng",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_nan_sun",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_huadong_mo",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_jingling_xue",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_shidong_pan",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "auth_yuekang_li",
      "value": 5
    },
    {
      "source": "2601.08516v1",
      "target": "topic_audio_captchas",
      "value": 2
    },
    {
      "source": "2601.08516v1",
      "target": "topic_audio_captcha",
      "value": 2
    },
    {
      "source": "2601.08516v1",
      "target": "topic_ai_captcha",
      "value": 2
    },
    {
      "source": "2601.08516v1",
      "target": "topic_captchas_existing",
      "value": 2
    },
    {
      "source": "2601.08516v1",
      "target": "topic_captcha_methods",
      "value": 2
    },
    {
      "source": "2601.08295v1",
      "target": "auth_gregor_autischer",
      "value": 5
    },
    {
      "source": "2601.08295v1",
      "target": "auth_kerstin_waxnegger",
      "value": 5
    },
    {
      "source": "2601.08295v1",
      "target": "auth_dominik_kowald",
      "value": 5
    },
    {
      "source": "2601.08295v1",
      "target": "topic_ai_certification",
      "value": 2
    },
    {
      "source": "2601.08295v1",
      "target": "topic_ai_assessment",
      "value": 2
    },
    {
      "source": "2601.08295v1",
      "target": "topic_certification_requirements",
      "value": 2
    },
    {
      "source": "2601.08295v1",
      "target": "topic_certification_criteria",
      "value": 2
    },
    {
      "source": "2601.08295v1",
      "target": "topic_ai_based",
      "value": 2
    },
    {
      "source": "2601.07973v1",
      "target": "auth_myra_cheng",
      "value": 5
    },
    {
      "source": "2601.07973v1",
      "target": "auth_vinodkumar_prabhakaran",
      "value": 5
    },
    {
      "source": "2601.07973v1",
      "target": "auth_alice_oh",
      "value": 5
    },
    {
      "source": "2601.07973v1",
      "target": "auth_hayk_stepanyan",
      "value": 5
    },
    {
      "source": "2601.07973v1",
      "target": "auth_aishwarya_verma",
      "value": 5
    },
    {
      "source": "2601.07973v1",
      "target": "auth_charu_kalia",
      "value": 5
    },
    {
      "source": "2601.07973v1",
      "target": "auth_erin_macmurray_van_liemt",
      "value": 5
    },
    {
      "source": "2601.07973v1",
      "target": "auth_sunipa_dev",
      "value": 5
    },
    {
      "source": "2601.07973v1",
      "target": "topic_sociocultural_norms",
      "value": 2
    },
    {
      "source": "2601.07973v1",
      "target": "topic_cultural_norm",
      "value": 2
    },
    {
      "source": "2601.07973v1",
      "target": "topic_human_norms",
      "value": 2
    },
    {
      "source": "2601.07973v1",
      "target": "topic_violate_norms",
      "value": 2
    },
    {
      "source": "2601.07973v1",
      "target": "topic_cultural_contexts",
      "value": 2
    },
    {
      "source": "2601.09600v1",
      "target": "auth_bhaskar_mitra",
      "value": 5
    },
    {
      "source": "2601.09600v1",
      "target": "auth_nicola_neophytou",
      "value": 5
    },
    {
      "source": "2601.09600v1",
      "target": "auth_sireesh_gururaja",
      "value": 5
    },
    {
      "source": "2601.09600v1",
      "target": "topic_sociotechnical_concerns",
      "value": 2
    },
    {
      "source": "2601.09600v1",
      "target": "topic_emancipatory_pedagogy",
      "value": 2
    },
    {
      "source": "2601.09600v1",
      "target": "topic_technologists_envision",
      "value": 2
    },
    {
      "source": "2601.09600v1",
      "target": "topic_sociotechnical",
      "value": 2
    },
    {
      "source": "2601.09600v1",
      "target": "topic_pedagogy_freire",
      "value": 2
    },
    {
      "source": "2601.09351v1",
      "target": "auth_ruomu_tan",
      "value": 5
    },
    {
      "source": "2601.09351v1",
      "target": "auth_martin_w_hoffmann",
      "value": 5
    },
    {
      "source": "2601.09351v1",
      "target": "topic_ethical_industrial",
      "value": 2
    },
    {
      "source": "2601.09351v1",
      "target": "topic_ai_industrial",
      "value": 2
    },
    {
      "source": "2601.09351v1",
      "target": "topic_industrial_ai",
      "value": 2
    },
    {
      "source": "2601.09351v1",
      "target": "topic_ai_empowered",
      "value": 2
    },
    {
      "source": "2601.09351v1",
      "target": "topic_examine_ethical",
      "value": 2
    },
    {
      "source": "2601.09182v1",
      "target": "auth_jungmin_yun",
      "value": 5
    },
    {
      "source": "2601.09182v1",
      "target": "auth_junehyoung_kwon",
      "value": 5
    },
    {
      "source": "2601.09182v1",
      "target": "auth_mihyeon_kim",
      "value": 5
    },
    {
      "source": "2601.09182v1",
      "target": "auth_youngbin_kim",
      "value": 5
    },
    {
      "source": "2601.09182v1",
      "target": "topic_helps_reviewers",
      "value": 2
    },
    {
      "source": "2601.09182v1",
      "target": "topic_strengthen_reviewer",
      "value": 2
    },
    {
      "source": "2601.09182v1",
      "target": "topic_cultivates_reviewers",
      "value": 2
    },
    {
      "source": "2601.09182v1",
      "target": "topic_peer_review",
      "value": 2
    },
    {
      "source": "2601.09182v1",
      "target": "topic_generate_reviews",
      "value": 2
    },
    {
      "source": "2601.09156v1",
      "target": "auth_woojin_kim",
      "value": 5
    },
    {
      "source": "2601.09156v1",
      "target": "auth_changkwon_lee",
      "value": 5
    },
    {
      "source": "2601.09156v1",
      "target": "auth_hyeoncheol_kim",
      "value": 5
    },
    {
      "source": "2601.09156v1",
      "target": "topic_knowledge_tracing",
      "value": 2
    },
    {
      "source": "2601.09156v1",
      "target": "topic_explanation_generation",
      "value": 2
    },
    {
      "source": "2601.09156v1",
      "target": "topic_ai_education",
      "value": 2
    },
    {
      "source": "2601.09156v1",
      "target": "topic_counterfactual_explanations",
      "value": 2
    },
    {
      "source": "2601.09156v1",
      "target": "topic_education_counterfactual",
      "value": 2
    },
    {
      "source": "2601.09117v1",
      "target": "auth_shalmoli_ghosh",
      "value": 5
    },
    {
      "source": "2601.09117v1",
      "target": "auth_matthew_r_deverna",
      "value": 5
    },
    {
      "source": "2601.09117v1",
      "target": "auth_filippo_menczer",
      "value": 5
    },
    {
      "source": "2601.09117v1",
      "target": "topic_content_bounties",
      "value": 2
    },
    {
      "source": "2601.09117v1",
      "target": "topic_bounty_requests",
      "value": 2
    },
    {
      "source": "2601.09117v1",
      "target": "topic_bounties_participation",
      "value": 2
    },
    {
      "source": "2601.09117v1",
      "target": "topic_bounty_marketplace",
      "value": 2
    },
    {
      "source": "2601.09117v1",
      "target": "topic_bounties",
      "value": 2
    },
    {
      "source": "2601.09112v1",
      "target": "auth_ying_he",
      "value": 5
    },
    {
      "source": "2601.09112v1",
      "target": "auth_baiyang_li",
      "value": 5
    },
    {
      "source": "2601.09112v1",
      "target": "auth_yule_cao",
      "value": 5
    },
    {
      "source": "2601.09112v1",
      "target": "auth_huirun_xu",
      "value": 5
    },
    {
      "source": "2601.09112v1",
      "target": "auth_qiuxian_chen",
      "value": 5
    },
    {
      "source": "2601.09112v1",
      "target": "auth_shu_chen",
      "value": 5
    },
    {
      "source": "2601.09112v1",
      "target": "auth_shangsheng_ren",
      "value": 5
    },
    {
      "source": "2601.09112v1",
      "target": "topic_value_safety",
      "value": 2
    },
    {
      "source": "2601.09112v1",
      "target": "topic_safety_scale",
      "value": 2
    },
    {
      "source": "2601.09112v1",
      "target": "topic_safety_critical",
      "value": 2
    },
    {
      "source": "2601.09112v1",
      "target": "topic_technical_safety",
      "value": 2
    },
    {
      "source": "2601.09112v1",
      "target": "topic_safety_benchmark",
      "value": 2
    },
    {
      "source": "2601.08951v1",
      "target": "auth_jing_jing_li",
      "value": 5
    },
    {
      "source": "2601.08951v1",
      "target": "auth_joel_mire",
      "value": 5
    },
    {
      "source": "2601.08951v1",
      "target": "auth_eve_fleisig",
      "value": 5
    },
    {
      "source": "2601.08951v1",
      "target": "auth_valentina_pyatkin",
      "value": 5
    },
    {
      "source": "2601.08951v1",
      "target": "auth_anne_collins",
      "value": 5
    },
    {
      "source": "2601.08951v1",
      "target": "auth_maarten_sap",
      "value": 5
    },
    {
      "source": "2601.08951v1",
      "target": "auth_sydney_levine",
      "value": 5
    },
    {
      "source": "2601.08951v1",
      "target": "topic_harmfulness_annotator",
      "value": 2
    },
    {
      "source": "2601.08951v1",
      "target": "topic_ai_harms",
      "value": 2
    },
    {
      "source": "2601.08951v1",
      "target": "topic_harm_judgments",
      "value": 2
    },
    {
      "source": "2601.08951v1",
      "target": "topic_ai_safety",
      "value": 2
    },
    {
      "source": "2601.08951v1",
      "target": "topic_safe_ai",
      "value": 2
    },
    {
      "source": "2601.10691v1",
      "target": "auth_lorena_a_barba",
      "value": 5
    },
    {
      "source": "2601.10691v1",
      "target": "auth_laura_stegner",
      "value": 5
    },
    {
      "source": "2601.10691v1",
      "target": "topic_conversational_exam",
      "value": 2
    },
    {
      "source": "2601.10691v1",
      "target": "topic_assessment",
      "value": 2
    },
    {
      "source": "2601.10691v1",
      "target": "topic_traditional_assessment",
      "value": 2
    },
    {
      "source": "2601.10691v1",
      "target": "topic_practice_students",
      "value": 2
    },
    {
      "source": "2601.10691v1",
      "target": "topic_exam_scalable",
      "value": 2
    },
    {
      "source": "2601.10658v1",
      "target": "auth_joseph_uguet",
      "value": 5
    },
    {
      "source": "2601.10658v1",
      "target": "auth_nicola_tollin",
      "value": 5
    },
    {
      "source": "2601.10658v1",
      "target": "auth_jordi_morato",
      "value": 5
    },
    {
      "source": "2601.10658v1",
      "target": "topic_resilience_urban",
      "value": 2
    },
    {
      "source": "2601.10658v1",
      "target": "topic_resilience_antifragility",
      "value": 2
    },
    {
      "source": "2601.10658v1",
      "target": "topic_sustainable_resilience",
      "value": 2
    },
    {
      "source": "2601.10658v1",
      "target": "topic_disaster_resilience",
      "value": 2
    },
    {
      "source": "2601.10658v1",
      "target": "topic_resilience_enhanced",
      "value": 2
    },
    {
      "source": "2601.10599v1",
      "target": "auth_federico_pierucci",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "auth_marcello_galisai",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "auth_marcantonio_syrnikov_bracale",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "auth_matteo_prandi",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "auth_piercosma_bisconti",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "auth_francesco_giarrusso",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "auth_olga_sorokoletova",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "auth_vincenzo_suriani",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "auth_daniele_nardi",
      "value": 5
    },
    {
      "source": "2601.10599v1",
      "target": "topic_institutional_ai",
      "value": 2
    },
    {
      "source": "2601.10599v1",
      "target": "topic_ai_agent",
      "value": 2
    },
    {
      "source": "2601.10599v1",
      "target": "topic_aligned_agents",
      "value": 2
    },
    {
      "source": "2601.10599v1",
      "target": "topic_agentic_alignment",
      "value": 2
    },
    {
      "source": "2601.10599v1",
      "target": "topic_constrain_agents",
      "value": 2
    },
    {
      "source": "2601.10567v1",
      "target": "auth_laura_ferrarotti",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_gian_maria_campedelli",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_roberto_dessì",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_andrea_baronchelli",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_giovanni_iacca",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_kathleen_m_carley",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_alex_pentland",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_joel_z_leibo",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_james_evans",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "auth_bruno_lepri",
      "value": 5
    },
    {
      "source": "2601.10567v1",
      "target": "topic_agent_generative",
      "value": 2
    },
    {
      "source": "2601.10567v1",
      "target": "topic_interact_social",
      "value": 2
    },
    {
      "source": "2601.10567v1",
      "target": "topic_generative_ai",
      "value": 2
    },
    {
      "source": "2601.10567v1",
      "target": "topic_interactionist_paradigm",
      "value": 2
    },
    {
      "source": "2601.10567v1",
      "target": "topic_understanding_collective",
      "value": 2
    },
    {
      "source": "2601.10520v1",
      "target": "auth_felix_jahn",
      "value": 5
    },
    {
      "source": "2601.10520v1",
      "target": "auth_yannic_muskalla",
      "value": 5
    },
    {
      "source": "2601.10520v1",
      "target": "auth_lisa_dargasz",
      "value": 5
    },
    {
      "source": "2601.10520v1",
      "target": "auth_patrick_schramowski",
      "value": 5
    },
    {
      "source": "2601.10520v1",
      "target": "auth_kevin_baum",
      "value": 5
    },
    {
      "source": "2601.10520v1",
      "target": "topic_normative_reasoning",
      "value": 2
    },
    {
      "source": "2601.10520v1",
      "target": "topic_enforces_moral",
      "value": 2
    },
    {
      "source": "2601.10520v1",
      "target": "topic_based_reasoning",
      "value": 2
    },
    {
      "source": "2601.10520v1",
      "target": "topic_logic_enabling",
      "value": 2
    },
    {
      "source": "2601.10520v1",
      "target": "topic_symbolic_reason",
      "value": 2
    },
    {
      "source": "2601.10468v1",
      "target": "auth_daniyaal_farooqi",
      "value": 5
    },
    {
      "source": "2601.10468v1",
      "target": "auth_gavin_pu",
      "value": 5
    },
    {
      "source": "2601.10468v1",
      "target": "auth_shreyasha_paudel",
      "value": 5
    },
    {
      "source": "2601.10468v1",
      "target": "auth_sharifa_sultana",
      "value": 5
    },
    {
      "source": "2601.10468v1",
      "target": "auth_syed_ishtiaque_ahmed",
      "value": 5
    },
    {
      "source": "2601.10468v1",
      "target": "topic_pursuing_computer",
      "value": 2
    },
    {
      "source": "2601.10468v1",
      "target": "topic_job_insecurity",
      "value": 2
    },
    {
      "source": "2601.10468v1",
      "target": "topic_ai_graduate",
      "value": 2
    },
    {
      "source": "2601.10468v1",
      "target": "topic_careers_oversaturation",
      "value": 2
    },
    {
      "source": "2601.10468v1",
      "target": "topic_pursuing_ai",
      "value": 2
    },
    {
      "source": "2601.10223v1",
      "target": "auth_ziqi_xu",
      "value": 5
    },
    {
      "source": "2601.10223v1",
      "target": "auth_yi_liu",
      "value": 5
    },
    {
      "source": "2601.10223v1",
      "target": "auth_yuekang_li",
      "value": 5
    },
    {
      "source": "2601.10223v1",
      "target": "auth_ling_shi",
      "value": 5
    },
    {
      "source": "2601.10223v1",
      "target": "auth_kailong_wang",
      "value": 5
    },
    {
      "source": "2601.10223v1",
      "target": "auth_yongxin_zhao",
      "value": 5
    },
    {
      "source": "2601.10223v1",
      "target": "topic_voice_assistants",
      "value": 2
    },
    {
      "source": "2601.10223v1",
      "target": "topic_speech_conversion",
      "value": 2
    },
    {
      "source": "2601.10223v1",
      "target": "topic_automatic_speech",
      "value": 2
    },
    {
      "source": "2601.10223v1",
      "target": "topic_voice_driven",
      "value": 2
    },
    {
      "source": "2601.10223v1",
      "target": "topic_repair_speech",
      "value": 2
    },
    {
      "source": "2601.09994v1",
      "target": "auth_conrad_borchers",
      "value": 5
    },
    {
      "source": "2601.09994v1",
      "target": "auth_ashish_gurung",
      "value": 5
    },
    {
      "source": "2601.09994v1",
      "target": "auth_qinyi_liu",
      "value": 5
    },
    {
      "source": "2601.09994v1",
      "target": "auth_danielle_r_thomas",
      "value": 5
    },
    {
      "source": "2601.09994v1",
      "target": "auth_mohammad_khalil",
      "value": 5
    },
    {
      "source": "2601.09994v1",
      "target": "auth_kenneth_r_koedinger",
      "value": 5
    },
    {
      "source": "2601.09994v1",
      "target": "topic_tutor_interactions",
      "value": 2
    },
    {
      "source": "2601.09994v1",
      "target": "topic_tutoring_prioritizing",
      "value": 2
    },
    {
      "source": "2601.09994v1",
      "target": "topic_tutors_efficiently",
      "value": 2
    },
    {
      "source": "2601.09994v1",
      "target": "topic_effective_tutor",
      "value": 2
    },
    {
      "source": "2601.09994v1",
      "target": "topic_human_tutors",
      "value": 2
    },
    {
      "source": "2601.09942v1",
      "target": "auth_hunjun_shin",
      "value": 5
    },
    {
      "source": "2601.09942v1",
      "target": "auth_hoonbae_moon",
      "value": 5
    },
    {
      "source": "2601.09942v1",
      "target": "auth_mohit_singhal",
      "value": 5
    },
    {
      "source": "2601.09942v1",
      "target": "topic_responses_diplomacy",
      "value": 2
    },
    {
      "source": "2601.09942v1",
      "target": "topic_discourse_network",
      "value": 2
    },
    {
      "source": "2601.09942v1",
      "target": "topic_summit_diplomacy",
      "value": 2
    },
    {
      "source": "2601.09942v1",
      "target": "topic_diplomacy_2018",
      "value": 2
    },
    {
      "source": "2601.09942v1",
      "target": "topic_diplomacy",
      "value": 2
    },
    {
      "source": "2601.09772v1",
      "target": "auth_paweł_niszczota",
      "value": 5
    },
    {
      "source": "2601.09772v1",
      "target": "auth_cassandra_grützner",
      "value": 5
    },
    {
      "source": "2601.09772v1",
      "target": "topic_social_sanctions",
      "value": 2
    },
    {
      "source": "2601.09772v1",
      "target": "topic_ai_users",
      "value": 2
    },
    {
      "source": "2601.09772v1",
      "target": "topic_model_punished",
      "value": 2
    },
    {
      "source": "2601.09772v1",
      "target": "topic_model_punishment",
      "value": 2
    },
    {
      "source": "2601.09772v1",
      "target": "topic_targets_participants",
      "value": 2
    },
    {
      "source": "2601.11513v1",
      "target": "auth_evan_dong",
      "value": 5
    },
    {
      "source": "2601.11513v1",
      "target": "auth_nikhil_garg",
      "value": 5
    },
    {
      "source": "2601.11513v1",
      "target": "auth_sarah_dean",
      "value": 5
    },
    {
      "source": "2601.11513v1",
      "target": "topic_predictions_admissions",
      "value": 2
    },
    {
      "source": "2601.11513v1",
      "target": "topic_admissions_probabilities",
      "value": 2
    },
    {
      "source": "2601.11513v1",
      "target": "topic_admissions_decisions",
      "value": 2
    },
    {
      "source": "2601.11513v1",
      "target": "topic_individual_admissions",
      "value": 2
    },
    {
      "source": "2601.11513v1",
      "target": "topic_admissions_data",
      "value": 2
    },
    {
      "source": "2601.11459v1",
      "target": "auth_brian_keith",
      "value": 5
    },
    {
      "source": "2601.11459v1",
      "target": "topic_narrative_analytics",
      "value": 2
    },
    {
      "source": "2601.11459v1",
      "target": "topic_narrative_sensemaking",
      "value": 2
    },
    {
      "source": "2601.11459v1",
      "target": "topic_interactive_narrative",
      "value": 2
    },
    {
      "source": "2601.11459v1",
      "target": "topic_computational_narrative",
      "value": 2
    },
    {
      "source": "2601.11459v1",
      "target": "topic_narrative_extraction",
      "value": 2
    },
    {
      "source": "2601.10983v1",
      "target": "auth_zhen_xu",
      "value": 5
    },
    {
      "source": "2601.10983v1",
      "target": "auth_xin_guan",
      "value": 5
    },
    {
      "source": "2601.10983v1",
      "target": "auth_chenxi_shi",
      "value": 5
    },
    {
      "source": "2601.10983v1",
      "target": "auth_qinhao_chen",
      "value": 5
    },
    {
      "source": "2601.10983v1",
      "target": "auth_renzhe_yu",
      "value": 5
    },
    {
      "source": "2601.10983v1",
      "target": "topic_curriculum_competency",
      "value": 2
    },
    {
      "source": "2601.10983v1",
      "target": "topic_annotated_curriculum",
      "value": 2
    },
    {
      "source": "2601.10983v1",
      "target": "topic_curriculum",
      "value": 2
    },
    {
      "source": "2601.10983v1",
      "target": "topic_curriculum_documents",
      "value": 2
    },
    {
      "source": "2601.10983v1",
      "target": "topic_curriculum_sources",
      "value": 2
    },
    {
      "source": "2601.10970v1",
      "target": "auth_canwen_wang",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_angela_chen",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_catherine_bao",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_siwei_jin",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_yee_kit_chan",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_jessica_r_mindel",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_sijia_xie",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_holly_swartz",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_tongshuang_wu",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_robert_e_kraut",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "auth_haiyi_zhu",
      "value": 5
    },
    {
      "source": "2601.10970v1",
      "target": "topic_couples_therapy",
      "value": 2
    },
    {
      "source": "2601.10970v1",
      "target": "topic_couples_therapists",
      "value": 2
    },
    {
      "source": "2601.10970v1",
      "target": "topic_interactions_couples",
      "value": 2
    },
    {
      "source": "2601.10970v1",
      "target": "topic_training_couples",
      "value": 2
    },
    {
      "source": "2601.10970v1",
      "target": "topic_relationship_counseling",
      "value": 2
    },
    {
      "source": "2601.10852v1",
      "target": "auth_khondokar_fida_hasan",
      "value": 5
    },
    {
      "source": "2601.10852v1",
      "target": "auth_william_hughes",
      "value": 5
    },
    {
      "source": "2601.10852v1",
      "target": "auth_adrita_rahman",
      "value": 5
    },
    {
      "source": "2601.10852v1",
      "target": "topic_cybersecurity_education",
      "value": 2
    },
    {
      "source": "2601.10852v1",
      "target": "topic_gamification_cybersecurity",
      "value": 2
    },
    {
      "source": "2601.10852v1",
      "target": "topic_cybersecurity_competencies",
      "value": 2
    },
    {
      "source": "2601.10852v1",
      "target": "topic_cybersecurity_risks",
      "value": 2
    },
    {
      "source": "2601.10852v1",
      "target": "topic_cybersecurity_scenarios",
      "value": 2
    },
    {
      "source": "2601.14190v1",
      "target": "auth_polina_smirnova",
      "value": 5
    },
    {
      "source": "2601.14190v1",
      "target": "auth_mykola_makhortykh",
      "value": 5
    },
    {
      "source": "2601.14190v1",
      "target": "topic_framings_russia",
      "value": 2
    },
    {
      "source": "2601.14190v1",
      "target": "topic_propaganda_blending",
      "value": 2
    },
    {
      "source": "2601.14190v1",
      "target": "topic_examines_russian",
      "value": 2
    },
    {
      "source": "2601.14190v1",
      "target": "topic_russia_informational",
      "value": 2
    },
    {
      "source": "2601.14190v1",
      "target": "topic_legitimacy_russia",
      "value": 2
    },
    {
      "source": "2601.13936v1",
      "target": "auth_theresa_züger",
      "value": 5
    },
    {
      "source": "2601.13936v1",
      "target": "auth_laura_state",
      "value": 5
    },
    {
      "source": "2601.13936v1",
      "target": "auth_lena_winter",
      "value": 5
    },
    {
      "source": "2601.13936v1",
      "target": "topic_ai_projects",
      "value": 2
    },
    {
      "source": "2601.13936v1",
      "target": "topic_assessment_ai",
      "value": 2
    },
    {
      "source": "2601.13936v1",
      "target": "topic_project_governance",
      "value": 2
    },
    {
      "source": "2601.13936v1",
      "target": "topic_debate_ai",
      "value": 2
    },
    {
      "source": "2601.13936v1",
      "target": "topic_propose_ai",
      "value": 2
    },
    {
      "source": "2601.13846v1",
      "target": "auth_glinskaya_maria",
      "value": 5
    },
    {
      "source": "2601.13846v1",
      "target": "topic_urban_identity",
      "value": 2
    },
    {
      "source": "2601.13846v1",
      "target": "topic_virtual_urbanism",
      "value": 2
    },
    {
      "source": "2601.13846v1",
      "target": "topic_quantifying_urban",
      "value": 2
    },
    {
      "source": "2601.13846v1",
      "target": "topic_replicas_urban",
      "value": 2
    },
    {
      "source": "2601.13846v1",
      "target": "topic_urbanism_vu",
      "value": 2
    },
    {
      "source": "2601.13749v1",
      "target": "auth_benaya_trabelsi",
      "value": 5
    },
    {
      "source": "2601.13749v1",
      "target": "auth_jonathan_shaki",
      "value": 5
    },
    {
      "source": "2601.13749v1",
      "target": "auth_sarit_kraus",
      "value": 5
    },
    {
      "source": "2601.13749v1",
      "target": "topic_ai_bias",
      "value": 2
    },
    {
      "source": "2601.13749v1",
      "target": "topic_preferential_bias",
      "value": 2
    },
    {
      "source": "2601.13749v1",
      "target": "topic_bias_favor",
      "value": 2
    },
    {
      "source": "2601.13749v1",
      "target": "topic_bias",
      "value": 2
    },
    {
      "source": "2601.13749v1",
      "target": "topic_salaries_ai",
      "value": 2
    },
    {
      "source": "2601.13516v1",
      "target": "auth_renkai_ma",
      "value": 5
    },
    {
      "source": "2601.13516v1",
      "target": "auth_ashwaq_alsoubai",
      "value": 5
    },
    {
      "source": "2601.13516v1",
      "target": "auth_jinkyung_katie_park",
      "value": 5
    },
    {
      "source": "2601.13516v1",
      "target": "auth_pamela_j_wisniewski",
      "value": 5
    },
    {
      "source": "2601.13516v1",
      "target": "topic_youth_safety",
      "value": 2
    },
    {
      "source": "2601.13516v1",
      "target": "topic_teen_centered",
      "value": 2
    },
    {
      "source": "2601.13516v1",
      "target": "topic_deployment_teens",
      "value": 2
    },
    {
      "source": "2601.13516v1",
      "target": "topic_youth_online",
      "value": 2
    },
    {
      "source": "2601.13516v1",
      "target": "topic_teen_empowerment",
      "value": 2
    },
    {
      "source": "2601.13487v1",
      "target": "auth_olivia_pal",
      "value": 5
    },
    {
      "source": "2601.13487v1",
      "target": "auth_agam_goyal",
      "value": 5
    },
    {
      "source": "2601.13487v1",
      "target": "auth_eshwar_chandrasekharan",
      "value": 5
    },
    {
      "source": "2601.13487v1",
      "target": "auth_koustuv_saha",
      "value": 5
    },
    {
      "source": "2601.13487v1",
      "target": "topic_news_consumption",
      "value": 2
    },
    {
      "source": "2601.13487v1",
      "target": "topic_news_effects",
      "value": 2
    },
    {
      "source": "2601.13487v1",
      "target": "topic_news_engagement",
      "value": 2
    },
    {
      "source": "2601.13487v1",
      "target": "topic_news_feeds",
      "value": 2
    },
    {
      "source": "2601.13487v1",
      "target": "topic_news_feed",
      "value": 2
    },
    {
      "source": "2601.13372v1",
      "target": "auth_mehmet_murat_albayrakoglu",
      "value": 5
    },
    {
      "source": "2601.13372v1",
      "target": "auth_mehmet_nafiz_aydin",
      "value": 5
    },
    {
      "source": "2601.13372v1",
      "target": "topic_intelligence_eu",
      "value": 2
    },
    {
      "source": "2601.13372v1",
      "target": "topic_eu_ai",
      "value": 2
    },
    {
      "source": "2601.13372v1",
      "target": "topic_ethics_components",
      "value": 2
    },
    {
      "source": "2601.13372v1",
      "target": "topic_ai_act",
      "value": 2
    },
    {
      "source": "2601.13372v1",
      "target": "topic_normative_ethical",
      "value": 2
    },
    {
      "source": "2601.13317v1",
      "target": "auth_samantha_sudhoff",
      "value": 5
    },
    {
      "source": "2601.13317v1",
      "target": "auth_pranav_perumal",
      "value": 5
    },
    {
      "source": "2601.13317v1",
      "target": "auth_zhaoqing_wu",
      "value": 5
    },
    {
      "source": "2601.13317v1",
      "target": "auth_tunazzina_islam",
      "value": 5
    },
    {
      "source": "2601.13317v1",
      "target": "topic_climate_discourse",
      "value": 2
    },
    {
      "source": "2601.13317v1",
      "target": "topic_climate_messaging",
      "value": 2
    },
    {
      "source": "2601.13317v1",
      "target": "topic_climate_communication",
      "value": 2
    },
    {
      "source": "2601.13317v1",
      "target": "topic_climate_narratives",
      "value": 2
    },
    {
      "source": "2601.13317v1",
      "target": "topic_discourse_online",
      "value": 2
    },
    {
      "source": "2601.13294v1",
      "target": "auth_yipeng_wang",
      "value": 5
    },
    {
      "source": "2601.13294v1",
      "target": "auth_huy_gia_han_vu",
      "value": 5
    },
    {
      "source": "2601.13294v1",
      "target": "auth_mohit_singhal",
      "value": 5
    },
    {
      "source": "2601.13294v1",
      "target": "topic_idf_models",
      "value": 2
    },
    {
      "source": "2601.13294v1",
      "target": "topic_message_credibility",
      "value": 2
    },
    {
      "source": "2601.13294v1",
      "target": "topic_posts_telegram",
      "value": 2
    },
    {
      "source": "2601.13294v1",
      "target": "topic_idf_tag2cred",
      "value": 2
    },
    {
      "source": "2601.13294v1",
      "target": "topic_telegram_short",
      "value": 2
    },
    {
      "source": "2601.13235v1",
      "target": "auth_drishti_goel",
      "value": 5
    },
    {
      "source": "2601.13235v1",
      "target": "auth_jeongah_lee",
      "value": 5
    },
    {
      "source": "2601.13235v1",
      "target": "auth_qiuyue_joy_zhong",
      "value": 5
    },
    {
      "source": "2601.13235v1",
      "target": "auth_violeta_j_rodriguez",
      "value": 5
    },
    {
      "source": "2601.13235v1",
      "target": "auth_daniel_s_brown",
      "value": 5
    },
    {
      "source": "2601.13235v1",
      "target": "auth_ravi_karkar",
      "value": 5
    },
    {
      "source": "2601.13235v1",
      "target": "auth_dong_whi_yoo",
      "value": 5
    },
    {
      "source": "2601.13235v1",
      "target": "auth_koustuv_saha",
      "value": 5
    },
    {
      "source": "2601.13235v1",
      "target": "topic_responses_caregiving",
      "value": 2
    },
    {
      "source": "2601.13235v1",
      "target": "topic_caregiving_responses",
      "value": 2
    },
    {
      "source": "2601.13235v1",
      "target": "topic_caregiving_contexts",
      "value": 2
    },
    {
      "source": "2601.13235v1",
      "target": "topic_caregivers_seeking",
      "value": 2
    },
    {
      "source": "2601.13235v1",
      "target": "topic_evaluating_risks",
      "value": 2
    },
    {
      "source": "2601.13188v1",
      "target": "auth_patrick_yung_kang_lee",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "auth_jessica_y_bo",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "auth_zixin_zhao",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "auth_paula_akemi_aoyagui",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "auth_matthew_varona",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "auth_ashton_anderson",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "auth_anastasia_kuzminykh",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "auth_fanny_chevalier",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "auth_carolina_nobre",
      "value": 5
    },
    {
      "source": "2601.13188v1",
      "target": "topic_ai_companionship",
      "value": 2
    },
    {
      "source": "2601.13188v1",
      "target": "topic_ai_companion",
      "value": 2
    },
    {
      "source": "2601.13188v1",
      "target": "topic_purpose_chatbots",
      "value": 2
    },
    {
      "source": "2601.13188v1",
      "target": "topic_chatbots_ai",
      "value": 2
    },
    {
      "source": "2601.13188v1",
      "target": "topic_conceptualize_companions",
      "value": 2
    },
    {
      "source": "2601.13187v1",
      "target": "auth_keigo_kusumegi",
      "value": 5
    },
    {
      "source": "2601.13187v1",
      "target": "auth_xinyu_yang",
      "value": 5
    },
    {
      "source": "2601.13187v1",
      "target": "auth_paul_ginsparg",
      "value": 5
    },
    {
      "source": "2601.13187v1",
      "target": "auth_mathijs_de_vaan",
      "value": 5
    },
    {
      "source": "2601.13187v1",
      "target": "auth_toby_stuart",
      "value": 5
    },
    {
      "source": "2601.13187v1",
      "target": "auth_yian_yin",
      "value": 5
    },
    {
      "source": "2601.13187v1",
      "target": "topic_documents_scientists",
      "value": 2
    },
    {
      "source": "2601.13187v1",
      "target": "topic_scientific_documents",
      "value": 2
    },
    {
      "source": "2601.13187v1",
      "target": "topic_cited_documents",
      "value": 2
    },
    {
      "source": "2601.13187v1",
      "target": "topic_draft_manuscripts",
      "value": 2
    },
    {
      "source": "2601.13187v1",
      "target": "topic_documents_findings",
      "value": 2
    }
  ]
}