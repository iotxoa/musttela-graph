{
  "nodes": [
    {
      "id": "2601.03222v1",
      "name": "The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI",
      "group": "paper",
      "val": 20,
      "abstract": "As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user's own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness.",
      "url": "https://arxiv.org/pdf/2601.03222v1",
      "date": "2026-01-06T18:07:52+00:00",
      "authors": [
        "Jacob Erickson"
      ]
    },
    {
      "id": "topic_trust_ai",
      "name": "trust ai",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_autonomy_trust",
      "name": "autonomy trust",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_conversational_ai",
      "name": "conversational ai",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_ai_agents",
      "name": "ai agents",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.03156v1",
      "name": "Prompt-Counterfactual Explanations for Generative AI System Behavior",
      "group": "paper",
      "val": 20,
      "abstract": "As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -- the prompt -- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.",
      "url": "https://arxiv.org/pdf/2601.03156v1",
      "date": "2026-01-06T16:33:19+00:00",
      "authors": [
        "Sofie Goethals",
        "Foster Provost",
        "JoÃ£o Sedoc"
      ]
    },
    {
      "id": "topic_explanations_generative",
      "name": "explanations generative",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_interpretability_generative",
      "name": "interpretability generative",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_counterfactual_explanations",
      "name": "counterfactual explanations",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_generative_ai",
      "name": "generative ai",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.03061v1",
      "name": "Vertical tacit collusion in AI-mediated markets",
      "group": "paper",
      "val": 20,
      "abstract": "AI shopping agents are being deployed to hundreds of millions of consumers, creating a new intermediary between platforms, sellers, and buyers. We identify a novel market failure: vertical tacit collusion, where platforms controlling rankings and sellers controlling product descriptions independently learn to exploit documented AI cognitive biases. Using multi-agent simulation calibrated to empirical measurements of large language model biases, we show that joint exploitation produces consumer harm more than double what would occur if strategies were independent. This super-additive harm arises because platform ranking determines which products occupy bias-triggering positions while seller manipulation determines conversion rates. Unlike horizontal algorithmic collusion, vertical tacit collusion requires no coordination and evades antitrust detection because harm emerges from aligned incentives rather than agreement. Our findings identify an urgent regulatory gap as AI shopping agents reach mainstream adoption.",
      "url": "https://arxiv.org/pdf/2601.03061v1",
      "date": "2026-01-06T14:43:14+00:00",
      "authors": [
        "Felipe M. Affonso"
      ]
    },
    {
      "id": "topic_ai_shopping",
      "name": "ai shopping",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_collusion_platforms",
      "name": "collusion platforms",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_evades_antitrust",
      "name": "evades antitrust",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_platforms_sellers",
      "name": "platforms sellers",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.02773v1",
      "name": "From Slaves to Synths? Superintelligence and the Evolution of Legal Personality",
      "group": "paper",
      "val": 20,
      "abstract": "This essay examines the evolving concept of legal personality through the lens of recent developments in artificial intelligence and the possible emergence of superintelligence. Legal systems have long been open to extending personhood to non-human entities, most prominently corporations, for instrumental or inherent reasons. Instrumental rationales emphasize accountability and administrative efficiency, whereas inherent ones appeal to moral worth and autonomy. Neither is yet sufficient to justify conferring personhood on AI. Nevertheless, the acceleration of technological autonomy may lead us to reconsider how law conceptualizes agency and responsibility. Drawing on comparative jurisprudence, corporate theory, and the emerging literature on AI governance, the paper argues that existing frameworks can address short-term accountability gaps, but the eventual development of superintelligence may force a paradigmatic shift in our understanding of law itself. In such a speculative future, legal personality may depend less on the cognitive sophistication of machines than on humanity's ability to preserve our own moral and institutional sovereignty.",
      "url": "https://arxiv.org/pdf/2601.02773v1",
      "date": "2026-01-06T07:09:55+00:00",
      "authors": [
        "Simon Chesterman"
      ]
    },
    {
      "id": "topic_superintelligence_legal",
      "name": "superintelligence legal",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_legal_personality",
      "name": "legal personality",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_ai_governance",
      "name": "ai governance",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_jurisprudence_corporate",
      "name": "jurisprudence corporate",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.02633v1",
      "name": "Fluid Agency in AI Systems: A Case for Functional Equivalence in Copyright, Patent, and Tort",
      "group": "paper",
      "val": 20,
      "abstract": "Modern Artificial Intelligence (AI) systems lack human-like consciousness or culpability, yet they exhibit fluid agency: behavior that is (i) stochastic (probabilistic and path-dependent), (ii) dynamic (co-evolving with user interaction), and (iii) adaptive (able to reorient across contexts). Fluid agency generates valuable outputs but collapses attribution, irreducibly entangling human and machine inputs. This fundamental unmappability fractures doctrines that assume traceable provenance--authorship, inventorship, and liability--yielding ownership gaps and moral \"crumple zones.\" This Article argues that only functional equivalence stabilizes doctrine. Where provenance is indeterminate, legal frameworks must treat human and AI contributions as equivalent for allocating rights and responsibility--not as a claim of moral or economic parity but as a pragmatic default. This principle stabilizes doctrine across domains, offering administrable rules: in copyright, vesting ownership in human orchestrators without parsing inseparable contributions; in patent, tying inventor-of-record status to human orchestration and reduction to practice, even when AI supplies the pivotal insight; and in tort, replacing intractable causation inquiries with enterprise-level and sector-specific strict or no-fault schemes. The contribution is both descriptive and normative: fluid agency explains why origin-based tests fail, while functional equivalence supplies an outcome-focused framework to allocate rights and responsibility when attribution collapses.",
      "url": "https://arxiv.org/pdf/2601.02633v1",
      "date": "2026-01-06T01:06:07+00:00",
      "authors": [
        "Anirban Mukherjee",
        "Hannah Hanwen Chang"
      ]
    },
    {
      "id": "topic_doctrine_provenance",
      "name": "doctrine provenance",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_ai_contributions",
      "name": "ai contributions",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_ai",
      "name": "ai",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_provenance",
      "name": "provenance",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.02631v1",
      "name": "Copyright Laundering Through the AI Ouroboros: Adapting the 'Fruit of the Poisonous Tree' Doctrine to Recursive AI Training",
      "group": "paper",
      "val": 20,
      "abstract": "Copyright enforcement rests on an evidentiary bargain: a plaintiff must show both the defendant's access to the work and substantial similarity in the challenged output. That bargain comes under strain when AI systems are trained through multi-generational pipelines with recursive synthetic data. As successive models are tuned on the outputs of its predecessors, any copyrighted material absorbed by an early model is diffused into deeper statistical abstractions. The result is an evidentiary blind spot where overlaps that emerge look coincidental, while the chain of provenance is too attenuated to trace. These conditions are ripe for \"copyright laundering\"--the use of multi-generational synthetic pipelines, an \"AI Ouroboros,\" to render traditional proof of infringement impracticable. This Article adapts the \"fruit of the poisonous tree\" (FOPT) principle to propose a AI-FOPT standard: if a foundational AI model's training is adjudged infringing (either for unlawful sourcing or for non-transformative ingestion that fails fair-use), then subsequent AI models principally derived from the foundational model's outputs or distilled weights carry a rebuttable presumption of taint. The burden shifts to downstream developers--those who control the evidence of provenance--to restore the evidentiary bargain by affirmatively demonstrating a verifiably independent and lawfully sourced lineage or a curative rebuild, without displacing fair-use analysis at the initial ingestion stage. Absent such proof, commercial deployment of tainted models and their outputs is actionable. This Article develops the standard by specifying its trigger, presumption, and concrete rebuttal paths (e.g., independent lineage or verifiable unlearning); addresses counterarguments concerning chilling innovation and fair use; and demonstrates why this lineage-focused approach is both administrable and essential.",
      "url": "https://arxiv.org/pdf/2601.02631v1",
      "date": "2026-01-06T01:02:50+00:00",
      "authors": [
        "Anirban Mukherjee",
        "Hannah Hanwen Chang"
      ]
    },
    {
      "id": "topic_proof_infringement",
      "name": "proof infringement",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_copyright_enforcement",
      "name": "copyright enforcement",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_copyright_laundering",
      "name": "copyright laundering",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_infringement_impracticable",
      "name": "infringement impracticable",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.02554v1",
      "name": "AI-exposed jobs deteriorated before ChatGPT",
      "group": "paper",
      "val": 20,
      "abstract": "Public debate links worsening job prospects for AI-exposed occupations to the release of ChatGPT in late 2022. Using monthly U.S. unemployment insurance records, we measure occupation- and location-specific unemployment risk and find that risk rose in AI-exposed occupations beginning in early 2022, months before ChatGPT. Analyzing millions of LinkedIn profiles, we show that graduate cohorts from 2021 onward entered AI-exposed jobs at lower rates than earlier cohorts, with gaps opening before late 2022. Finally, from millions of university syllabi, we find that graduates taking more AI-exposed curricula had higher first-job pay and shorter job searches after ChatGPT. Together, these results point to forces pre-dating generative AI and to the ongoing value of LLM-relevant education.",
      "url": "https://arxiv.org/pdf/2601.02554v1",
      "date": "2026-01-05T21:03:21+00:00",
      "authors": [
        "Morgan R. Frank",
        "Alireza Javadian Sabet",
        "Lisa Simon",
        "Sarah H. Bana",
        "Renzhe Yu"
      ]
    },
    {
      "id": "topic_job_prospects",
      "name": "job prospects",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_exposed_jobs",
      "name": "exposed jobs",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_occupations_release",
      "name": "occupations release",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_prospects_ai",
      "name": "prospects ai",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.02504v1",
      "name": "Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support",
      "group": "paper",
      "val": 20,
      "abstract": "Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.",
      "url": "https://arxiv.org/pdf/2601.02504v1",
      "date": "2026-01-05T19:20:59+00:00",
      "authors": [
        "Elizaveta Artser",
        "Daniil Karol",
        "Anna Potriasaeva",
        "Aleksei Rostovskii",
        "Katsiaryna Dzialets",
        "Ekaterina Koshchenko",
        "Xiaotian Su",
        "April Yi Wang",
        "Anastasiia Birillo"
      ]
    },
    {
      "id": "topic_teaching_debugging",
      "name": "teaching debugging",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_debugging_assistant",
      "name": "debugging assistant",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_debugging",
      "name": "debugging",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_powered_debugging",
      "name": "powered debugging",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.02205v1",
      "name": "From Chat Control to Robot Control: The Backdoors Left Open for the Sake of Safety",
      "group": "paper",
      "val": 20,
      "abstract": "This paper explores how a recent European Union proposal, the so-called Chat Control law, which creates regulatory incentives for providers to implement content detection and communication scanning, could transform the foundations of human-robot interaction (HRI). As robots increasingly act as interpersonal communication channels in care, education, and telepresence, they convey not only speech but also gesture, emotion, and contextual cues. We argue that extending digital surveillance laws to such embodied systems would entail continuous monitoring, embedding observation into the very design of everyday robots. This regulation blurs the line between protection and control, turning companions into potential informants. At the same time, monitoring mechanisms that undermine end-to-end encryption function as de facto backdoors, expanding the attack surface and allowing adversaries to exploit legally induced monitoring infrastructures. This creates a paradox of safety through insecurity: systems introduced to protect users may instead compromise their privacy, autonomy, and trust. This work does not aim to predict the future, but to raise awareness and help prevent certain futures from materialising.",
      "url": "https://arxiv.org/pdf/2601.02205v1",
      "date": "2026-01-05T15:27:07+00:00",
      "authors": [
        "Neziha Akalin",
        "Alberto Giaretta"
      ]
    },
    {
      "id": "topic_privacy_autonomy",
      "name": "privacy autonomy",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_compromise_privacy",
      "name": "compromise privacy",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_robots_regulation",
      "name": "robots regulation",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_everyday_robots",
      "name": "everyday robots",
      "group": "topic",
      "val": 8
    },
    {
      "id": "2601.01836v1",
      "name": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "group": "paper",
      "val": 20,
      "abstract": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
      "url": "https://arxiv.org/pdf/2601.01836v1",
      "date": "2026-01-05T06:57:45+00:00",
      "authors": [
        "Dasol Choi",
        "DongGeon Lee",
        "Brigitta Jesica Kartono",
        "Helena Berndt",
        "Taeyoun Kwon",
        "Joonwon Jang",
        "Haon Park",
        "Hwanjo Yu",
        "Minsuk Kahng"
      ]
    },
    {
      "id": "topic_compliance_adversarial",
      "name": "compliance adversarial",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_ai_safety",
      "name": "ai safety",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_organizational_allowlist",
      "name": "organizational allowlist",
      "group": "topic",
      "val": 8
    },
    {
      "id": "topic_comply_organizational",
      "name": "comply organizational",
      "group": "topic",
      "val": 8
    }
  ],
  "links": [
    {
      "source": "2601.03222v1",
      "target": "topic_trust_ai",
      "value": 6.37
    },
    {
      "source": "2601.03222v1",
      "target": "topic_autonomy_trust",
      "value": 4.86
    },
    {
      "source": "2601.03222v1",
      "target": "topic_conversational_ai",
      "value": 4.6
    },
    {
      "source": "2601.03222v1",
      "target": "topic_ai_agents",
      "value": 4.5
    },
    {
      "source": "2601.03156v1",
      "target": "topic_explanations_generative",
      "value": 5.71
    },
    {
      "source": "2601.03156v1",
      "target": "topic_interpretability_generative",
      "value": 5.21
    },
    {
      "source": "2601.03156v1",
      "target": "topic_counterfactual_explanations",
      "value": 5.11
    },
    {
      "source": "2601.03156v1",
      "target": "topic_generative_ai",
      "value": 4.93
    },
    {
      "source": "2601.03061v1",
      "target": "topic_ai_shopping",
      "value": 5.31
    },
    {
      "source": "2601.03061v1",
      "target": "topic_collusion_platforms",
      "value": 4.87
    },
    {
      "source": "2601.03061v1",
      "target": "topic_evades_antitrust",
      "value": 4.66
    },
    {
      "source": "2601.03061v1",
      "target": "topic_platforms_sellers",
      "value": 4.58
    },
    {
      "source": "2601.02773v1",
      "target": "topic_superintelligence_legal",
      "value": 5.97
    },
    {
      "source": "2601.02773v1",
      "target": "topic_legal_personality",
      "value": 5.96
    },
    {
      "source": "2601.02773v1",
      "target": "topic_ai_governance",
      "value": 5.66
    },
    {
      "source": "2601.02773v1",
      "target": "topic_jurisprudence_corporate",
      "value": 5.59
    },
    {
      "source": "2601.02633v1",
      "target": "topic_doctrine_provenance",
      "value": 4.82
    },
    {
      "source": "2601.02633v1",
      "target": "topic_ai_contributions",
      "value": 4.67
    },
    {
      "source": "2601.02633v1",
      "target": "topic_ai",
      "value": 4.61
    },
    {
      "source": "2601.02633v1",
      "target": "topic_provenance",
      "value": 4.43
    },
    {
      "source": "2601.02631v1",
      "target": "topic_proof_infringement",
      "value": 5.84
    },
    {
      "source": "2601.02631v1",
      "target": "topic_copyright_enforcement",
      "value": 5.53
    },
    {
      "source": "2601.02631v1",
      "target": "topic_copyright_laundering",
      "value": 5.47
    },
    {
      "source": "2601.02631v1",
      "target": "topic_infringement_impracticable",
      "value": 5.42
    },
    {
      "source": "2601.02554v1",
      "target": "topic_job_prospects",
      "value": 5.64
    },
    {
      "source": "2601.02554v1",
      "target": "topic_exposed_jobs",
      "value": 5.24
    },
    {
      "source": "2601.02554v1",
      "target": "topic_occupations_release",
      "value": 4.63
    },
    {
      "source": "2601.02554v1",
      "target": "topic_prospects_ai",
      "value": 4.62
    },
    {
      "source": "2601.02504v1",
      "target": "topic_teaching_debugging",
      "value": 7.29
    },
    {
      "source": "2601.02504v1",
      "target": "topic_debugging_assistant",
      "value": 6.9
    },
    {
      "source": "2601.02504v1",
      "target": "topic_debugging",
      "value": 5.81
    },
    {
      "source": "2601.02504v1",
      "target": "topic_powered_debugging",
      "value": 5.44
    },
    {
      "source": "2601.02205v1",
      "target": "topic_privacy_autonomy",
      "value": 5.27
    },
    {
      "source": "2601.02205v1",
      "target": "topic_compromise_privacy",
      "value": 5.09
    },
    {
      "source": "2601.02205v1",
      "target": "topic_robots_regulation",
      "value": 5.09
    },
    {
      "source": "2601.02205v1",
      "target": "topic_everyday_robots",
      "value": 4.97
    },
    {
      "source": "2601.01836v1",
      "target": "topic_compliance_adversarial",
      "value": 5.85
    },
    {
      "source": "2601.01836v1",
      "target": "topic_ai_safety",
      "value": 4.8
    },
    {
      "source": "2601.01836v1",
      "target": "topic_organizational_allowlist",
      "value": 4.39
    },
    {
      "source": "2601.01836v1",
      "target": "topic_comply_organizational",
      "value": 4.36
    }
  ]
}